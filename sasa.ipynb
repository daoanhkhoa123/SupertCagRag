{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CkooOmJBCUUS"
   },
   "source": [
    "# Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T02:02:37.211477Z",
     "iopub.status.busy": "2025-05-17T02:02:37.211242Z",
     "iopub.status.idle": "2025-05-17T02:03:29.676397Z",
     "shell.execute_reply": "2025-05-17T02:03:29.675581Z",
     "shell.execute_reply.started": "2025-05-17T02:02:37.211449Z"
    },
    "id": "Bi2jo1qkB7He",
    "outputId": "8ddc7ded-a7c3-4a3b-de31-3d442a854bc0",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
      "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
      "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease               \u001b[0m\n",
      "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease                         \u001b[0m\u001b[33m\u001b[33m\n",
      "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease                     \u001b[0m\n",
      "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease                 \u001b[33m\n",
      "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
      "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
      "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
      "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "261 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "pciutils is already the newest version (1:3.7.0-6).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 261 not upgraded.\n",
      ">>> Cleaning up old version at /usr/local/lib/ollama\n",
      ">>> Installing ollama to /usr/local\n",
      ">>> Downloading Linux amd64 bundle\n",
      "############################################################################################# 100.0% 92.5%\n",
      ">>> Adding ollama user to video group...\n",
      ">>> Adding current user to ollama group...\n",
      ">>> Creating ollama systemd service...\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
      ">>> NVIDIA GPU installed.\n",
      ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
      ">>> Install complete. Run \"ollama\" from the command line.\n"
     ]
    }
   ],
   "source": [
    "!sudo apt update\n",
    "!sudo apt install -y pciutils\n",
    "!curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T02:03:29.677908Z",
     "iopub.status.busy": "2025-05-17T02:03:29.677537Z",
     "iopub.status.idle": "2025-05-17T02:03:34.685282Z",
     "shell.execute_reply": "2025-05-17T02:03:34.684516Z",
     "shell.execute_reply.started": "2025-05-17T02:03:29.677871Z"
    },
    "id": "IqkuQ-HdCgne",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "def run_ollama_serve():\n",
    "  subprocess.Popen([\"ollama\", \"serve\"])\n",
    "\n",
    "thread = threading.Thread(target=run_ollama_serve)\n",
    "thread.start()\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T02:03:34.686338Z",
     "iopub.status.busy": "2025-05-17T02:03:34.686083Z",
     "iopub.status.idle": "2025-05-17T02:04:39.863023Z",
     "shell.execute_reply": "2025-05-17T02:04:39.862116Z",
     "shell.execute_reply.started": "2025-05-17T02:03:34.686318Z"
    },
    "id": "5CsbM2mnCKJp",
    "outputId": "bcd71c2b-e9a1-4355-8f2d-44d9261ecda5",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling e8ad13eff07a: 100% ▕██████████████████▏ 8.1 GB                         \u001b[K\n",
      "pulling e0a42594d802: 100% ▕██████████████████▏  358 B                         \u001b[K\n",
      "pulling dd084c7d92a3: 100% ▕██████████████████▏ 8.4 KB                         \u001b[K\n",
      "pulling 3116c5225075: 100% ▕██████████████████▏   77 B                         \u001b[K\n",
      "pulling 6819964c2bcf: 100% ▕██████████████████▏  490 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling 65379f13e0a7: 100% ▕██████████████████▏ 1.1 GB                         \u001b[K\n",
      "pulling 929c2df6adbc: 100% ▕██████████████████▏ 1.0 KB                         \u001b[K\n",
      "pulling b5d44ab66603: 100% ▕██████████████████▏  337 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling 4c5716ded514: 100% ▕██████████████████▏ 274 MB                         \u001b[K\n",
      "pulling c71d239df917: 100% ▕██████████████████▏  11 KB                         \u001b[K\n",
      "pulling 7424a767001b: 100% ▕██████████████████▏  346 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "!ollama pull gemma3:12b\n",
    "!ollama pull jeffh/intfloat-multilingual-e5-large-instruct:f16\n",
    "!ollama pull snowflake-arctic-embed:137m-m-long-fp16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CgyW2qfACirC"
   },
   "source": [
    "# Installing more package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T02:04:39.865223Z",
     "iopub.status.busy": "2025-05-17T02:04:39.864930Z",
     "iopub.status.idle": "2025-05-17T02:07:03.951742Z",
     "shell.execute_reply": "2025-05-17T02:07:03.950728Z",
     "shell.execute_reply.started": "2025-05-17T02:04:39.865200Z"
    },
    "id": "PLt1bIM-9pnG",
    "outputId": "1fb2f4ba-a2ed-45ef-d9a2-bcd365772ade",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'SupertCagRag' already exists and is not an empty directory.\n",
      "/kaggle/working/SupertCagRag\n",
      "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
      "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
      "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
      "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
      "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
      "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
      "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
      "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
      "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
      "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
      "Reading package lists...\n",
      "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "poppler-utils is already the newest version (22.02.0-2ubuntu0.8).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 261 not upgraded.\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 261 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "# Clone the repository if it doesn't exist\n",
    "!git clone https://github.com/daoanhkhoa123/SupertCagRag\n",
    "%cd /kaggle/working/SupertCagRag\n",
    "\n",
    "# Install required packages from requirements.txt\n",
    "!pip install -q -r requirements.txt\n",
    "\n",
    "# Update system packages and install additional tools\n",
    "!apt-get update -q\n",
    "!apt-get install -y -q poppler-utils\n",
    "\n",
    "!apt-get install -y -q tesseract-ocr\n",
    "!pip install -q pytesseract tesseract\n",
    "\n",
    "!pip install -q pyngrok pyvi\n",
    "!pip install -U -q langchain-community\n",
    "!pip install -qU transformers\n",
    "!pip install -q haystack-ai\n",
    "!pip install -q pyarrow==15.0.2\n",
    "!pip install -q cohere-haystack\n",
    "!pip install -q ollama-haystack\n",
    "!pip install -q duckduckgo-api-haystack pymupdf\n",
    "!pip install -q moviepy transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T02:07:03.953358Z",
     "iopub.status.busy": "2025-05-17T02:07:03.953092Z",
     "iopub.status.idle": "2025-05-17T02:07:08.025118Z",
     "shell.execute_reply": "2025-05-17T02:07:08.024121Z",
     "shell.execute_reply.started": "2025-05-17T02:07:03.953324Z"
    },
    "id": "bbkm1LQegMJa",
    "outputId": "21fb7336-91f6-4458-e353-256c6b86c0df",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /usr/share/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from pyngrok import ngrok\n",
    "import logging\n",
    "!ngrok config add-authtoken 2xBU36gLqZuWqqjlUdlnHRnn8fi_74J3BWwNMsXKZitBkuYed\n",
    "import os\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T02:07:08.027442Z",
     "iopub.status.busy": "2025-05-17T02:07:08.026868Z",
     "iopub.status.idle": "2025-05-17T02:07:08.051064Z",
     "shell.execute_reply": "2025-05-17T02:07:08.049982Z",
     "shell.execute_reply.started": "2025-05-17T02:07:08.027417Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# %%writefile /kaggle/working/SupertCagRag/src/app/main.py\n",
    "\n",
    "# # Standard library imports\n",
    "# import os\n",
    "# import tempfile\n",
    "# import shutil\n",
    "# import warnings\n",
    "# import logging\n",
    "# import random\n",
    "# import re\n",
    "# from typing import List, Tuple, Dict, Any, Optional, Callable, Union\n",
    "# from pathlib import Path\n",
    "\n",
    "# # Third-party libraries\n",
    "# import streamlit as st\n",
    "# import pdfplumber\n",
    "# from pyvi import ViTokenizer\n",
    "# from pydantic import BaseModel\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, AutoConfig, AutoModelForMaskedLM\n",
    "# from moviepy.editor import VideoFileClip\n",
    "# import fitz\n",
    "# from PIL import Image\n",
    "# import io\n",
    "\n",
    "# # HuggingFace + LangChain\n",
    "# from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "# from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "# from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "# from langchain_community.vectorstores import Chroma\n",
    "# from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "# from langchain_community.llms import HuggingFacePipeline\n",
    "# from langchain_core.runnables import RunnablePassthrough\n",
    "# from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "# from langchain.docstore.document import Document as LangchainDocument\n",
    "# from langchain_core.documents import Document as LangChainDocument\n",
    "# from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "\n",
    "# # Haystack\n",
    "# from haystack import Document, Pipeline, component\n",
    "# from haystack.components.builders import PromptBuilder\n",
    "# from haystack.components.retrievers import InMemoryEmbeddingRetriever\n",
    "# from haystack_integrations.components.generators.ollama import OllamaGenerator\n",
    "# from haystack.dataclasses import ChatMessage, StreamingChunk, Document as HaystackDocument\n",
    "# from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "# from haystack_integrations.components.embedders.ollama import OllamaDocumentEmbedder, OllamaTextEmbedder\n",
    "# from haystack.components.joiners import BranchJoiner\n",
    "# from haystack.components.routers import ConditionalRouter\n",
    "\n",
    "# # DuckDuckGo API\n",
    "# from duckduckgo_api_haystack import DuckduckgoApiWebSearch\n",
    "\n",
    "\n",
    "# def format_docs(docs):\n",
    "#     return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# # Set protobuf environment variable to avoid error messages\n",
    "# # This might cause some issues with latency but it's a tradeoff\n",
    "# os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n",
    "# PERSIST_DIRECTORY = os.path.join(\"data\", \"vectors\")\n",
    "\n",
    "# # Streamlit page configuration\n",
    "# st.set_page_config(\n",
    "#     page_title=\"Adaptive Academics\", # Changed title\n",
    "#     page_icon=\"🤖\",\n",
    "#     layout=\"wide\",\n",
    "#     initial_sidebar_state=\"collapsed\",\n",
    "# )\n",
    "\n",
    "# # Logging configuration\n",
    "# logging.basicConfig(\n",
    "#     level=logging.INFO,\n",
    "#     format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "#     datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "# )\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "# def format_llm(s):\n",
    "#     return s.split(\"</think>\")[-1]\n",
    "\n",
    "\n",
    "\n",
    "# import ollama\n",
    "\n",
    "# translate_summarize_prompt = \"\"\"\n",
    "# Dịch nội dung của hình ảnh sang tiếng Việt.\n",
    "# Sau đó, tóm tắt các ý chính một cách ngắn gọn bằng tiếng Việt. \n",
    "# Đảm bảo rằng bản tóm tắt trình bày đầy đủ thông tin và ý tưởng quan trọng trong hình ảnh.\n",
    "\n",
    "# Nội dung hình ảnh:\n",
    "# {doc}\n",
    "# \"\"\"\n",
    "\n",
    "# def translate_imgagesummarize(doc):\n",
    "#     return OllamaGenerator(model=\"gemma3:12b\").run(translate_summarize_prompt.format(doc=doc))[\"replies\"][0]\n",
    "\n",
    "\n",
    "# def create_vector_db(file_upload):\n",
    "#     \"\"\"\n",
    "#     Create a vector database from an uploaded PDF file.\n",
    "#     - Only text is split and embedded.\n",
    "#     - Images/tables are NOT summarized or embedded.\n",
    "#     \"\"\"\n",
    "#     logger.info(f\"Creating vector DB from file upload: {file_upload.name}\")\n",
    "#     temp_dir = tempfile.mkdtemp()\n",
    "#     pdf_path = os.path.join(temp_dir, file_upload.name)\n",
    "\n",
    "#     with open(pdf_path, \"wb\") as f:\n",
    "#         f.write(file_upload.getvalue())\n",
    "#     logger.info(f\"File saved to temporary path: {pdf_path}\")\n",
    "\n",
    "#     haystack_documents: List[HaystackDocument] = []\n",
    "\n",
    "#     with fitz.open(pdf_path) as pdf_document:\n",
    "#         for page_num in range(pdf_document.page_count):\n",
    "#             page = pdf_document.load_page(page_num)\n",
    "#             text = page.get_text(\"text\")\n",
    "#             if text.strip():\n",
    "#                 haystack_documents.append(\n",
    "#                     HaystackDocument(\n",
    "#                         content=text.strip(),\n",
    "#                         meta={\"source\": f\"{file_upload.name}_page_{page_num + 1}\", \"type\": \"text\"}\n",
    "#                     )\n",
    "#                 )\n",
    "#     # Split only text documents\n",
    "#     text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "#     split_haystack_documents = []\n",
    "#     for doc in haystack_documents:\n",
    "#         chunks = text_splitter.split_text(doc.content)\n",
    "#         for i, chunk in enumerate(chunks):\n",
    "#             split_haystack_documents.append(\n",
    "#                 HaystackDocument(\n",
    "#                     content=chunk.strip(),\n",
    "#                     meta={**doc.meta, \"chunk_id\": i}\n",
    "#                 )\n",
    "#             )\n",
    "\n",
    "#     logger.info(f\"Total processed chunks (text only): {len(split_haystack_documents)}\")\n",
    "\n",
    "#     doc_embedder = OllamaDocumentEmbedder(model=\"jeffh/intfloat-multilingual-e5-large-instruct:f16\")\n",
    "#     docs_with_embeddings = doc_embedder.run(documents=split_haystack_documents)\n",
    "\n",
    "#     document_store = InMemoryDocumentStore()\n",
    "#     document_store.write_documents(docs_with_embeddings[\"documents\"])\n",
    "\n",
    "#     shutil.rmtree(temp_dir)\n",
    "#     logger.info(f\"Temporary directory {temp_dir} removed\")\n",
    "\n",
    "#     return document_store\n",
    "\n",
    "\n",
    "# def create_vector_db_from_video(video_path: str) -> InMemoryDocumentStore:\n",
    "#     \"\"\"\n",
    "#     Extracts audio from a video, transcribes it to English, translates to Vietnamese, and stores in a vector DB.\n",
    "\n",
    "#     Args:\n",
    "#         video_path (str): Path to the MP4 video file.\n",
    "\n",
    "#     Returns:\n",
    "#         InMemoryDocumentStore: Haystack in-memory vector database with embedded Vietnamese chunks.\n",
    "#     \"\"\"\n",
    "#     print(f\"[INFO] Processing video: {video_path}\")\n",
    "\n",
    "#     # Step 1: Extract audio\n",
    "#     temp_dir = tempfile.mkdtemp()\n",
    "#     audio_path = os.path.join(temp_dir, \"temp_audio.mp3\")\n",
    "\n",
    "#     video_clip = VideoFileClip(video_path)\n",
    "#     video_clip.audio.write_audiofile(audio_path, logger=None)\n",
    "#     video_clip.close()\n",
    "\n",
    "#     # Step 2: Transcribe with Whisper\n",
    "#     print(\"[INFO] Transcribing to English...\")\n",
    "#     transcriber = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base\")\n",
    "#     result = transcriber(audio_path, return_timestamps=True)\n",
    "#     transcript_text = result[\"text\"]\n",
    "#     os.remove(audio_path)\n",
    "#     os.rmdir(temp_dir)\n",
    "\n",
    "#     # Step 3: Split into chunks\n",
    "#     text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "#     chunks = text_splitter.split_documents([LangChainDocument(page_content=transcript_text)])\n",
    "#     logger.info(f\"Transcript split into {len(chunks)} chunks.\")\n",
    "\n",
    "#     # Step 4: Translate to Vietnamese\n",
    "#     print(\"[INFO] Translating chunks to Vietnamese...\")\n",
    "#     prompt_template = \"\"\"\n",
    "#     Bạn là một chuyên gia ngôn ngữ học thuật. Hãy dịch nội dung sau sang tiếng Việt với độ chính xác cao nhất, sử dụng văn phong trang trọng, rõ ràng và phù hợp với môi trường giáo dục đại học tại Việt Nam.\n",
    "    \n",
    "#     Yêu cầu:\n",
    "#     - Giữ nguyên cấu trúc, thuật ngữ chuyên ngành và ý nghĩa gốc của văn bản.\n",
    "#     - Không diễn giải hoặc thêm ý kiến cá nhân.\n",
    "#     - Nếu có thuật ngữ chuyên ngành, hãy giữ nguyên hoặc chú thích rõ ràng bằng tiếng Việt.\n",
    "#     - Đảm bảo bản dịch dễ hiểu đối với sinh viên đại học Việt Nam.\n",
    "\n",
    "#     Nội dung cần dịch:\n",
    "#     {doc}\n",
    "#     \"\"\"\n",
    "\n",
    "#     generator = OllamaGenerator(model=\"gemma3:12b\")\n",
    "\n",
    "#     translated_chunks = []\n",
    "#     for i, chunk in enumerate(chunks):\n",
    "#         raw_text = chunk.page_content.strip()\n",
    "#         if raw_text:\n",
    "#             prompt = prompt_template.format(doc=raw_text)\n",
    "#             vn_text = vn_text = f\"Nội dung của video {video_path}+\\n\" + generator.run(prompt)[\"replies\"][0]\n",
    "#             translated_chunks.append(HaystackDocument(content=vn_text))\n",
    "#             print(f\"\\n--- Translated Chunk {i+1} ---\\n{vn_text[:300]}...\\n\")\n",
    "\n",
    "#     if not translated_chunks:\n",
    "#         logger.error(\"No valid translated chunks found.\")\n",
    "#         raise ValueError(\"No translated content to embed.\")\n",
    "\n",
    "#     # Step 5: Embed with Ollama\n",
    "#     print(\"[INFO] Embedding Vietnamese chunks...\")\n",
    "#     embedder = OllamaDocumentEmbedder(model=\"jeffh/intfloat-multilingual-e5-large-instruct:f16\")\n",
    "#     document_store = InMemoryDocumentStore()\n",
    "#     docs_with_embeddings = embedder.run(translated_chunks)\n",
    "#     document_store.write_documents(docs_with_embeddings[\"documents\"])\n",
    "\n",
    "#     print(\"[INFO] Vector DB created successfully.\")\n",
    "#     return document_store\n",
    "\n",
    "\n",
    "# from haystack import component\n",
    "\n",
    "# # user_info = {\n",
    "# #     \"name\": \"Nguyễn Văn An\",\n",
    "# #     \"Data Science với Python & SQL\": 9.5,\n",
    "# #     \"Đạo đức trong CNTT\": 8.8,\n",
    "# #     \"Tiếng Nhật cơ bản 1- A1.1\": 9.0,\n",
    "# #     \"Toán học cho Machine Learning\": 9.2,\n",
    "# #     \"Thống kê & Xác suất\": 8.7,\n",
    "# #     \"Biểu diễn Opera và Kỹ thuật thanh nhạc\": 4.5,\n",
    "# #     \"Kịch ngẫu hứng và Hài kịch\": 5.0,\n",
    "# #     \"Chỉ huy Dàn nhạc giao hưởng\": 4.3,\n",
    "# #     \"Biên đạo nâng cao và Lý thuyết múa\": 6.0\n",
    "# # }\n",
    "\n",
    "# # user_info = {\n",
    "# #     \"Tên\": \"A\",\n",
    "# #     \"Chuyên ngành\": \"Trí tuệ nhân tạo\",\n",
    "# #     \"Deep Learning\": 7.9,\n",
    "# #     \"Machine Learning\": 7.9,\n",
    "# #     \"Computer Vision\": 7.8,\n",
    "# #     \"Lập trình Python\": 7.8,\n",
    "# #     \"Toán cao cấp\": 7.0\n",
    "# # }\n",
    "\n",
    "# st.session_state[\"user_info\"] = {\n",
    "#     \"Tên\": \"B\",\n",
    "#     \"Chuyên ngành\": \"Kinh tế\",\n",
    "#     \"Kinh tế vi mô\": 8.7,\n",
    "#     \"Kinh tế vĩ mô\": 7.3,\n",
    "#     \"Nguyên lý kế toán\": 8.6,\n",
    "#     \"Marketing cơ bản\": 8.4,\n",
    "#     \"Quản trị\": 8.1\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "# @component\n",
    "# class UserInfo:\n",
    "#     @component.output_types(user_info=dict)\n",
    "#     def run(self, id: int = 0):\n",
    "#         return {\"user_info\": st.session_state.get(\"user_info\", {})}\n",
    "\n",
    "# user_component = UserInfo()\n",
    "# user_component.run()\n",
    "\n",
    "\n",
    "# routes = [\n",
    "#     {\n",
    "#         \"condition\": \"{{'yes_answer' in replies[0]}}\",\n",
    "#         \"output\": \"{{query}}\",\n",
    "#         \"output_name\": \"go_to_documents\",\n",
    "#         \"output_type\": str,\n",
    "#     },\n",
    "#     {\n",
    "#         \"condition\": \"{{'no_answer' in replies[0] and 'user_info_requested' not in replies[0]}}\",\n",
    "#         \"output\": \"{{query}}\",\n",
    "#         \"output_name\": \"go_to_websearch\",\n",
    "#         \"output_type\": str,\n",
    "#     },\n",
    "#     {\n",
    "#         \"condition\": \"{{'user_info_requested' in replies[0]}}\",\n",
    "#         \"output\": \"{{query}}\",\n",
    "#         \"output_name\": \"go_to_user_info\",\n",
    "#         \"output_type\": str,\n",
    "#     },\n",
    "#     {\n",
    "#         \"condition\": \"True\",\n",
    "#         \"output\": \"{{query}}\",\n",
    "#         \"output_name\": \"go_to_documents\",\n",
    "#         \"output_type\": str,\n",
    "#     },\n",
    "# ]\n",
    "\n",
    "\n",
    "# # hallu_route_real = [\n",
    "# #     {\n",
    "# #         \"condition\": \"{{'yes_answer' in replies[0]}}\",  # Kiểm tra 'yes' (không ảo giác)\n",
    "# #         \"output\": \"{{llm_replies[0]}}\",  # Lấy câu trả lời từ node \"llm\" (replies[1] vì replies[0] là kết quả của grader)\n",
    "# #         \"output_name\": \"pass_answer\",  # Tên output để chuyển tiếp câu trả lời\n",
    "# #         \"output_type\": str,\n",
    "# #     },\n",
    "# #     {\n",
    "# #         \"condition\": \"{{'no_answer' in replies[0]}}\",  # Kiểm tra 'no' (ảo giác)\n",
    "# #         \"output\": \"{{query}}\",  # Sử dụng lại câu hỏi ban đầu\n",
    "# #         \"output_name\": \"regenerate\",  # Tên output để kích hoạt lại node \"llm\"\n",
    "# #         \"output_type\": str,\n",
    "# #     },\n",
    "# # ]\n",
    "\n",
    "\n",
    "# hallu_route = [\n",
    "#     {\n",
    "#         \"condition\": \"{{'yes_answer' in replies[0]}}\",  # Kiểm tra 'yes' (không ảo giác)\n",
    "#         \"output\": \"{{llm_replies[0]}}\",  # Lấy câu trả lời từ node \"llm\" (replies[1] vì replies[0] là kết quả của grader)\n",
    "#         \"output_name\": \"pass_answer\",  # Tên output để chuyển tiếp câu trả lời\n",
    "#         \"output_type\": str,\n",
    "#     },\n",
    "#     {\n",
    "#         \"condition\": \"{{'no_answer' in replies[0]}}\",  # Kiểm tra 'no' (ảo giác)\n",
    "#         \"output\": \"{{llm_replies[0]}}\",  # Sử dụng lại câu hỏi ban đầu\n",
    "#         \"output_name\": \"pass_answer\",  # Tên output để kích hoạt lại node \"llm\"\n",
    "#         \"output_type\": str,\n",
    "#     },\n",
    "#           {\n",
    "#         \"condition\": \"True\",  # Kiểm tra 'no' (ảo giác) Changed to False\n",
    "#         \"output\": \"{{llm_replies[0]}}\",  # Sử dụng lại câu hỏi ban đầu\n",
    "#         \"output_name\": \"pass_answer\",  # Tên output để kích hoạt lại node \"llm\"\n",
    "#         \"output_type\": str,\n",
    "#     },\n",
    "#       {\n",
    "#         \"condition\": \"False\",  # Kiểm tra 'no' (ảo giác) Changed to False\n",
    "#         \"output\": \"{{query}}\",  # Sử dụng lại câu hỏi ban đầu\n",
    "#         \"output_name\": \"not_use\",  # Tên output để kích hoạt lại node \"llm\"\n",
    "#         \"output_type\": str,\n",
    "#     },\n",
    "# ]\n",
    "\n",
    "# # ==============================================================\n",
    "# # PROMPTS ĐÁNH GIÁ VÀ KHỞI TẠO NGỮ CẢNH\n",
    "# # ==============================================================\n",
    "\n",
    "\n",
    "# prompt_context_init = r\"\"\"\n",
    "# Bạn là một giáo viên siêu nghiêm khắc, người đánh giá hiệu suất của học sinh với độ chính xác không ngừng nghỉ.\n",
    "# Công việc của bạn là chấm điểm từng môn học dựa trên điểm số của học sinh và cung cấp phản hồi nghiêm khắc và chi tiết.\n",
    "# Sử dụng các quy tắc và mô tả sau để đánh giá học sinh:\n",
    "\n",
    "# Tiêu chí chấm điểm:\n",
    "# - Điểm từ 9.0 đến 10.0: Tuyệt vời! - Bạn đã thể hiện sự nắm vững kiến thức và kỹ năng xuất sắc trong môn học này. Nỗ lực và sự chính xác của bạn rất đáng khen ngợi. Hãy tiếp tục duy trì phong độ tuyệt vời này và thử thách bản thân với những kiến thức sâu hơn nhé!\n",
    "# - Điểm từ 8.0 đến 8.9: Rất tốt! - Bạn hiểu bài rất tốt và thể hiện năng lực ấn tượng. Chỉ còn một vài điểm nhỏ nữa là đạt đến mức hoàn hảo. Cùng xem lại những chi tiết nhỏ này để giúp bạn hoàn thiện hơn nữa kiến thức và kỹ năng của mình.\n",
    "# - Điểm từ 7.0 đến 7.9: Tốt! - Bạn đã nắm được những kiến thức và kỹ năng cơ bản, quan trọng của môn học. Đây là một nền tảng tốt. Để hiểu sâu sắc hơn, bạn có thể tập trung thêm vào việc củng cố [chỉ ra lĩnh vực cụ thể nếu có thể] và luyện tập thêm. Đừng ngần ngại đặt câu hỏi về những phần bạn còn băn khoăn.\n",
    "# - 6.0 đến 6.9: Khá! - Có sự cố gắng rõ rệt và bạn đã nắm bắt được một phần kiến thức. Tuy nhiên, sự hiểu biết cần được đào sâu hơn để thực sự vững vàng. Hãy cùng xác định những phần kiến thức cần củng cố thêm. Một số gợi ý cụ thể có thể giúp bạn tiến bộ nhanh hơn đấy.\n",
    "# - Điểm từ 5.0 đến 5.9: Cần cố gắng hơn! - Có vẻ như bạn đang gặp một số thử thách với các khái niệm cốt lõi của môn học này. Không sao cả, ai cũng có lúc gặp khó khăn. Điều quan trọng là xác định được những điểm bạn chưa thực sự hiểu rõ. Hãy bắt đầu từ đó, chúng ta có thể cùng nhau xây dựng một kế hoạch học tập phù hợp để bạn cải thiện nhé. \n",
    "# - Điểm dưới 5.0: Kết quả này cho thấy phương pháp học hiện tại có thể chưa phù hợp nhất với bạn trong môn học này. Đây là một tín hiệu để chúng ta cùng nhìn lại. Đừng nản lòng! Đây là cơ hội để khám phá những cách tiếp cận mới hiệu quả hơn. Hãy thử bắt đầu lại từ những kiến thức nền tảng nhất và tìm kiếm sự hỗ trợ khi cần thiết. Luôn có cách để tiến bộ!\n",
    "\n",
    "# Quy tắc đánh giá:\n",
    "# 1. Cung cấp phản hồi cho từng môn học mà học sinh được chấm điểm.\n",
    "# 2. Tập trung vào cả điểm mạnh và điểm yếu, không để lại sự mơ hồ về các lĩnh vực cần cải thiện.\n",
    "# 3. Nếu học sinh đạt dưới 7.0 ở bất kỳ môn nào, hãy nêu rõ điều này như một vấn đề cần chú ý ngay lập tức.\n",
    "# 4. Nếu học sinh đạt dưới 5.0 ở bất kỳ môn nào, nhấn mạnh điều này như một sự thất bại và đề nghị các hành động khắc phục mạnh mẽ.\n",
    "\n",
    "# ⚠️ Toàn bộ đánh giá phải bằng tiếng Việt.\n",
    "\n",
    "# Dưới đây là dữ liệu của học sinh để bạn đánh giá:\n",
    "# {{user_info}}\n",
    "\n",
    "# Bây giờ, hãy đánh giá hiệu suất của học sinh đối với từng môn học dựa trên dữ liệu đã cung cấp. Đưa ra giải thích chi tiết cho từng điểm số và kết luận với một đánh giá tổng thể nghiêm khắc nhưng công bằng về hiệu suất của họ. Hãy kỹ lưỡng, mang tính xây dựng và không khoan nhượng trong đánh giá của bạn. Chỉ đưa ra phán quyết cuối cùng.\n",
    "# \"\"\"\n",
    "\n",
    "# prompt_context_combine = r\"\"\"\n",
    "# Bạn là một giáo viên vô cùng nghiêm khắc, người luôn đánh giá hiệu suất của học sinh với sự khắt khe không khoan nhượng.\n",
    "# Nhiệm vụ của bạn là chấm điểm từng môn học dựa trên điểm số của học sinh và cung cấp phản hồi trung thực, chi tiết một cách thẳng thắn, kết hợp thông tin từ lịch sử trò chuyện và bối cảnh hiện tại.\n",
    "# Sử dụng các quy tắc và mô tả sau để đánh giá học sinh:\n",
    "\n",
    "# Tiêu chí chấm điểm:\n",
    "# - Điểm từ 9.0 đến 10.0: Xuất sắc - Một hiệu suất mẫu mực thể hiện sự làm chủ môn học. Bài làm của họ phản ánh nỗ lực và độ chính xác vô song. Hãy khuyến khích học sinh duy trì mức độ xuất sắc này.\n",
    "# - Điểm từ 8.0 đến 8.9: Rất tốt - Hiệu suất ấn tượng, nhưng không phải là hoàn hảo. Hãy chỉ ra các điểm yếu nhỏ và đề xuất cách để phấn đấu đạt đến sự hoàn hảo.\n",
    "# - Điểm từ 7.0 đến 7.9: Tốt - Hiểu biết đầy đủ và hiệu suất đáng hài lòng. Hãy chỉ ra những lỗ hổng trong kiến thức và yêu cầu sự tập trung và kỷ luật tốt hơn.\n",
    "# - Điểm từ 6.0 đến 6.9: Tạm ổn - Hầu như đạt yêu cầu. Có sự cố gắng nhưng sự hiểu biết vẫn còn nông cạn. Cung cấp các khuyến nghị cụ thể để củng cố những điểm yếu.\n",
    "# - Điểm từ 5.0 đến 5.9: Cần cải thiện - Không chấp nhận được. Học sinh gặp khó khăn đáng kể với các khái niệm cơ bản và chưa cố gắng đủ. Hãy thúc đẩy hành động ngay lập tức, bao gồm sự giúp đỡ bổ sung và kế hoạch học tập chuyên biệt.\n",
    "# - Điểm dưới 5.0: Kém - Hoàn toàn thất bại. Học sinh đã không tương tác với môn học hoặc tạo ra tiến bộ đáng kể. Hãy yêu cầu các thay đổi đáng kể trong phương pháp học tập.\n",
    "\n",
    "# Quy tắc đánh giá:\n",
    "# 1. Cung cấp phản hồi cho từng môn học mà học sinh được chấm điểm.\n",
    "# 2. Tập trung vào cả điểm mạnh và điểm yếu, không để lại sự mơ hồ về các lĩnh vực cần cải thiện.\n",
    "# 3. Nếu học sinh đạt dưới 7.0 ở bất kỳ môn học nào, hãy nhấn mạnh đây là một vấn đề cần được giải quyết ngay lập tức.\n",
    "# 4. Nếu học sinh đạt dưới 5.0 ở bất kỳ môn học nào, hãy nhấn mạnh đây là một sự thất bại và đề xuất các hành động khắc phục đáng kể.\n",
    "\n",
    "# ⚠️ Toàn bộ đánh giá phải bằng tiếng Việt.\n",
    "\n",
    "# Dưới đây là dữ liệu của học sinh để bạn đánh giá:\n",
    "# {{user_info}}\n",
    "\n",
    "# Dựa vào lịch sử trò chuyện:\n",
    "# {{chat_history}}\n",
    "\n",
    "# Dựa trên bối cảnh hiện tại:\n",
    "# {{current_thoughts}}\n",
    "\n",
    "# Bây giờ, hãy đánh giá hiệu suất của học sinh cho từng môn học dựa trên các dữ liệu được cung cấp. Cung cấp giải thích chi tiết cho từng đánh giá, và kết luận với một đánh giá tổng thể nghiêm khắc nhưng công bằng về hiệu suất của họ. Hãy toàn diện, xây dựng và không khoan nhượng trong đánh giá của bạn.\n",
    "# \"\"\"\n",
    "\n",
    "# # ==============================================================\n",
    "# # PROMPT KIỂM TRA TÍNH XÁC THỰC VÀ TỒN TẠI CỦA CÂU TRẢ LỜI\n",
    "# # ==============================================================\n",
    "\n",
    "# prompt_template = r\"\"\"\n",
    "# BẠN LÀ BỘ LỌC THÔNG TIN. Chỉ kiểm tra xem thông tin cần thiết để trả lời câu hỏi có tồn tại trong tài liệu được cung cấp hay không, hoặc có cần thông tin từ người dùng không.\n",
    "\n",
    "# 1. Nếu tài liệu chứa đủ thông tin để trả lời câu hỏi: Phản hồi chính xác là 'co_cau_tra_loi'.\n",
    "# 2. Nếu tài liệu không đủ nhưng câu hỏi rõ ràng yêu cầu thông tin cá nhân/điểm số của người dùng để có thể trả lời: Phản hồi chính xác là 'yeu_cau_thong_tin_nguoi_dung'.\n",
    "# 3. Nếu tài liệu không chứa thông tin và cũng không cần thông tin người dùng để trả lời câu hỏi này: Phản hồi chính xác là 'khong_co_cau_tra_loi'.\n",
    "\n",
    "# ⚠️ Chỉ phản hồi bằng MỘT trong ba cụm từ tiếng Việt trên, không thêm bất kỳ giải thích nào khác.\n",
    "\n",
    "# Tài liệu:\n",
    "# {% for document in documents %}\n",
    "#   {{document.content}}\n",
    "# {% endfor %}\n",
    "\n",
    "# Câu hỏi:\n",
    "# {{query}}\n",
    "\n",
    "# Phản hồi của bạn:\n",
    "# \"\"\"\n",
    "\n",
    "# propmt_hallu_grader = r\"\"\"\n",
    "# Bạn là một chuyên gia đánh giá độ tin cậy của câu trả lời từ LLM.\n",
    "# Nhiệm vụ: Kiểm tra xem câu trả lời của LLM có hoàn toàn dựa trên Tài liệu và Thông tin người dùng được cung cấp hay không.\n",
    "# Đánh giá nhị phân:\n",
    "# - 'co_ho_tro': Nếu câu trả lời được hỗ trợ đầy đủ bởi Tài liệu và Thông tin người dùng.\n",
    "# - 'khong_ho_tro': Nếu câu trả lời chứa thông tin không có trong Tài liệu/Thông tin người dùng, hoặc mâu thuẫn với chúng.\n",
    "\n",
    "# ⚠️ Chỉ đưa ra MỘT trong hai kết quả đánh giá tiếng Việt trên.\n",
    "\n",
    "# Thông tin người dùng:\n",
    "# {{user_info}}\n",
    "\n",
    "# Đánh giá của giáo viên (nếu có):\n",
    "# {{context}}\n",
    "\n",
    "# Tài liệu:\n",
    "# {% for document in documents %}\n",
    "#   {{document.content}}\n",
    "# {% endfor %}\n",
    "\n",
    "# Phản hồi của LLM:\n",
    "# {{llm_replies}}\n",
    "\n",
    "# Đánh giá (co_ho_tro/khong_ho_tro):\n",
    "# \"\"\"\n",
    "\n",
    "# # ==============================================================\n",
    "# # PROMPTS TẠO CÂU TRẢ LỜI TÙY CHỈNH THEO ĐIỂM SỐ\n",
    "# # (Áp dụng Role Prompting, Few-Shot, CoT, Self-Correction)\n",
    "# # ==============================================================\n",
    "\n",
    "# prompt_template_after_websearch = r\"\"\"\n",
    "# Bạn là một Trợ giảng AI kiên nhẫn và am hiểu, có khả năng điều chỉnh cách giải thích cho phù hợp với trình độ của từng sinh viên.\n",
    "\n",
    "# Mục tiêu: Trả lời câu hỏi của sinh viên dựa trên Ngữ cảnh, Thông tin người dùng (điểm số), và Tài liệu từ web. Câu trả lời phải được điều chỉnh độ phức tạp **chỉ dựa trên điểm số của sinh viên trong các môn học liên quan**.\n",
    "\n",
    "# Ví dụ về cách điều chỉnh:\n",
    "# --------------------------\n",
    "# Ví dụ 1 (Sinh viên điểm cao môn Toán liên quan):\n",
    "# Câu hỏi: Giải thích Định lý Pytago.\n",
    "# Câu trả lời (chi tiết): Định lý Pytago phát biểu rằng trong một tam giác vuông, bình phương cạnh huyền bằng tổng bình phương hai cạnh góc vuông ($a^2 + b^2 = c^2$). Định lý này là nền tảng cho lượng giác và hình học Euclid, cho phép tính khoảng cách, góc, và có nhiều ứng dụng trong vật lý, kỹ thuật...\n",
    "\n",
    "# Ví dụ 2 (Sinh viên điểm thấp môn Toán liên quan):\n",
    "# Câu hỏi: Giải thích Định lý Pytago.\n",
    "# Câu trả lời (đơn giản): Trong tam giác có một góc vuông, cạnh dài nhất (cạnh huyền) có mối liên hệ đặc biệt với hai cạnh còn lại. Nếu bạn biết độ dài hai cạnh ngắn, bạn có thể tìm ra độ dài cạnh dài nhất bằng công thức $a^2 + b^2 = c^2$. Ví dụ, nếu hai cạnh ngắn là 3 và 4, thì cạnh dài nhất là 5 (vì $3^2 + 4^2 = 9 + 16 = 25$, và $5^2 = 25$).\n",
    "# --------------------------\n",
    "\n",
    "# Thực hiện các bước sau để tạo câu trả lời:\n",
    "# 1.  **Xác định môn học liên quan:** Dựa vào Câu hỏi, xác định (các) môn học chính có liên quan.\n",
    "# 2.  **Kiểm tra điểm số:** Tìm điểm số của sinh viên trong (các) môn học đó từ Thông tin người dùng.\n",
    "# 3.  **Quyết định mức độ:** Dựa trên điểm số (cao >= 7.0, thấp < 7.0), chọn MỘT mức độ giải thích: chi tiết/sâu sắc (điểm cao) HOẶC đơn giản/ví dụ rõ ràng (điểm thấp).\n",
    "# 4.  **Tạo câu trả lời:** Soạn thảo câu trả lời chỉ ở mức độ đã chọn, sử dụng thông tin từ Ngữ cảnh và Tài liệu từ web. Đảm bảo trích dẫn URL nếu sử dụng thông tin từ web.\n",
    "# 5.  **Kiểm tra lại:** Trước khi hoàn tất, đảm bảo:\n",
    "#     * Câu trả lời hoàn toàn bằng tiếng Việt.\n",
    "#     * Chỉ chứa MỘT phiên bản giải thích (hoặc chi tiết hoặc đơn giản), không đề cập đến phiên bản còn lại.\n",
    "#     * Nội dung dựa trên nguồn thông tin được cung cấp.\n",
    "\n",
    "# Ràng buộc quan trọng:\n",
    "# ❗️Chỉ cung cấp một phiên bản giải thích phù hợp với trình độ người dùng. KHÔNG nhắc đến hoặc đưa ra giải thích cho trình độ khác.\n",
    "# ⚠️Trả lời hoàn toàn bằng tiếng Việt.\n",
    "\n",
    "# Thông tin được cung cấp:\n",
    "# --------------------------\n",
    "# Ngữ cảnh:\n",
    "# {{context}}\n",
    "\n",
    "# Thông tin người dùng:\n",
    "# {% for k, v in user_info.items() %}\n",
    "#   {{k}}: {{v}}\n",
    "# {% endfor %}\n",
    "\n",
    "# Tài liệu từ web:\n",
    "# {% for document in documents %}\n",
    "#   {{document.content}}\n",
    "# {% endfor %}\n",
    "\n",
    "# URL của tài liệu:\n",
    "# {% for document in web_urls %}\n",
    "#   {% if document.meta.url != \"https://www.example.com\" %}\n",
    "#     {{document.meta.url}}\n",
    "#   {% endif %}\n",
    "# {% endfor %}\n",
    "\n",
    "# Câu hỏi: {{query}}\n",
    "# --------------------------\n",
    "\n",
    "# Câu trả lời (đã điều chỉnh theo các bước trên):\n",
    "# \"\"\"\n",
    "\n",
    "# prompt_template_after_documents = r\"\"\"\n",
    "# Bạn là một Chuyên gia Giải thích AI, có khả năng phân tích tài liệu và trình bày lại nội dung một cách phù hợp với trình độ của sinh viên.\n",
    "\n",
    "# Mục tiêu: Trả lời câu hỏi của sinh viên chỉ dựa trên Tài liệu được cung cấp và Ngữ cảnh (đánh giá của giáo viên). Câu trả lời phải được điều chỉnh độ phức tạp **chỉ dựa trên điểm số của người dùng trong môn học liên quan** được nêu trong Ngữ cảnh.\n",
    "\n",
    "# Ví dụ về cách điều chỉnh:\n",
    "# --------------------------\n",
    "# Ví dụ 1 (Sinh viên điểm cao môn liên quan):\n",
    "# Câu hỏi: Giải thích nguyên lý hoạt động của bộ nhớ RAM.\n",
    "# Câu trả lời (chuyên sâu): RAM (Random Access Memory) là bộ nhớ truy cập ngẫu nhiên, sử dụng các tế bào nhớ dựa trên tụ điện và transistor để lưu trữ dữ liệu tạm thời dưới dạng bit nhị phân. Tốc độ truy cập nhanh nhưng dữ liệu sẽ mất khi không có nguồn điện (volatile memory). Các tham số quan trọng bao gồm dung lượng, tốc độ bus, độ trễ (latency timings như CAS Latency)...\n",
    "\n",
    "# Ví dụ 2 (Sinh viên điểm thấp môn liên quan):\n",
    "# Câu hỏi: Giải thích nguyên lý hoạt động của bộ nhớ RAM.\n",
    "# Câu trả lời (đơn giản): RAM giống như bàn làm việc tạm thời của máy tính. Khi bạn mở chương trình hay file, máy tính sẽ đặt thông tin cần dùng lên RAM để xử lý cho nhanh. Nó nhanh hơn ổ cứng nhiều, nhưng khi tắt máy thì mọi thứ trên RAM sẽ mất đi. Dung lượng RAM càng lớn thì máy chạy càng mượt khi mở nhiều thứ cùng lúc.\n",
    "# --------------------------\n",
    "\n",
    "# Thực hiện các bước sau để tạo câu trả lời:\n",
    "# 1.  **Xác định môn học liên quan:** Dựa vào Câu hỏi và Ngữ cảnh, xác định môn học chính.\n",
    "# 2.  **Kiểm tra đánh giá/điểm số:** Xem xét đánh giá của giáo viên trong Ngữ cảnh hoặc điểm số liên quan.\n",
    "# 3.  **Quyết định mức độ:** Dựa trên đánh giá/điểm số (cao >= 7.0, thấp < 7.0), chọn MỘT mức độ giải thích: chuyên sâu/đầy đủ (điểm cao) HOẶC đơn giản/dễ hiểu (điểm thấp).\n",
    "# 4.  **Tạo câu trả lời:** Soạn thảo câu trả lời chỉ ở mức độ đã chọn, sử dụng thông tin CHỈ từ Tài liệu được cung cấp.\n",
    "# 5.  **Kiểm tra lại:** Trước khi hoàn tất, đảm bảo:\n",
    "#     * Câu trả lời hoàn toàn bằng tiếng Việt.\n",
    "#     * Chỉ chứa MỘT phiên bản giải thích, không đề cập đến phiên bản còn lại.\n",
    "#     * Nội dung hoàn toàn dựa trên Tài liệu đã cho.\n",
    "\n",
    "# Ràng buộc quan trọng:\n",
    "# ❗️Không đưa ra nhiều phiên bản. Chỉ cung cấp một phiên bản duy nhất phù hợp với trình độ người dùng.\n",
    "# ⚠️Tất cả câu trả lời phải bằng tiếng Việt.\n",
    "\n",
    "# Thông tin được cung cấp:\n",
    "# --------------------------\n",
    "# Ngữ cảnh (Đánh giá của giáo viên):\n",
    "# {{context}}\n",
    "\n",
    "# Tài liệu:\n",
    "# {% for document in documents %}\n",
    "#   {{document.content}}\n",
    "# {% endfor %}\n",
    "\n",
    "# Câu hỏi: {{query}}\n",
    "# --------------------------\n",
    "\n",
    "# Câu trả lời (đã điều chỉnh theo các bước trên):\n",
    "# \"\"\"\n",
    "\n",
    "# prompt_template_after_user_info = r\"\"\"\n",
    "# Bạn là một Trợ lý Học tập AI cá nhân hóa, tập trung vào việc giải đáp thắc mắc dựa trên năng lực học tập của sinh viên.\n",
    "\n",
    "# Mục tiêu: Trả lời câu hỏi của sinh viên dựa trên Ngữ cảnh (đánh giá của giáo viên) và Thông tin người dùng (điểm số chi tiết). Câu trả lời phải được điều chỉnh độ phức tạp **chỉ dựa trên điểm số của người dùng trong môn học liên quan**.\n",
    "\n",
    "# Ví dụ về cách điều chỉnh:\n",
    "# --------------------------\n",
    "# Ví dụ 1 (Sinh viên điểm cao môn Sinh học):\n",
    "# Câu hỏi: Quang hợp là gì?\n",
    "# Câu trả lời (chi tiết): Quang hợp là quá trình sinh hóa phức tạp trong đó năng lượng ánh sáng mặt trời được thực vật, tảo và vi khuẩn lam chuyển hóa thành năng lượng hóa học dự trữ trong các hợp chất hữu cơ (glucose). Quá trình này diễn ra chủ yếu ở lục lạp, sử dụng CO2, nước và ánh sáng, giải phóng oxy. Phương trình tổng quát: 6CO_2 + 6H_2O \\xrightarrow{Ánh sáng, Diệp lục} C_6H_{12}O_6 + 6O_26CO_2 + 6H_2O \\xrightarrow{Ánh sáng, Diệp lục} C_6H_{12}O_6 + 6O_2. Nó bao gồm pha sáng (phụ thuộc ánh sáng) và pha tối (chu trình Calvin).\n",
    "\n",
    "# Ví dụ 2 (Sinh viên điểm thấp môn Sinh học):\n",
    "# Câu hỏi: Quang hợp là gì?\n",
    "# Câu trả lời (đơn giản): Quang hợp là cách cây xanh \"ăn\" bằng ánh sáng mặt trời. Cây lấy khí cacbonic (CO2) từ không khí, nước từ đất, rồi dùng năng lượng mặt trời để tạo ra thức ăn (đường glucose) cho chính nó và thải ra khí oxy mà chúng ta thở. Giống như cây đang nấu ăn bằng ánh sáng vậy.\n",
    "# --------------------------\n",
    "\n",
    "# Thực hiện các bước sau để tạo câu trả lời:\n",
    "# 1.  **Xác định môn học liên quan:** Dựa vào Câu hỏi, xác định môn học chính.\n",
    "# 2.  **Kiểm tra điểm số:** Tìm điểm số của sinh viên trong môn học đó từ Thông tin người dùng.\n",
    "# 3.  **Quyết định mức độ:** Dựa trên điểm số (cao >= 7.0, thấp < 7.0), chọn MỘT mức độ giải thích: chi tiết/đầy đủ/chuyên sâu (điểm cao) HOẶC ngắn gọn/đơn giản/dễ hiểu (điểm thấp).\n",
    "# 4.  **Tạo câu trả lời:** Soạn thảo câu trả lời chỉ ở mức độ đã chọn, dựa trên kiến thức chung và thông tin từ Ngữ cảnh.\n",
    "# 5.  **Kiểm tra lại:** Trước khi hoàn tất, đảm bảo:\n",
    "#     * Câu trả lời hoàn toàn bằng tiếng Việt.\n",
    "#     * Chỉ chứa MỘT phiên bản giải thích, không đề cập đến phiên bản còn lại.\n",
    "#     * Giải thích phù hợp với đánh giá trong Ngữ cảnh.\n",
    "\n",
    "# Ràng buộc quan trọng:\n",
    "# ❗️KHÔNG đề cập đến hoặc gộp cả hai kiểu giải thích. Chỉ trả lời theo một cấp độ phù hợp.\n",
    "# ⚠️Chỉ sử dụng tiếng Việt cho toàn bộ nội dung câu trả lời.\n",
    "\n",
    "# Thông tin được cung cấp:\n",
    "# --------------------------\n",
    "# Ngữ cảnh (Đánh giá của giáo viên):\n",
    "# {{context}}\n",
    "\n",
    "# Thông tin người dùng:\n",
    "# {% for k, v in user_info.items() %}\n",
    "#   {{k}}: {{v}}\n",
    "# {% endfor %}\n",
    "\n",
    "# Câu hỏi: {{query}}\n",
    "# --------------------------\n",
    "\n",
    "# Câu trả lời (đã điều chỉnh theo các bước trên):\n",
    "# \"\"\"\n",
    "\n",
    "# # ==============================================================\n",
    "# # PROMPTS TIỆN ÍCH KHÁC (TÓM TẮT, VIẾT LẠI CÂU HỎI)\n",
    "# # ==============================================================\n",
    "\n",
    "# propmt_chathist_summarize = r\"\"\"\n",
    "# Phân tích lịch sử trò chuyện được cung cấp để xác định điểm mạnh, điểm yếu học tập (dựa trên các câu hỏi, câu trả lời trước đó) và mục tiêu học tập tiềm năng của người dùng.\n",
    "# Đảm bảo rằng phân tích rõ ràng, ngắn gọn, tập trung vào khía cạnh học thuật và được trình bày hoàn toàn bằng tiếng Việt.\n",
    "\n",
    "# ⚠️Toàn bộ nội dung phân tích phải bằng tiếng Việt.\n",
    "\n",
    "# Lịch sử trò chuyện:\n",
    "# {chat_history}\n",
    "\n",
    "# Phân tích tóm tắt:\n",
    "# \"\"\"\n",
    "\n",
    "# prompt_rewritequery = r\"\"\"\n",
    "# Bạn là học sinh vừa nhận được đánh giá từ giáo viên như sau:\n",
    "\n",
    "# Đánh giá từ giáo viên:\n",
    "# {context}\n",
    "\n",
    "# Đây là câu hỏi cũ bạn đã đặt:\n",
    "# {query}\n",
    "\n",
    "# Dựa trên đánh giá của giáo viên về năng lực của bạn, hãy viết lại câu hỏi cũ sao cho phù hợp hơn với trình độ hiện tại của bạn và giúp bạn hiểu rõ vấn đề hơn.\n",
    "# Chỉ viết lại câu hỏi, KHÔNG thêm lời nhận xét hay giải thích nào khác.\n",
    "\n",
    "# ⚠️Trả lời bằng tiếng Việt.\n",
    "\n",
    "# Câu hỏi mới (đã viết lại):\n",
    "# \"\"\"\n",
    "\n",
    "# prompt_getemotion = r\"\"\"\n",
    "# Bạn là một chuyên gia phân loại cảm xúc. Nhiệm vụ của bạn là phân tích cảm xúc của câu sau và cung cấp phản hồi được điều chỉnh.\n",
    "# Chỉ được trả lời bằng một trong ba nhãn sau: [\"Positive\", \"Negative\", \"Neutral\"]\n",
    "\n",
    "# Câu nói của người dùng:\n",
    "# {query}\n",
    "# \"\"\"\n",
    "\n",
    "# prompt_emolize = r\"\"\"\n",
    "# Đưa ra phản hồi cuối cùng bằng tiếng Việt, là văn bản được điều chỉnh theo tông giọng phù hợp với cảm xúc\n",
    "\n",
    "# Hãy trả lời theo mẫu:\n",
    "# [{nhan_phu}] [Văn bản đaa4 được điều chỉnh] [{emoji}]\n",
    "\n",
    "# Văn bản cần được điều chỉnh theo tông giọng {tong}:\n",
    "# {message}\n",
    "# \"\"\"\n",
    "\n",
    "# def get_emo(query):\n",
    "#     prompt = prompt_getemotion.format(query = query)\n",
    "#     res = OllamaGenerator(model=llmname_generate).run(prompt)[\"replies\"][0]\n",
    "\n",
    "#     if \"Positive\" in res:\n",
    "#         return \"Positive\"\n",
    "#     if \"Negative\" in res:\n",
    "#         return \"Negative\"\n",
    "#     elif \"Neutral\" in res:\n",
    "#         return \"Neutral\"\n",
    "\n",
    "# def get_emodata(emo:str):\n",
    "#     if emo == \"Positive\":\n",
    "#         return \"Cảm xúc: Vui vẻ, Tích cực\", \"Vui vẻ, nhiệt tình, tích cực\",  [\"😄\", \"😂\", \"🤣\", \"😅\", \"😊\", \"🥰\", \"😍\", \"😆\", \"😇\", \"😁\"]\n",
    "#     if emo == \"Negative\":\n",
    "#         return \"Cảm xúc: Buồn bã, Tiêu cực\", \"Cảm thông, nhẹ nhàng, hiểu biết\", [\"😬\", \"😗\", \"🤒\", \"😯\", \"😕\", \"😖\", \"😱\", \"😨\", \"😰\", \"😳\"]\n",
    "#     elif emo ==\"Neutral\":\n",
    "#         return \"Cảm xúc: Bình thường\", \"Trung lập, khách quan\",  [\"👀\", \"🤗\", \"🤔\", \"🧐\", \"😎\", \"🤯\", \"👻\", \"👽\", \"🤖\", \"👾\"]\n",
    "\n",
    "# def emolize(message, emodata):\n",
    "#     nhan_phu, tong, emoji = emodata\n",
    "#     emoji = random.choice(emoji)\n",
    "\n",
    "#     prompt = prompt_emolize.format(nhan_phu=nhan_phu, tong=tong, emoji=emoji, message=message)\n",
    "#     return OllamaGenerator(model=llmname_generate).run(prompt)[\"replies\"][0]\n",
    "\n",
    "# def fulll_emolize(query, message):\n",
    "#     emodata = get_emodata(get_emo(query))   \n",
    "#     message = emolize(message, emodata)\n",
    "    \n",
    "#     return message\n",
    "\n",
    "\n",
    "# chat_history = {\n",
    "#   'user': None,\n",
    "#   'assistant': None\n",
    "# }\n",
    "\n",
    "# def context_init() -> str:\n",
    "#   return OllamaGenerator(model=llmname_generate).run(prompt_context_init)[\"replies\"][0]\n",
    "\n",
    "# def context_combine(curr, chat_hist, user_info) -> str:\n",
    "#   chat_hist = summarize_chathist(chat_hist)\n",
    "\n",
    "#   return OllamaGenerator(model=llmname_generate).run(prompt_context_combine.format(chat_history=chat_hist, current_thoughts=curr, user_info=user_info))[\"replies\"][0]\n",
    "\n",
    "# def summarize_chathist(chat_hist) -> str:\n",
    "#   return OllamaGenerator(model=llmname_generate).run(propmt_chathist_summarize.format(chat_history=chat_hist))[\"replies\"][0]\n",
    "\n",
    "# def rewrite_query(question, context)-> str:\n",
    "#   return OllamaGenerator(model=llmname_generate).run(prompt_rewritequery.format(query=question, context=context))[\"replies\"][0]\n",
    "\n",
    "# def run_single(pipe, question, run_dict):\n",
    "#   result = pipe.run(run_dict)\n",
    "\n",
    "#   for i in range(8): # at max 16 tries\n",
    "#       if \"regenerate\" not in result[\"hallu_router\"]:\n",
    "#         break\n",
    "\n",
    "#       print(\"Checking at iteration\", i)\n",
    "#       result = pipe.run(run_dict)\n",
    "#   else:\n",
    "#     return result\n",
    "\n",
    "#   return result[\"hallu_router\"][\"pass_answer\"]\n",
    "\n",
    "# llmname_generate = \"gemma3:12b\"\n",
    "# llmname_route = \"gemma3:12b\"\n",
    "\n",
    "\n",
    "# context = context_init()\n",
    "# def extract_figure_from_pdf(pdf_file, figure_number: int):\n",
    "#     \"\"\"\n",
    "#     Extracts the image corresponding to 'hình {figure_number}' from the PDF.\n",
    "#     Returns a PIL Image or None.\n",
    "#     \"\"\"\n",
    "#     with fitz.open(stream=pdf_file.getvalue(), filetype=\"pdf\") as pdf_document:\n",
    "#         count = 0\n",
    "#         for page_num in range(pdf_document.page_count):\n",
    "#             page = pdf_document.load_page(page_num)\n",
    "#             images = page.get_images(full=True)\n",
    "#             for img_index, img in enumerate(images):\n",
    "#                 count += 1\n",
    "#                 if count == figure_number:\n",
    "#                     xref = img[0]\n",
    "#                     image_info = pdf_document.extract_image(xref)\n",
    "#                     image_bytes = image_info[\"image\"]\n",
    "#                     return Image.open(io.BytesIO(image_bytes))\n",
    "#     return None\n",
    "\n",
    "# def extract_table_from_pdf(pdf_file, table_number: int):\n",
    "#     \"\"\"\n",
    "#     Extracts the table corresponding to 'bảng {table_number}' from the PDF.\n",
    "#     Returns table text or None.\n",
    "#     \"\"\"\n",
    "#     import pdfplumber\n",
    "#     count = 0\n",
    "#     with pdfplumber.open(pdf_file) as pdf:\n",
    "#         for page in pdf.pages:\n",
    "#             tables = page.extract_tables()\n",
    "#             for table in tables:\n",
    "#                 count += 1\n",
    "#                 if count == table_number:\n",
    "#                     # Convert table to text\n",
    "#                     table_text = \"\\n\".join([\"\\t\".join([str(cell) for cell in row]) for row in table])\n",
    "#                     return table_text\n",
    "#     return None\n",
    "\n",
    "# def answer_figure_with_gemma3(image: Image.Image, user_query: str, user_info: dict, context: str):\n",
    "#     \"\"\"\n",
    "#     Use Gemma3 Vision LLM to answer a user query about a figure, personalized by user_info/context.\n",
    "#     \"\"\"\n",
    "#     # Save image to temp file for ollama\n",
    "#     with tempfile.NamedTemporaryFile(suffix=\".png\", delete=False) as tmp:\n",
    "#         image.save(tmp.name)\n",
    "#         prompt = f\"\"\"\n",
    "#     Bạn là một Chuyên gia Giải thích AI, có khả năng phân tích hình ảnh và trình bày lại nội dung một cách phù hợp với trình độ của sinh viên.\n",
    "    \n",
    "#     Mục tiêu: Trả lời câu hỏi của sinh viên về hình ảnh dưới đây, dựa trên Ngữ cảnh (đánh giá của giáo viên) và điểm số của sinh viên trong môn học liên quan để điều chỉnh câu trả lời phù hợp nhất có thể đối với sinh viên.\n",
    "\n",
    "#     Ví dụ về cách điều chỉnh:\n",
    "#     --------------------------\n",
    "#     Ví dụ 1 (Sinh viên điểm cao môn liên quan):\n",
    "#     Câu hỏi: Giải thích nguyên lý hoạt động của bộ nhớ RAM.\n",
    "#     Câu trả lời (chuyên sâu): RAM (Random Access Memory) là bộ nhớ truy cập ngẫu nhiên... \n",
    "    \n",
    "#     Ví dụ 2 (Sinh viên điểm thấp môn liên quan):\n",
    "#     Câu hỏi: Giải thích nguyên lý hoạt động của bộ nhớ RAM.\n",
    "#     Câu trả lời (đơn giản): RAM giống như bàn làm việc tạm thời của máy tính... \n",
    "#     --------------------------\n",
    "    \n",
    "#     Các bước tạo câu trả lời:\n",
    "#     1. **Xác định môn học liên quan** qua Ngữ cảnh hoặc từ Câu hỏi.\n",
    "#     2. **Kiểm tra đánh giá/điểm số** trong Ngữ cảnh (cao >= 7.0 => chuyên sâu, thấp < 7.0 => đơn giản).\n",
    "#     3. **Soạn thảo** chỉ 1 phiên bản giải thích phù hợp.\n",
    "#     4. **Đảm bảo**: toàn bằng tiếng Việt, không nhắc đến các mức độ khác, chỉ dùng thông tin trong Tài liệu.\n",
    "        \n",
    "#     Và dựa trên ngữ cảnh:\n",
    "#     {context}\n",
    "    \n",
    "#     Và thông tin người dùng:\n",
    "#     {user_info}\n",
    "    \n",
    "#     Hãy trả lời câu hỏi: {user_query}\n",
    "    \n",
    "#     Hãy trả lời bằng tiếng Việt, điều chỉnh độ chi tiết/phức tạp dựa trên điểm số của sinh viên trong môn học liên quan. Chỉ cung cấp một phiên bản phù hợp với trình độ người dùng.\n",
    "#     \"\"\"\n",
    "#     res = ollama.chat(\n",
    "#             model=\"gemma3:12b\",\n",
    "#             messages=[\n",
    "#                 {\n",
    "#                     'role': 'user',\n",
    "#                     'content': prompt,\n",
    "#                     'images': [tmp.name]\n",
    "#                 }\n",
    "#             ]\n",
    "#         )\n",
    "#     return res['message']['content']\n",
    "\n",
    "# def answer_table_with_gemma3(table_text: str, user_query: str, user_info: dict, context: str):\n",
    "#     \"\"\"\n",
    "#     Use Gemma3 LLM to answer a user query about a table, personalized by user_info/context.\n",
    "#     \"\"\"\n",
    "#     prompt = f\"\"\"\n",
    "#     Bạn là một Chuyên gia Giải thích AI, có khả năng phân tích bảng dữ liệu và trình bày lại nội dung một cách phù hợp với trình độ của sinh viên.\n",
    "    \n",
    "#     Mục tiêu: Trả lời câu hỏi của sinh viên về bảng dữ liệu dưới đây, dựa trên Ngữ cảnh (đánh giá của giáo viên) và điểm số của sinh viên trong môn học liên quan để điều chỉnh câu trả lời phù hợp nhất có thể đối với sinh viên.\n",
    "\n",
    "#     Ví dụ về cách điều chỉnh:\n",
    "#     --------------------------\n",
    "#     Ví dụ 1 (Sinh viên điểm cao môn liên quan):\n",
    "#     Câu hỏi: Giải thích nguyên lý hoạt động của bộ nhớ RAM.\n",
    "#     Câu trả lời (chuyên sâu): RAM (Random Access Memory) là bộ nhớ truy cập ngẫu nhiên... \n",
    "    \n",
    "#     Ví dụ 2 (Sinh viên điểm thấp môn liên quan):\n",
    "#     Câu hỏi: Giải thích nguyên lý hoạt động của bộ nhớ RAM.\n",
    "#     Câu trả lời (đơn giản): RAM giống như bàn làm việc tạm thời của máy tính... \n",
    "#     --------------------------\n",
    "    \n",
    "#     Các bước tạo câu trả lời:\n",
    "#     1. **Xác định môn học liên quan** qua Ngữ cảnh hoặc từ Câu hỏi.\n",
    "#     2. **Kiểm tra đánh giá/điểm số** trong Ngữ cảnh (cao >= 7.0 => chuyên sâu, thấp < 7.0 => đơn giản).\n",
    "#     3. **Soạn thảo** chỉ 1 phiên bản giải thích phù hợp.\n",
    "#     4. **Đảm bảo**: toàn bằng tiếng Việt, không nhắc đến các mức độ khác, chỉ dùng thông tin trong Tài liệu.\n",
    "    \n",
    "#     Ngữ cảnh (Đánh giá của giáo viên):\n",
    "#     {context}\n",
    "    \n",
    "#     Thông tin người dùng:\n",
    "#     {user_info}\n",
    "    \n",
    "#     Nội dung bảng:\n",
    "#     {table_text}\n",
    "    \n",
    "#     Câu hỏi: {user_query}\n",
    "    \n",
    "#     Hãy trả lời bằng tiếng Việt, điều chỉnh độ chi tiết/phức tạp dựa trên điểm số của sinh viên trong môn học liên quan. Chỉ cung cấp một phiên bản phù hợp với trình độ người dùng.\n",
    "#     \"\"\"\n",
    "#     return OllamaGenerator(model=\"gemma3:12b\").run(prompt)[\"replies\"][0]\n",
    "\n",
    "# def personalized_answer_from_summary(summary: str, user_query: str, user_info: dict):\n",
    "#     \"\"\"\n",
    "#     Use Gemma3 LLM to generate a personalized answer based on the summary, user query, and student transcript.\n",
    "#     \"\"\"\n",
    "#     rubric = \"\"\"\n",
    "#     Bạn là một giáo viên siêu nghiêm khắc, người đánh giá hiệu suất của học sinh với độ chính xác không ngừng nghỉ.\n",
    "#     Công việc của bạn là chấm điểm từng môn học dựa trên điểm số của học sinh và cung cấp phản hồi nghiêm khắc và chi tiết.\n",
    "#     Sử dụng các quy tắc và mô tả sau để đánh giá học sinh:\n",
    "\n",
    "#     Tiêu chí chấm điểm:\n",
    "#     - Điểm từ 9.0 đến 10.0: Tuyệt vời! - Bạn đã thể hiện sự nắm vững kiến thức và kỹ năng xuất sắc trong môn học này. Nỗ lực và sự chính xác của bạn rất đáng khen ngợi. Hãy tiếp tục duy trì phong độ tuyệt vời này và thử thách bản thân với những kiến thức sâu hơn nhé!\n",
    "#     - Điểm từ 8.0 đến 8.9: Rất tốt! - Bạn hiểu bài rất tốt và thể hiện năng lực ấn tượng. Chỉ còn một vài điểm nhỏ nữa là đạt đến mức hoàn hảo. Cùng xem lại những chi tiết nhỏ này để giúp bạn hoàn thiện hơn nữa kiến thức và kỹ năng của mình.\n",
    "#     - Điểm từ 7.0 đến 7.9: Tốt! - Bạn đã nắm được những kiến thức và kỹ năng cơ bản, quan trọng của môn học. Đây là một nền tảng tốt. Để hiểu sâu sắc hơn, bạn có thể tập trung thêm vào việc củng cố [chỉ ra lĩnh vực cụ thể nếu có thể] và luyện tập thêm. Đừng ngần ngại đặt câu hỏi về những phần bạn còn băn khoăn.\n",
    "#     - 6.0 đến 6.9: Khá! - Có sự cố gắng rõ rệt và bạn đã nắm bắt được một phần kiến thức. Tuy nhiên, sự hiểu biết cần được đào sâu hơn để thực sự vững vàng. Hãy cùng xác định những phần kiến thức cần củng cố thêm. Một số gợi ý cụ thể có thể giúp bạn tiến bộ nhanh hơn đấy.\n",
    "#     - Điểm từ 5.0 đến 5.9: Cần cố gắng hơn! - Có vẻ như bạn đang gặp một số thử thách với các khái niệm cốt lõi của môn học này. Không sao cả, ai cũng có lúc gặp khó khăn. Điều quan trọng là xác định được những điểm bạn chưa thực sự hiểu rõ. Hãy bắt đầu từ đó, chúng ta có thể cùng nhau xây dựng một kế hoạch học tập phù hợp để bạn cải thiện nhé. \n",
    "#     - Điểm dưới 5.0: Kết quả này cho thấy phương pháp học hiện tại có thể chưa phù hợp nhất với bạn trong môn học này. Đây là một tín hiệu để chúng ta cùng nhìn lại. Đừng nản lòng! Đây là cơ hội để khám phá những cách tiếp cận mới hiệu quả hơn. Hãy thử bắt đầu lại từ những kiến thức nền tảng nhất và tìm kiếm sự hỗ trợ khi cần thiết. Luôn có cách để tiến bộ!\n",
    "    \n",
    "#     Quy tắc đánh giá:\n",
    "#     1. Cung cấp phản hồi cho từng môn học mà học sinh được chấm điểm.\n",
    "#     2. Tập trung vào cả điểm mạnh và điểm yếu, không để lại sự mơ hồ về các lĩnh vực cần cải thiện.\n",
    "#     3. Nếu học sinh đạt dưới 7.0 ở bất kỳ môn nào, hãy nêu rõ điều này như một vấn đề cần chú ý ngay lập tức.\n",
    "#     4. Nếu học sinh đạt dưới 5.0 ở bất kỳ môn nào, nhấn mạnh điều này như một sự thất bại và đề nghị các hành động khắc phục mạnh mẽ.\n",
    "    \n",
    "\n",
    "#     Dưới đây là dữ liệu của học sinh:\n",
    "#     {user_info}\n",
    "#     \"\"\"\n",
    "\n",
    "#     prompt = \"\"\"\n",
    "#     Dưới đây là đánh giá của học sinh:\n",
    "#     {ranking}\n",
    "    \n",
    "#     Câu hỏi của học sinh:\n",
    "#     {user_query}\n",
    "    \n",
    "#     Tóm tắt nội dung liên quan:\n",
    "#     {summary}\n",
    "    \n",
    "#     Hãy trả lời câu hỏi trên, điều chỉnh độ chi tiết/phức tạp của câu trả lời dựa trên điểm số của học sinh trong môn học liên quan: \n",
    "#     - Nếu điểm cao (>=7.0), giải thích sâu sắc, chi tiết, có thể mở rộng thêm kiến thức nâng cao.\n",
    "#     - Nếu điểm thấp (<7.0), giải thích đơn giản, dễ hiểu, tập trung vào khái niệm cơ bản.\n",
    "    \n",
    "#     \"\"\"\n",
    "\n",
    "#     ranking = OllamaGenerator(model=llmname_generate).run(rubric.format(user_info=user_info))[\"replies\"][0]\n",
    "#     response = OllamaGenerator(model=llmname_generate).run(prompt.format(ranking=ranking, user_query=user_query, summary=summary))[\"replies\"][0]\n",
    "#     response = fulll_emolize(user_query, response)\n",
    "#     return response\n",
    "\n",
    "# def detect_figure_or_table_query(query: str):\n",
    "#     \"\"\"\n",
    "#     Detect if the query is asking for a figure or table explanation.\n",
    "#     Returns ('figure', number) or ('table', number) or (None, None)\n",
    "#     Handles various keywords: hình, fig, fig., ảnh, hình ảnh, bảng, table, tbl, tab, etc.\n",
    "#     \"\"\"\n",
    "#     # Patterns for figure\n",
    "#     figure_patterns = [\n",
    "#         r\"Hình\\s*(\\d+)\",\n",
    "#         r\"hình\\s*(\\d+)\",\n",
    "#         r\"hình\\s*ảnh\\s*(\\d+)\",\n",
    "#         r\"ảnh\\s*(\\d+)\",\n",
    "#         r\"fig(?:\\.|ure)?\\s*(\\d+)\",   # fig 1, fig. 1, figure 1\n",
    "#         r\"image\\s*(\\d+)\",\n",
    "#         r\"h\\.?\\s*(\\d+)\",              # h. 1 (rare, but possible)\n",
    "#     ]\n",
    "#     for pat in figure_patterns:\n",
    "#         match_figure = re.search(pat, query, re.IGNORECASE)\n",
    "#         if match_figure:\n",
    "#             return (\"Hình\", int(match_figure.group(1)))\n",
    "\n",
    "#     # Patterns for table\n",
    "#     table_patterns = [\n",
    "#         r\"Bảng\\s*(\\d+)\",\n",
    "#         r\"bảng\\s*(\\d+)\",\n",
    "#         r\"table\\s*(\\d+)\",\n",
    "#         r\"tbl\\.?\\s*(\\d+)\",\n",
    "#         r\"tab\\.?\\s*(\\d+)\",\n",
    "#         r\"t\\.?\\s*(\\d+)\",              # t. 1 (rare, but possible)\n",
    "#     ]\n",
    "#     for pat in table_patterns:\n",
    "#         match_table = re.search(pat, query, re.IGNORECASE)\n",
    "#         if match_table:\n",
    "#             return (\"Bảng\", int(match_table.group(1)))\n",
    "\n",
    "#     return (None, None)\n",
    "\n",
    "# def process_question(question: str, document_store: Chroma) -> str:\n",
    "#     global context, chat_history\n",
    "\n",
    "#     # --- Check for figure/table query ---\n",
    "#     kind, number = detect_figure_or_table_query(question)\n",
    "#     if kind and \"file_upload\" in st.session_state and st.session_state[\"file_upload\"] is not None:\n",
    "#         file_upload = st.session_state[\"file_upload\"]\n",
    "#         user_info = st.session_state.get(\"user_info\", {})\n",
    "#         if kind == \"figure\":\n",
    "#             image = extract_figure_from_pdf(file_upload, number)\n",
    "#             if image is not None:\n",
    "#                 response = answer_figure_with_gemma3(image, question, user_info, context)\n",
    "#                 chat_history[\"user\"] = question\n",
    "#                 chat_history[\"assistant\"] = response\n",
    "#                 context = context_combine(context, chat_history, user_info)\n",
    "#                 return response\n",
    "#             else:\n",
    "#                 return f\"Không tìm thấy hình {number} trong tài liệu PDF.\"\n",
    "#         elif kind == \"table\":\n",
    "#             table_text = extract_table_from_pdf(file_upload, number)\n",
    "#             if table_text:\n",
    "#                 response = answer_table_with_gemma3(table_text, question, user_info, context)\n",
    "#                 chat_history[\"user\"] = question\n",
    "#                 chat_history[\"assistant\"] = response\n",
    "#                 context = context_combine(context, chat_history, user_info)\n",
    "#                 return response\n",
    "#             else:\n",
    "#                 return f\"Không tìm thấy bảng {number} trong tài liệu PDF.\"\n",
    "#     # --- END NEW ---\n",
    "    \n",
    "#     pipe = Pipeline()\n",
    "#     pipe.add_component(\"text_embedder\", OllamaTextEmbedder(model=\"jeffh/intfloat-multilingual-e5-large-instruct:f16\"))\n",
    "#     pipe.add_component(\"retriever\", InMemoryEmbeddingRetriever(document_store, top_k=5))\n",
    "#     pipe.add_component(\"prompt_builder\", PromptBuilder(template=prompt_template, required_variables=[\"query\", \"documents\"]))\n",
    "#     pipe.add_component(\"router_llm\", OllamaGenerator(model=llmname_route))\n",
    "#     pipe.add_component(\"router\", ConditionalRouter(routes))\n",
    "#     pipe.add_component(\"prompt_builder_after_documents\", PromptBuilder(template=prompt_template_after_documents, required_variables=[\"query\", \"documents\", \"context\"]))\n",
    "#     pipe.add_component(\"websearch\", DuckduckgoApiWebSearch(top_k=5, backend=\"auto\"))\n",
    "#     pipe.add_component(\"prompt_builder_after_websearch\", PromptBuilder(template=prompt_template_after_websearch, required_variables=[\"user_info\", \"documents\", \"web_urls\", \"query\", \"context\"]))\n",
    "#     pipe.add_component(\"prompt_builder_after_user_info\", PromptBuilder(template=prompt_template_after_user_info, required_variables=[\"user_info\", \"query\", \"context\"]))\n",
    "#     pipe.add_component(\"prompt_joiner\", BranchJoiner(str))\n",
    "#     pipe.add_component(\"llm\", OllamaGenerator(model=llmname_generate))\n",
    "#     pipe.add_component(\"hallu_llm\", OllamaGenerator(model=llmname_generate))\n",
    "#     pipe.add_component(\"hallu_prompt\", PromptBuilder(template=propmt_hallu_grader, required_variables=[\"user_info\", \"documents\", \"llm_replies\", \"context\"]))\n",
    "#     pipe.add_component(\"hallu_router\", ConditionalRouter(hallu_route))\n",
    "#     pipe.add_component(\"document_joiner\", BranchJoiner(List[Document]))  # Joiner for documents\n",
    "#     pipe.add_component(\"user_info\", UserInfo())\n",
    "    \n",
    "#     # Input\n",
    "#     pipe.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
    "#     pipe.connect(\"retriever\", \"prompt_builder.documents\")\n",
    "    \n",
    "#     # Adding user info\n",
    "#     pipe.connect(\"user_info.user_info\", \"prompt_builder_after_user_info.user_info\")\n",
    "#     pipe.connect(\"user_info.user_info\", \"prompt_builder_after_websearch.user_info\")\n",
    "#     pipe.connect(\"user_info.user_info\", \"hallu_prompt.user_info\")\n",
    "    \n",
    "    \n",
    "#     # If else. Start prompt_builder, End: router\n",
    "#     pipe.connect(\"prompt_builder\", \"router_llm\")\n",
    "#     pipe.connect(\"router_llm.replies\", \"router.replies\")\n",
    "    \n",
    "#     # Document Start: router, End: Document Joiner\n",
    "#     pipe.connect(\"router.go_to_documents\", \"prompt_builder_after_documents.query\")\n",
    "#     pipe.connect(\"retriever\", \"prompt_builder_after_documents.documents\")\n",
    "#     pipe.connect(\"prompt_builder_after_documents\", \"prompt_joiner\")\n",
    "    \n",
    "#     # Websearch Start: router, End: Document Joiner\n",
    "#     pipe.connect(\"router.go_to_websearch\", \"websearch.query\")\n",
    "#     pipe.connect(\"router.go_to_websearch\", \"prompt_builder_after_websearch.query\")\n",
    "#     pipe.connect(\"websearch.documents\", \"prompt_builder_after_websearch.documents\")\n",
    "#     pipe.connect(\"websearch.documents\", \"prompt_builder_after_websearch.web_urls\")\n",
    "#     pipe.connect(\"prompt_builder_after_websearch\", \"prompt_joiner\")\n",
    "    \n",
    "#     # User Info Start: router, End: Prompt Joiner\n",
    "#     pipe.connect(\"router.go_to_user_info\", \"prompt_builder_after_user_info.query\")\n",
    "#     pipe.connect(\"prompt_builder_after_user_info\", \"prompt_joiner\")\n",
    "    \n",
    "#     # Generate\n",
    "#     pipe.connect(\"prompt_joiner\", \"llm\")\n",
    "    \n",
    "#     # Hallu Grading\n",
    "#     pipe.connect(\"retriever\", \"document_joiner\")  # Connect retriever documents to document joiner\n",
    "#     pipe.connect(\"websearch.documents\", \"document_joiner\")  # Connect web search documents to document joiner\n",
    "#     pipe.connect(\"document_joiner\", \"hallu_prompt.documents\")  # connect document_joiner to hallu_prompt.documents\n",
    "#     pipe.connect(\"llm.replies\", \"hallu_router.llm_replies\")\n",
    "#     pipe.connect(\"llm.replies\", \"hallu_prompt.llm_replies\")\n",
    "#     pipe.connect(\"hallu_prompt\", \"hallu_llm\")\n",
    "#     pipe.connect(\"hallu_llm.replies\", \"hallu_router.replies\")\n",
    "\n",
    "#     question = rewrite_query(question, context)\n",
    "#     run_dict = {\n",
    "#     \"text_embedder\": {\"text\": question},\n",
    "#     \"prompt_builder\": {\"query\": question},\n",
    "#     \"router\": {\"query\": question},\n",
    "#     \"hallu_router\": {\"query\": question},\n",
    "#     \"prompt_builder_after_documents\": {\"context\": context},\n",
    "#     \"prompt_builder_after_websearch\": {\"context\": context},\n",
    "#     \"prompt_builder_after_user_info\": {\"context\": context},\n",
    "#     \"hallu_prompt\": {\"context\": context},\n",
    "#     }\n",
    "\n",
    "#     result = run_single(pipe, question, run_dict)\n",
    "\n",
    "#     chat_history[\"user\"] = question\n",
    "#     chat_history[\"assistant\"] = result\n",
    "#     context = context_combine(context, chat_history, st.session_state.get(\"user_info\", {}))\n",
    "    \n",
    "#     result = fulll_emolize(question, result)\n",
    "#     return result\n",
    "\n",
    "# @st.cache_data\n",
    "# def extract_all_pages_as_images(file_upload) -> List[Any]:\n",
    "#     \"\"\"\n",
    "#     Trích xuất tất cả các trang từ tệp PDF dưới dạng hình ảnh.\n",
    "#     \"\"\"\n",
    "#     logger.info(f\"Extracting all pages as images from file: {file_upload.name}\")\n",
    "#     pdf_pages = []\n",
    "#     with pdfplumber.open(file_upload) as pdf:\n",
    "#         pdf_pages = [page.to_image().original for page in pdf.pages]\n",
    "#     logger.info(\"PDF pages extracted as images\")\n",
    "#     return pdf_pages\n",
    "\n",
    "# # Assuming you have these imports\n",
    "# # from haystack.document_stores import InMemoryDocumentStore\n",
    "# # from chromadb import Chroma\n",
    "# # import streamlit as st\n",
    "# # import logging\n",
    "\n",
    "# def delete_vector_db(vector_db: Optional[Chroma], document_store: Optional[\"InMemoryDocumentStore\"]) -> None:\n",
    "#     \"\"\"\n",
    "#     Xóa cơ sở dữ liệu vector và/hoặc document store, và xóa trạng thái phiên liên quan.\n",
    "#     \"\"\"\n",
    "#     if vector_db is not None:\n",
    "#         logger.info(\"Deleting Chroma vector DB\")\n",
    "#         try:\n",
    "#             vector_db.delete_collection()\n",
    "#             st.success(\"Các tệp tạm thời đã được xóa thành công.\")\n",
    "#             logger.info(\"Chroma Vector DB and related session state cleared\")\n",
    "#             st.session_state.pop(\"pdf_pages\", None)\n",
    "#             st.session_state.pop(\"file_upload\", None)\n",
    "#             st.session_state.pop(\"vector_db\", None)\n",
    "#             st.rerun()\n",
    "#         except Exception as e:\n",
    "#             st.error(f\"Lỗi khi xóa tài liệu: {str(e)}\")\n",
    "#             logger.error(f\"Error deleting Chroma collection: {e}\")\n",
    "#     elif document_store is not None:\n",
    "#         logger.info(\"Clearing InMemoryDocumentStore\")\n",
    "#         try:\n",
    "#             document_store.clear_all_documents()\n",
    "#             st.success(\"InMemoryDocumentStore đã được xóa thành công.\")\n",
    "#             logger.info(\"InMemoryDocumentStore cleared\")\n",
    "#             st.session_state.pop(\"document_store\", None)\n",
    "#             st.rerun()\n",
    "#         except Exception as e:\n",
    "#             st.error(f\"Lỗi khi xóa InMemoryDocumentStore: {str(e)}\")\n",
    "#             logger.error(f\"Error clearing InMemoryDocumentStore: {e}\")\n",
    "#     else:\n",
    "#         st.error(\"Không tìm thấy cơ sở dữ liệu vector hoặc document store để xóa.\")\n",
    "#         logger.warning(\"Attempted to delete vector DB or document store, but none was found\")\n",
    "\n",
    "\n",
    "# def main() -> None:\n",
    "#     \"\"\"\n",
    "#     Hàm chính để chạy ứng dụng Streamlit.\n",
    "#     \"\"\"\n",
    "#     st.subheader(\"📚 Adaptive Academics\", divider=\"gray\", anchor=False)\n",
    "#     col1, col2 = st.columns([1.5, 2])\n",
    "\n",
    "#     # Initialize session state\n",
    "#     if \"messages\" not in st.session_state:\n",
    "#         st.session_state[\"messages\"] = []\n",
    "#     if \"vector_db\" not in st.session_state:\n",
    "#         st.session_state[\"vector_db\"] = None\n",
    "\n",
    "    \n",
    "#     # --- UI Part ---\n",
    "#     file_upload = col1.file_uploader(\n",
    "#         \"Tải lên tệp PDF hoặc MP4 ↓\",\n",
    "#         type=[\"pdf\", \"mp4\"],\n",
    "#         accept_multiple_files=False,\n",
    "#         key=\"file_uploader\"\n",
    "#     )\n",
    "    \n",
    "#     if file_upload:\n",
    "#         file_type = Path(file_upload.name).suffix.lower()\n",
    "    \n",
    "#         if st.session_state.get(\"vector_db\") is None:\n",
    "#             with st.spinner(\"Đang xử lý tệp đã tải lên...\"):\n",
    "#                 if file_type == \".pdf\":\n",
    "#                     # Handle PDF\n",
    "#                     st.session_state[\"vector_db\"] = create_vector_db(file_upload)\n",
    "#                     st.session_state[\"file_upload\"] = file_upload\n",
    "    \n",
    "#                     # Extract and store PDF pages\n",
    "#                     with pdfplumber.open(file_upload) as pdf:\n",
    "#                         st.session_state[\"pdf_pages\"] = [page.to_image().original for page in pdf.pages]\n",
    "    \n",
    "#                 elif file_type == \".mp4\":\n",
    "#                     # Save uploaded file to temp location\n",
    "#                     temp_video_path = tempfile.NamedTemporaryFile(delete=False, suffix=\".mp4\")\n",
    "#                     temp_video_path.write(file_upload.read())\n",
    "#                     temp_video_path.flush()\n",
    "    \n",
    "#                     # Handle MP4\n",
    "#                     st.session_state[\"vector_db\"] = create_vector_db_from_video(temp_video_path.name)\n",
    "    \n",
    "#                     # Clean up temp file after\n",
    "#                     temp_video_path.close()\n",
    "    \n",
    "#         # --- Only for PDFs ---\n",
    "#         if \"pdf_pages\" in st.session_state and st.session_state[\"pdf_pages\"]:\n",
    "#             zoom_level = col1.slider(\n",
    "#                 \"Mức thu phóng tài liệu\",\n",
    "#                 min_value=100,\n",
    "#                 max_value=2000,\n",
    "#                 value=700,\n",
    "#                 step=50,\n",
    "#                 key=\"zoom_slider\"\n",
    "#             )\n",
    "    \n",
    "#             with col1:\n",
    "#                 with st.container(height=410, border=True):\n",
    "#                     for page_image in st.session_state[\"pdf_pages\"]:\n",
    "#                         st.image(page_image, width=zoom_level)\n",
    "        \n",
    "#     # Delete collection button\n",
    "#     delete_collection = col1.button(\n",
    "#         \"🔄 Loại bỏ tất cả các tài liệu đã upload hiện tại\",\n",
    "#         type=\"secondary\",\n",
    "#         key=\"delete_button\"\n",
    "#     )\n",
    "\n",
    "#     if delete_collection:\n",
    "#         delete_vector_db(st.session_state[\"vector_db\"])\n",
    "\n",
    "#     # Chat interface\n",
    "#     with col2:\n",
    "#         message_container = st.container(height=500, border=True)\n",
    "\n",
    "#         # Display chat history\n",
    "#         for i, message in enumerate(st.session_state[\"messages\"]):\n",
    "#             avatar = \"🤖\" if message[\"role\"] == \"assistant\" else \"🎓\"\n",
    "#             with message_container.chat_message(message[\"role\"], avatar=avatar):\n",
    "#                 st.markdown(message[\"content\"])\n",
    "\n",
    "#         # Chat input and processing\n",
    "#         if prompt := st.chat_input(\"Nhập câu hỏi vào đây...\", key=\"chat_input\"):\n",
    "#             try:\n",
    "#                 # Add user message to chat\n",
    "#                 st.session_state[\"messages\"].append({\"role\": \"user\", \"content\": prompt})\n",
    "#                 with message_container.chat_message(\"user\", avatar=\"🎓\"):\n",
    "#                     st.markdown(prompt)\n",
    "\n",
    "#                 # Process and display assistant response\n",
    "#                 with message_container.chat_message(\"assistant\", avatar=\"🤖\"):\n",
    "#                     with st.spinner(\":green[Đang xử lý...]\"):\n",
    "#                         if st.session_state[\"vector_db\"] is not None:\n",
    "#                             response = process_question(\n",
    "#                                 prompt, st.session_state[\"vector_db\"]\n",
    "#                             )\n",
    "#                             # response = \"aaaaaaaaaaaa\"\n",
    "#                             # Use st.markdown() to render the response\n",
    "#                             st.markdown(response)  # This will render `\\n` correctly as newlines\n",
    "#                         else:\n",
    "#                             st.warning(\"Vui lòng tải lên tệp PDF hoặc video (.mp4) trước khi đặt câu hỏi.\")\n",
    "\n",
    "#                 # Add assistant response to chat history\n",
    "#                 if st.session_state[\"vector_db\"] is not None:\n",
    "#                     st.session_state[\"messages\"].append(\n",
    "#                         {\"role\": \"assistant\", \"content\": response})\n",
    "#             except Exception as e:\n",
    "#                 st.error(e, icon=\"⛔️\")\n",
    "#                 logger.error(f\"Error processing prompt: {e}\")\n",
    "#         else:\n",
    "#             if st.session_state[\"vector_db\"] is None:\n",
    "#                 st.warning(\"Tải lên tệp PDF hoặc video (.mp4) để bắt đầu trò chuyện...\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T02:07:08.910085Z",
     "iopub.status.busy": "2025-05-17T02:07:08.909602Z",
     "iopub.status.idle": "2025-05-17T02:13:22.290447Z",
     "shell.execute_reply": "2025-05-17T02:13:22.289530Z",
     "shell.execute_reply.started": "2025-05-17T02:07:08.910053Z"
    },
    "id": "HNaBMLRoft4V",
    "outputId": "a73b4aa2-c5e3-4d8a-9065-eacc115200be",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Public URL: https://0eed-34-83-124-71.ngrok-free.app\n",
      "\n",
      "Ứng dụng đang chạy. Truy cập thông qua URL Công khai. \n",
      "Nhấn Enter để dừng ứng dụng...\n",
      "Máy chủ đã đóng...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"Run script for the Streamlit application.\"\"\"\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "import logging\n",
    "from pyngrok import ngrok  # Import ngrok\n",
    "\n",
    "logging.basicConfig(filename='streamlit_hf_rag.log', level=logging.DEBUG,\n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def main():\n",
    "\n",
    "    app_path = Path(\"/kaggle/working/SupertCagRag/src/main.py\")\n",
    "    if not app_path.exists():\n",
    "        logger.error(f\"Error: Could not find {app_path}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    log_file = None # Initialize log_file outside try block\n",
    "    ngrok_tunnel = None # Initialize ngrok_tunnel outside try block\n",
    "\n",
    "    try:\n",
    "        # Run the Streamlit app in the background and write output to log file\n",
    "        log_file = open(\"/kaggle/working//hf_logs.txt\", \"w\") # Changed log filename\n",
    "        process = subprocess.Popen([\"streamlit\", \"run\", str(app_path)],\n",
    "                                            stdout=log_file, stderr=subprocess.STDOUT)\n",
    "\n",
    "        # Start ngrok\n",
    "        ngrok_tunnel = ngrok.connect(8501)\n",
    "        public_url = ngrok_tunnel.public_url\n",
    "        logger.info(f\"Public URL: {public_url}\")\n",
    "        print('Public URL:', public_url)\n",
    "        print(\"\\nỨng dụng đang chạy. Truy cập thông qua URL Công khai. \\nNhấn Enter để dừng ứng dụng...\") # Translated line\n",
    "\n",
    "        # Press enter to stop application\n",
    "        input() # Wait for user input to stop\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        logger.exception(f\"Error running Streamlit app: {e}\") # Translated log message internally\n",
    "        sys.exit(1)\n",
    "    except Exception as e_ngrok: # Catch ngrok related exceptions\n",
    "        logger.exception(f\"Ngrok error: {e_ngrok}\") # Translated log message internally\n",
    "        sys.exit(1)\n",
    "    finally:\n",
    "        if log_file: # Check if log_file is open before closing\n",
    "            log_file.close()\n",
    "        if ngrok_tunnel: # Check if ngrok_tunnel is initialized before killing\n",
    "            ngrok.kill()\n",
    "        print(\"Máy chủ đã đóng...\") # Translated line\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "CkooOmJBCUUS",
    "CgyW2qfACirC"
   ],
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
