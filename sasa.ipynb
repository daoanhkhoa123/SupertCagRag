{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CkooOmJBCUUS"
   },
   "source": [
    "# Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T02:02:37.211477Z",
     "iopub.status.busy": "2025-05-17T02:02:37.211242Z",
     "iopub.status.idle": "2025-05-17T02:03:29.676397Z",
     "shell.execute_reply": "2025-05-17T02:03:29.675581Z",
     "shell.execute_reply.started": "2025-05-17T02:02:37.211449Z"
    },
    "id": "Bi2jo1qkB7He",
    "outputId": "8ddc7ded-a7c3-4a3b-de31-3d442a854bc0",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
      "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
      "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease               \u001b[0m\n",
      "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease                         \u001b[0m\u001b[33m\u001b[33m\n",
      "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease                     \u001b[0m\n",
      "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease                 \u001b[33m\n",
      "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
      "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
      "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
      "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "261 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "pciutils is already the newest version (1:3.7.0-6).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 261 not upgraded.\n",
      ">>> Cleaning up old version at /usr/local/lib/ollama\n",
      ">>> Installing ollama to /usr/local\n",
      ">>> Downloading Linux amd64 bundle\n",
      "############################################################################################# 100.0% 92.5%\n",
      ">>> Adding ollama user to video group...\n",
      ">>> Adding current user to ollama group...\n",
      ">>> Creating ollama systemd service...\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
      ">>> NVIDIA GPU installed.\n",
      ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
      ">>> Install complete. Run \"ollama\" from the command line.\n"
     ]
    }
   ],
   "source": [
    "!sudo apt update\n",
    "!sudo apt install -y pciutils\n",
    "!curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T02:03:29.677908Z",
     "iopub.status.busy": "2025-05-17T02:03:29.677537Z",
     "iopub.status.idle": "2025-05-17T02:03:34.685282Z",
     "shell.execute_reply": "2025-05-17T02:03:34.684516Z",
     "shell.execute_reply.started": "2025-05-17T02:03:29.677871Z"
    },
    "id": "IqkuQ-HdCgne",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "def run_ollama_serve():\n",
    "  subprocess.Popen([\"ollama\", \"serve\"])\n",
    "\n",
    "thread = threading.Thread(target=run_ollama_serve)\n",
    "thread.start()\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T02:03:34.686338Z",
     "iopub.status.busy": "2025-05-17T02:03:34.686083Z",
     "iopub.status.idle": "2025-05-17T02:04:39.863023Z",
     "shell.execute_reply": "2025-05-17T02:04:39.862116Z",
     "shell.execute_reply.started": "2025-05-17T02:03:34.686318Z"
    },
    "id": "5CsbM2mnCKJp",
    "outputId": "bcd71c2b-e9a1-4355-8f2d-44d9261ecda5",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†ã \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†ô \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†π \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†∏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling e8ad13eff07a: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 8.1 GB                         \u001b[K\n",
      "pulling e0a42594d802: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  358 B                         \u001b[K\n",
      "pulling dd084c7d92a3: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 8.4 KB                         \u001b[K\n",
      "pulling 3116c5225075: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   77 B                         \u001b[K\n",
      "pulling 6819964c2bcf: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  490 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†ã \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†ô \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†π \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†∏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†º \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†¥ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling 65379f13e0a7: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.1 GB                         \u001b[K\n",
      "pulling 929c2df6adbc: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 1.0 KB                         \u001b[K\n",
      "pulling b5d44ab66603: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  337 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n",
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†ã \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†ô \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ‚†π \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling 4c5716ded514: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè 274 MB                         \u001b[K\n",
      "pulling c71d239df917: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  11 KB                         \u001b[K\n",
      "pulling 7424a767001b: 100% ‚ñï‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  346 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "!ollama pull gemma3:12b\n",
    "!ollama pull jeffh/intfloat-multilingual-e5-large-instruct:f16\n",
    "!ollama pull snowflake-arctic-embed:137m-m-long-fp16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CgyW2qfACirC"
   },
   "source": [
    "# Installing more package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T02:04:39.865223Z",
     "iopub.status.busy": "2025-05-17T02:04:39.864930Z",
     "iopub.status.idle": "2025-05-17T02:07:03.951742Z",
     "shell.execute_reply": "2025-05-17T02:07:03.950728Z",
     "shell.execute_reply.started": "2025-05-17T02:04:39.865200Z"
    },
    "id": "PLt1bIM-9pnG",
    "outputId": "1fb2f4ba-a2ed-45ef-d9a2-bcd365772ade",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'SupertCagRag' already exists and is not an empty directory.\n",
      "/kaggle/working/SupertCagRag\n",
      "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
      "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
      "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
      "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
      "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
      "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
      "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
      "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
      "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
      "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
      "Reading package lists...\n",
      "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "poppler-utils is already the newest version (22.02.0-2ubuntu0.8).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 261 not upgraded.\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 261 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "# Clone the repository if it doesn't exist\n",
    "!git clone https://github.com/daoanhkhoa123/SupertCagRag\n",
    "%cd /kaggle/working/SupertCagRag\n",
    "\n",
    "# Install required packages from requirements.txt\n",
    "!pip install -q -r requirements.txt\n",
    "\n",
    "# Update system packages and install additional tools\n",
    "!apt-get update -q\n",
    "!apt-get install -y -q poppler-utils\n",
    "\n",
    "!apt-get install -y -q tesseract-ocr\n",
    "!pip install -q pytesseract tesseract\n",
    "\n",
    "!pip install -q pyngrok pyvi\n",
    "!pip install -U -q langchain-community\n",
    "!pip install -qU transformers\n",
    "!pip install -q haystack-ai\n",
    "!pip install -q pyarrow==15.0.2\n",
    "!pip install -q cohere-haystack\n",
    "!pip install -q ollama-haystack\n",
    "!pip install -q duckduckgo-api-haystack pymupdf\n",
    "!pip install -q moviepy transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T02:07:03.953358Z",
     "iopub.status.busy": "2025-05-17T02:07:03.953092Z",
     "iopub.status.idle": "2025-05-17T02:07:08.025118Z",
     "shell.execute_reply": "2025-05-17T02:07:08.024121Z",
     "shell.execute_reply.started": "2025-05-17T02:07:03.953324Z"
    },
    "id": "bbkm1LQegMJa",
    "outputId": "21fb7336-91f6-4458-e353-256c6b86c0df",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /usr/share/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from pyngrok import ngrok\n",
    "import logging\n",
    "!ngrok config add-authtoken 2xBU36gLqZuWqqjlUdlnHRnn8fi_74J3BWwNMsXKZitBkuYed\n",
    "import os\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T02:07:08.027442Z",
     "iopub.status.busy": "2025-05-17T02:07:08.026868Z",
     "iopub.status.idle": "2025-05-17T02:07:08.051064Z",
     "shell.execute_reply": "2025-05-17T02:07:08.049982Z",
     "shell.execute_reply.started": "2025-05-17T02:07:08.027417Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# %%writefile /kaggle/working/SupertCagRag/src/app/main.py\n",
    "\n",
    "# # Standard library imports\n",
    "# import os\n",
    "# import tempfile\n",
    "# import shutil\n",
    "# import warnings\n",
    "# import logging\n",
    "# import random\n",
    "# import re\n",
    "# from typing import List, Tuple, Dict, Any, Optional, Callable, Union\n",
    "# from pathlib import Path\n",
    "\n",
    "# # Third-party libraries\n",
    "# import streamlit as st\n",
    "# import pdfplumber\n",
    "# from pyvi import ViTokenizer\n",
    "# from pydantic import BaseModel\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, AutoConfig, AutoModelForMaskedLM\n",
    "# from moviepy.editor import VideoFileClip\n",
    "# import fitz\n",
    "# from PIL import Image\n",
    "# import io\n",
    "\n",
    "# # HuggingFace + LangChain\n",
    "# from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "# from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "# from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "# from langchain_community.vectorstores import Chroma\n",
    "# from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "# from langchain_community.llms import HuggingFacePipeline\n",
    "# from langchain_core.runnables import RunnablePassthrough\n",
    "# from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "# from langchain.docstore.document import Document as LangchainDocument\n",
    "# from langchain_core.documents import Document as LangChainDocument\n",
    "# from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "\n",
    "# # Haystack\n",
    "# from haystack import Document, Pipeline, component\n",
    "# from haystack.components.builders import PromptBuilder\n",
    "# from haystack.components.retrievers import InMemoryEmbeddingRetriever\n",
    "# from haystack_integrations.components.generators.ollama import OllamaGenerator\n",
    "# from haystack.dataclasses import ChatMessage, StreamingChunk, Document as HaystackDocument\n",
    "# from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "# from haystack_integrations.components.embedders.ollama import OllamaDocumentEmbedder, OllamaTextEmbedder\n",
    "# from haystack.components.joiners import BranchJoiner\n",
    "# from haystack.components.routers import ConditionalRouter\n",
    "\n",
    "# # DuckDuckGo API\n",
    "# from duckduckgo_api_haystack import DuckduckgoApiWebSearch\n",
    "\n",
    "\n",
    "# def format_docs(docs):\n",
    "#     return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# # Set protobuf environment variable to avoid error messages\n",
    "# # This might cause some issues with latency but it's a tradeoff\n",
    "# os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n",
    "# PERSIST_DIRECTORY = os.path.join(\"data\", \"vectors\")\n",
    "\n",
    "# # Streamlit page configuration\n",
    "# st.set_page_config(\n",
    "#     page_title=\"Adaptive Academics\", # Changed title\n",
    "#     page_icon=\"ü§ñ\",\n",
    "#     layout=\"wide\",\n",
    "#     initial_sidebar_state=\"collapsed\",\n",
    "# )\n",
    "\n",
    "# # Logging configuration\n",
    "# logging.basicConfig(\n",
    "#     level=logging.INFO,\n",
    "#     format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "#     datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "# )\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "# def format_llm(s):\n",
    "#     return s.split(\"</think>\")[-1]\n",
    "\n",
    "\n",
    "\n",
    "# import ollama\n",
    "\n",
    "# translate_summarize_prompt = \"\"\"\n",
    "# D·ªãch n·ªôi dung c·ªßa h√¨nh ·∫£nh sang ti·∫øng Vi·ªát.\n",
    "# Sau ƒë√≥, t√≥m t·∫Øt c√°c √Ω ch√≠nh m·ªôt c√°ch ng·∫Øn g·ªçn b·∫±ng ti·∫øng Vi·ªát. \n",
    "# ƒê·∫£m b·∫£o r·∫±ng b·∫£n t√≥m t·∫Øt tr√¨nh b√†y ƒë·∫ßy ƒë·ªß th√¥ng tin v√† √Ω t∆∞·ªüng quan tr·ªçng trong h√¨nh ·∫£nh.\n",
    "\n",
    "# N·ªôi dung h√¨nh ·∫£nh:\n",
    "# {doc}\n",
    "# \"\"\"\n",
    "\n",
    "# def translate_imgagesummarize(doc):\n",
    "#     return OllamaGenerator(model=\"gemma3:12b\").run(translate_summarize_prompt.format(doc=doc))[\"replies\"][0]\n",
    "\n",
    "\n",
    "# def create_vector_db(file_upload):\n",
    "#     \"\"\"\n",
    "#     Create a vector database from an uploaded PDF file.\n",
    "#     - Only text is split and embedded.\n",
    "#     - Images/tables are NOT summarized or embedded.\n",
    "#     \"\"\"\n",
    "#     logger.info(f\"Creating vector DB from file upload: {file_upload.name}\")\n",
    "#     temp_dir = tempfile.mkdtemp()\n",
    "#     pdf_path = os.path.join(temp_dir, file_upload.name)\n",
    "\n",
    "#     with open(pdf_path, \"wb\") as f:\n",
    "#         f.write(file_upload.getvalue())\n",
    "#     logger.info(f\"File saved to temporary path: {pdf_path}\")\n",
    "\n",
    "#     haystack_documents: List[HaystackDocument] = []\n",
    "\n",
    "#     with fitz.open(pdf_path) as pdf_document:\n",
    "#         for page_num in range(pdf_document.page_count):\n",
    "#             page = pdf_document.load_page(page_num)\n",
    "#             text = page.get_text(\"text\")\n",
    "#             if text.strip():\n",
    "#                 haystack_documents.append(\n",
    "#                     HaystackDocument(\n",
    "#                         content=text.strip(),\n",
    "#                         meta={\"source\": f\"{file_upload.name}_page_{page_num + 1}\", \"type\": \"text\"}\n",
    "#                     )\n",
    "#                 )\n",
    "#     # Split only text documents\n",
    "#     text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "#     split_haystack_documents = []\n",
    "#     for doc in haystack_documents:\n",
    "#         chunks = text_splitter.split_text(doc.content)\n",
    "#         for i, chunk in enumerate(chunks):\n",
    "#             split_haystack_documents.append(\n",
    "#                 HaystackDocument(\n",
    "#                     content=chunk.strip(),\n",
    "#                     meta={**doc.meta, \"chunk_id\": i}\n",
    "#                 )\n",
    "#             )\n",
    "\n",
    "#     logger.info(f\"Total processed chunks (text only): {len(split_haystack_documents)}\")\n",
    "\n",
    "#     doc_embedder = OllamaDocumentEmbedder(model=\"jeffh/intfloat-multilingual-e5-large-instruct:f16\")\n",
    "#     docs_with_embeddings = doc_embedder.run(documents=split_haystack_documents)\n",
    "\n",
    "#     document_store = InMemoryDocumentStore()\n",
    "#     document_store.write_documents(docs_with_embeddings[\"documents\"])\n",
    "\n",
    "#     shutil.rmtree(temp_dir)\n",
    "#     logger.info(f\"Temporary directory {temp_dir} removed\")\n",
    "\n",
    "#     return document_store\n",
    "\n",
    "\n",
    "# def create_vector_db_from_video(video_path: str) -> InMemoryDocumentStore:\n",
    "#     \"\"\"\n",
    "#     Extracts audio from a video, transcribes it to English, translates to Vietnamese, and stores in a vector DB.\n",
    "\n",
    "#     Args:\n",
    "#         video_path (str): Path to the MP4 video file.\n",
    "\n",
    "#     Returns:\n",
    "#         InMemoryDocumentStore: Haystack in-memory vector database with embedded Vietnamese chunks.\n",
    "#     \"\"\"\n",
    "#     print(f\"[INFO] Processing video: {video_path}\")\n",
    "\n",
    "#     # Step 1: Extract audio\n",
    "#     temp_dir = tempfile.mkdtemp()\n",
    "#     audio_path = os.path.join(temp_dir, \"temp_audio.mp3\")\n",
    "\n",
    "#     video_clip = VideoFileClip(video_path)\n",
    "#     video_clip.audio.write_audiofile(audio_path, logger=None)\n",
    "#     video_clip.close()\n",
    "\n",
    "#     # Step 2: Transcribe with Whisper\n",
    "#     print(\"[INFO] Transcribing to English...\")\n",
    "#     transcriber = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base\")\n",
    "#     result = transcriber(audio_path, return_timestamps=True)\n",
    "#     transcript_text = result[\"text\"]\n",
    "#     os.remove(audio_path)\n",
    "#     os.rmdir(temp_dir)\n",
    "\n",
    "#     # Step 3: Split into chunks\n",
    "#     text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "#     chunks = text_splitter.split_documents([LangChainDocument(page_content=transcript_text)])\n",
    "#     logger.info(f\"Transcript split into {len(chunks)} chunks.\")\n",
    "\n",
    "#     # Step 4: Translate to Vietnamese\n",
    "#     print(\"[INFO] Translating chunks to Vietnamese...\")\n",
    "#     prompt_template = \"\"\"\n",
    "#     B·∫°n l√† m·ªôt chuy√™n gia ng√¥n ng·ªØ h·ªçc thu·∫≠t. H√£y d·ªãch n·ªôi dung sau sang ti·∫øng Vi·ªát v·ªõi ƒë·ªô ch√≠nh x√°c cao nh·∫•t, s·ª≠ d·ª•ng vƒÉn phong trang tr·ªçng, r√µ r√†ng v√† ph√π h·ª£p v·ªõi m√¥i tr∆∞·ªùng gi√°o d·ª•c ƒë·∫°i h·ªçc t·∫°i Vi·ªát Nam.\n",
    "    \n",
    "#     Y√™u c·∫ßu:\n",
    "#     - Gi·ªØ nguy√™n c·∫•u tr√∫c, thu·∫≠t ng·ªØ chuy√™n ng√†nh v√† √Ω nghƒ©a g·ªëc c·ªßa vƒÉn b·∫£n.\n",
    "#     - Kh√¥ng di·ªÖn gi·∫£i ho·∫∑c th√™m √Ω ki·∫øn c√° nh√¢n.\n",
    "#     - N·∫øu c√≥ thu·∫≠t ng·ªØ chuy√™n ng√†nh, h√£y gi·ªØ nguy√™n ho·∫∑c ch√∫ th√≠ch r√µ r√†ng b·∫±ng ti·∫øng Vi·ªát.\n",
    "#     - ƒê·∫£m b·∫£o b·∫£n d·ªãch d·ªÖ hi·ªÉu ƒë·ªëi v·ªõi sinh vi√™n ƒë·∫°i h·ªçc Vi·ªát Nam.\n",
    "\n",
    "#     N·ªôi dung c·∫ßn d·ªãch:\n",
    "#     {doc}\n",
    "#     \"\"\"\n",
    "\n",
    "#     generator = OllamaGenerator(model=\"gemma3:12b\")\n",
    "\n",
    "#     translated_chunks = []\n",
    "#     for i, chunk in enumerate(chunks):\n",
    "#         raw_text = chunk.page_content.strip()\n",
    "#         if raw_text:\n",
    "#             prompt = prompt_template.format(doc=raw_text)\n",
    "#             vn_text = vn_text = f\"N·ªôi dung c·ªßa video {video_path}+\\n\" + generator.run(prompt)[\"replies\"][0]\n",
    "#             translated_chunks.append(HaystackDocument(content=vn_text))\n",
    "#             print(f\"\\n--- Translated Chunk {i+1} ---\\n{vn_text[:300]}...\\n\")\n",
    "\n",
    "#     if not translated_chunks:\n",
    "#         logger.error(\"No valid translated chunks found.\")\n",
    "#         raise ValueError(\"No translated content to embed.\")\n",
    "\n",
    "#     # Step 5: Embed with Ollama\n",
    "#     print(\"[INFO] Embedding Vietnamese chunks...\")\n",
    "#     embedder = OllamaDocumentEmbedder(model=\"jeffh/intfloat-multilingual-e5-large-instruct:f16\")\n",
    "#     document_store = InMemoryDocumentStore()\n",
    "#     docs_with_embeddings = embedder.run(translated_chunks)\n",
    "#     document_store.write_documents(docs_with_embeddings[\"documents\"])\n",
    "\n",
    "#     print(\"[INFO] Vector DB created successfully.\")\n",
    "#     return document_store\n",
    "\n",
    "\n",
    "# from haystack import component\n",
    "\n",
    "# # user_info = {\n",
    "# #     \"name\": \"Nguy·ªÖn VƒÉn An\",\n",
    "# #     \"Data Science v·ªõi Python & SQL\": 9.5,\n",
    "# #     \"ƒê·∫°o ƒë·ª©c trong CNTT\": 8.8,\n",
    "# #     \"Ti·∫øng Nh·∫≠t c∆° b·∫£n 1- A1.1\": 9.0,\n",
    "# #     \"To√°n h·ªçc cho Machine Learning\": 9.2,\n",
    "# #     \"Th·ªëng k√™ & X√°c su·∫•t\": 8.7,\n",
    "# #     \"Bi·ªÉu di·ªÖn Opera v√† K·ªπ thu·∫≠t thanh nh·∫°c\": 4.5,\n",
    "# #     \"K·ªãch ng·∫´u h·ª©ng v√† H√†i k·ªãch\": 5.0,\n",
    "# #     \"Ch·ªâ huy D√†n nh·∫°c giao h∆∞·ªüng\": 4.3,\n",
    "# #     \"Bi√™n ƒë·∫°o n√¢ng cao v√† L√Ω thuy·∫øt m√∫a\": 6.0\n",
    "# # }\n",
    "\n",
    "# # user_info = {\n",
    "# #     \"T√™n\": \"A\",\n",
    "# #     \"Chuy√™n ng√†nh\": \"Tr√≠ tu·ªá nh√¢n t·∫°o\",\n",
    "# #     \"Deep Learning\": 7.9,\n",
    "# #     \"Machine Learning\": 7.9,\n",
    "# #     \"Computer Vision\": 7.8,\n",
    "# #     \"L·∫≠p tr√¨nh Python\": 7.8,\n",
    "# #     \"To√°n cao c·∫•p\": 7.0\n",
    "# # }\n",
    "\n",
    "# st.session_state[\"user_info\"] = {\n",
    "#     \"T√™n\": \"B\",\n",
    "#     \"Chuy√™n ng√†nh\": \"Kinh t·∫ø\",\n",
    "#     \"Kinh t·∫ø vi m√¥\": 8.7,\n",
    "#     \"Kinh t·∫ø vƒ© m√¥\": 7.3,\n",
    "#     \"Nguy√™n l√Ω k·∫ø to√°n\": 8.6,\n",
    "#     \"Marketing c∆° b·∫£n\": 8.4,\n",
    "#     \"Qu·∫£n tr·ªã\": 8.1\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "# @component\n",
    "# class UserInfo:\n",
    "#     @component.output_types(user_info=dict)\n",
    "#     def run(self, id: int = 0):\n",
    "#         return {\"user_info\": st.session_state.get(\"user_info\", {})}\n",
    "\n",
    "# user_component = UserInfo()\n",
    "# user_component.run()\n",
    "\n",
    "\n",
    "# routes = [\n",
    "#     {\n",
    "#         \"condition\": \"{{'yes_answer' in replies[0]}}\",\n",
    "#         \"output\": \"{{query}}\",\n",
    "#         \"output_name\": \"go_to_documents\",\n",
    "#         \"output_type\": str,\n",
    "#     },\n",
    "#     {\n",
    "#         \"condition\": \"{{'no_answer' in replies[0] and 'user_info_requested' not in replies[0]}}\",\n",
    "#         \"output\": \"{{query}}\",\n",
    "#         \"output_name\": \"go_to_websearch\",\n",
    "#         \"output_type\": str,\n",
    "#     },\n",
    "#     {\n",
    "#         \"condition\": \"{{'user_info_requested' in replies[0]}}\",\n",
    "#         \"output\": \"{{query}}\",\n",
    "#         \"output_name\": \"go_to_user_info\",\n",
    "#         \"output_type\": str,\n",
    "#     },\n",
    "#     {\n",
    "#         \"condition\": \"True\",\n",
    "#         \"output\": \"{{query}}\",\n",
    "#         \"output_name\": \"go_to_documents\",\n",
    "#         \"output_type\": str,\n",
    "#     },\n",
    "# ]\n",
    "\n",
    "\n",
    "# # hallu_route_real = [\n",
    "# #     {\n",
    "# #         \"condition\": \"{{'yes_answer' in replies[0]}}\",  # Ki·ªÉm tra 'yes' (kh√¥ng ·∫£o gi√°c)\n",
    "# #         \"output\": \"{{llm_replies[0]}}\",  # L·∫•y c√¢u tr·∫£ l·ªùi t·ª´ node \"llm\" (replies[1] v√¨ replies[0] l√† k·∫øt qu·∫£ c·ªßa grader)\n",
    "# #         \"output_name\": \"pass_answer\",  # T√™n output ƒë·ªÉ chuy·ªÉn ti·∫øp c√¢u tr·∫£ l·ªùi\n",
    "# #         \"output_type\": str,\n",
    "# #     },\n",
    "# #     {\n",
    "# #         \"condition\": \"{{'no_answer' in replies[0]}}\",  # Ki·ªÉm tra 'no' (·∫£o gi√°c)\n",
    "# #         \"output\": \"{{query}}\",  # S·ª≠ d·ª•ng l·∫°i c√¢u h·ªèi ban ƒë·∫ßu\n",
    "# #         \"output_name\": \"regenerate\",  # T√™n output ƒë·ªÉ k√≠ch ho·∫°t l·∫°i node \"llm\"\n",
    "# #         \"output_type\": str,\n",
    "# #     },\n",
    "# # ]\n",
    "\n",
    "\n",
    "# hallu_route = [\n",
    "#     {\n",
    "#         \"condition\": \"{{'yes_answer' in replies[0]}}\",  # Ki·ªÉm tra 'yes' (kh√¥ng ·∫£o gi√°c)\n",
    "#         \"output\": \"{{llm_replies[0]}}\",  # L·∫•y c√¢u tr·∫£ l·ªùi t·ª´ node \"llm\" (replies[1] v√¨ replies[0] l√† k·∫øt qu·∫£ c·ªßa grader)\n",
    "#         \"output_name\": \"pass_answer\",  # T√™n output ƒë·ªÉ chuy·ªÉn ti·∫øp c√¢u tr·∫£ l·ªùi\n",
    "#         \"output_type\": str,\n",
    "#     },\n",
    "#     {\n",
    "#         \"condition\": \"{{'no_answer' in replies[0]}}\",  # Ki·ªÉm tra 'no' (·∫£o gi√°c)\n",
    "#         \"output\": \"{{llm_replies[0]}}\",  # S·ª≠ d·ª•ng l·∫°i c√¢u h·ªèi ban ƒë·∫ßu\n",
    "#         \"output_name\": \"pass_answer\",  # T√™n output ƒë·ªÉ k√≠ch ho·∫°t l·∫°i node \"llm\"\n",
    "#         \"output_type\": str,\n",
    "#     },\n",
    "#           {\n",
    "#         \"condition\": \"True\",  # Ki·ªÉm tra 'no' (·∫£o gi√°c) Changed to False\n",
    "#         \"output\": \"{{llm_replies[0]}}\",  # S·ª≠ d·ª•ng l·∫°i c√¢u h·ªèi ban ƒë·∫ßu\n",
    "#         \"output_name\": \"pass_answer\",  # T√™n output ƒë·ªÉ k√≠ch ho·∫°t l·∫°i node \"llm\"\n",
    "#         \"output_type\": str,\n",
    "#     },\n",
    "#       {\n",
    "#         \"condition\": \"False\",  # Ki·ªÉm tra 'no' (·∫£o gi√°c) Changed to False\n",
    "#         \"output\": \"{{query}}\",  # S·ª≠ d·ª•ng l·∫°i c√¢u h·ªèi ban ƒë·∫ßu\n",
    "#         \"output_name\": \"not_use\",  # T√™n output ƒë·ªÉ k√≠ch ho·∫°t l·∫°i node \"llm\"\n",
    "#         \"output_type\": str,\n",
    "#     },\n",
    "# ]\n",
    "\n",
    "# # ==============================================================\n",
    "# # PROMPTS ƒê√ÅNH GI√Å V√Ä KH·ªûI T·∫†O NG·ªÆ C·∫¢NH\n",
    "# # ==============================================================\n",
    "\n",
    "\n",
    "# prompt_context_init = r\"\"\"\n",
    "# B·∫°n l√† m·ªôt gi√°o vi√™n si√™u nghi√™m kh·∫Øc, ng∆∞·ªùi ƒë√°nh gi√° hi·ªáu su·∫•t c·ªßa h·ªçc sinh v·ªõi ƒë·ªô ch√≠nh x√°c kh√¥ng ng·ª´ng ngh·ªâ.\n",
    "# C√¥ng vi·ªác c·ªßa b·∫°n l√† ch·∫•m ƒëi·ªÉm t·ª´ng m√¥n h·ªçc d·ª±a tr√™n ƒëi·ªÉm s·ªë c·ªßa h·ªçc sinh v√† cung c·∫•p ph·∫£n h·ªìi nghi√™m kh·∫Øc v√† chi ti·∫øt.\n",
    "# S·ª≠ d·ª•ng c√°c quy t·∫Øc v√† m√¥ t·∫£ sau ƒë·ªÉ ƒë√°nh gi√° h·ªçc sinh:\n",
    "\n",
    "# Ti√™u ch√≠ ch·∫•m ƒëi·ªÉm:\n",
    "# - ƒêi·ªÉm t·ª´ 9.0 ƒë·∫øn 10.0: Tuy·ªát v·ªùi! - B·∫°n ƒë√£ th·ªÉ hi·ªán s·ª± n·∫Øm v·ªØng ki·∫øn th·ª©c v√† k·ªπ nƒÉng xu·∫•t s·∫Øc trong m√¥n h·ªçc n√†y. N·ªó l·ª±c v√† s·ª± ch√≠nh x√°c c·ªßa b·∫°n r·∫•t ƒë√°ng khen ng·ª£i. H√£y ti·∫øp t·ª•c duy tr√¨ phong ƒë·ªô tuy·ªát v·ªùi n√†y v√† th·ª≠ th√°ch b·∫£n th√¢n v·ªõi nh·ªØng ki·∫øn th·ª©c s√¢u h∆°n nh√©!\n",
    "# - ƒêi·ªÉm t·ª´ 8.0 ƒë·∫øn 8.9: R·∫•t t·ªët! - B·∫°n hi·ªÉu b√†i r·∫•t t·ªët v√† th·ªÉ hi·ªán nƒÉng l·ª±c ·∫•n t∆∞·ª£ng. Ch·ªâ c√≤n m·ªôt v√†i ƒëi·ªÉm nh·ªè n·ªØa l√† ƒë·∫°t ƒë·∫øn m·ª©c ho√†n h·∫£o. C√πng xem l·∫°i nh·ªØng chi ti·∫øt nh·ªè n√†y ƒë·ªÉ gi√∫p b·∫°n ho√†n thi·ªán h∆°n n·ªØa ki·∫øn th·ª©c v√† k·ªπ nƒÉng c·ªßa m√¨nh.\n",
    "# - ƒêi·ªÉm t·ª´ 7.0 ƒë·∫øn 7.9: T·ªët! - B·∫°n ƒë√£ n·∫Øm ƒë∆∞·ª£c nh·ªØng ki·∫øn th·ª©c v√† k·ªπ nƒÉng c∆° b·∫£n, quan tr·ªçng c·ªßa m√¥n h·ªçc. ƒê√¢y l√† m·ªôt n·ªÅn t·∫£ng t·ªët. ƒê·ªÉ hi·ªÉu s√¢u s·∫Øc h∆°n, b·∫°n c√≥ th·ªÉ t·∫≠p trung th√™m v√†o vi·ªác c·ªßng c·ªë [ch·ªâ ra lƒ©nh v·ª±c c·ª• th·ªÉ n·∫øu c√≥ th·ªÉ] v√† luy·ªán t·∫≠p th√™m. ƒê·ª´ng ng·∫ßn ng·∫°i ƒë·∫∑t c√¢u h·ªèi v·ªÅ nh·ªØng ph·∫ßn b·∫°n c√≤n bƒÉn khoƒÉn.\n",
    "# - 6.0 ƒë·∫øn 6.9: Kh√°! - C√≥ s·ª± c·ªë g·∫Øng r√µ r·ªát v√† b·∫°n ƒë√£ n·∫Øm b·∫Øt ƒë∆∞·ª£c m·ªôt ph·∫ßn ki·∫øn th·ª©c. Tuy nhi√™n, s·ª± hi·ªÉu bi·∫øt c·∫ßn ƒë∆∞·ª£c ƒë√†o s√¢u h∆°n ƒë·ªÉ th·ª±c s·ª± v·ªØng v√†ng. H√£y c√πng x√°c ƒë·ªãnh nh·ªØng ph·∫ßn ki·∫øn th·ª©c c·∫ßn c·ªßng c·ªë th√™m. M·ªôt s·ªë g·ª£i √Ω c·ª• th·ªÉ c√≥ th·ªÉ gi√∫p b·∫°n ti·∫øn b·ªô nhanh h∆°n ƒë·∫•y.\n",
    "# - ƒêi·ªÉm t·ª´ 5.0 ƒë·∫øn 5.9: C·∫ßn c·ªë g·∫Øng h∆°n! - C√≥ v·∫ª nh∆∞ b·∫°n ƒëang g·∫∑p m·ªôt s·ªë th·ª≠ th√°ch v·ªõi c√°c kh√°i ni·ªám c·ªët l√µi c·ªßa m√¥n h·ªçc n√†y. Kh√¥ng sao c·∫£, ai c≈©ng c√≥ l√∫c g·∫∑p kh√≥ khƒÉn. ƒêi·ªÅu quan tr·ªçng l√† x√°c ƒë·ªãnh ƒë∆∞·ª£c nh·ªØng ƒëi·ªÉm b·∫°n ch∆∞a th·ª±c s·ª± hi·ªÉu r√µ. H√£y b·∫Øt ƒë·∫ßu t·ª´ ƒë√≥, ch√∫ng ta c√≥ th·ªÉ c√πng nhau x√¢y d·ª±ng m·ªôt k·∫ø ho·∫°ch h·ªçc t·∫≠p ph√π h·ª£p ƒë·ªÉ b·∫°n c·∫£i thi·ªán nh√©. \n",
    "# - ƒêi·ªÉm d∆∞·ªõi 5.0: K·∫øt qu·∫£ n√†y cho th·∫•y ph∆∞∆°ng ph√°p h·ªçc hi·ªán t·∫°i c√≥ th·ªÉ ch∆∞a ph√π h·ª£p nh·∫•t v·ªõi b·∫°n trong m√¥n h·ªçc n√†y. ƒê√¢y l√† m·ªôt t√≠n hi·ªáu ƒë·ªÉ ch√∫ng ta c√πng nh√¨n l·∫°i. ƒê·ª´ng n·∫£n l√≤ng! ƒê√¢y l√† c∆° h·ªôi ƒë·ªÉ kh√°m ph√° nh·ªØng c√°ch ti·∫øp c·∫≠n m·ªõi hi·ªáu qu·∫£ h∆°n. H√£y th·ª≠ b·∫Øt ƒë·∫ßu l·∫°i t·ª´ nh·ªØng ki·∫øn th·ª©c n·ªÅn t·∫£ng nh·∫•t v√† t√¨m ki·∫øm s·ª± h·ªó tr·ª£ khi c·∫ßn thi·∫øt. Lu√¥n c√≥ c√°ch ƒë·ªÉ ti·∫øn b·ªô!\n",
    "\n",
    "# Quy t·∫Øc ƒë√°nh gi√°:\n",
    "# 1. Cung c·∫•p ph·∫£n h·ªìi cho t·ª´ng m√¥n h·ªçc m√† h·ªçc sinh ƒë∆∞·ª£c ch·∫•m ƒëi·ªÉm.\n",
    "# 2. T·∫≠p trung v√†o c·∫£ ƒëi·ªÉm m·∫°nh v√† ƒëi·ªÉm y·∫øu, kh√¥ng ƒë·ªÉ l·∫°i s·ª± m∆° h·ªì v·ªÅ c√°c lƒ©nh v·ª±c c·∫ßn c·∫£i thi·ªán.\n",
    "# 3. N·∫øu h·ªçc sinh ƒë·∫°t d∆∞·ªõi 7.0 ·ªü b·∫•t k·ª≥ m√¥n n√†o, h√£y n√™u r√µ ƒëi·ªÅu n√†y nh∆∞ m·ªôt v·∫•n ƒë·ªÅ c·∫ßn ch√∫ √Ω ngay l·∫≠p t·ª©c.\n",
    "# 4. N·∫øu h·ªçc sinh ƒë·∫°t d∆∞·ªõi 5.0 ·ªü b·∫•t k·ª≥ m√¥n n√†o, nh·∫•n m·∫°nh ƒëi·ªÅu n√†y nh∆∞ m·ªôt s·ª± th·∫•t b·∫°i v√† ƒë·ªÅ ngh·ªã c√°c h√†nh ƒë·ªông kh·∫Øc ph·ª•c m·∫°nh m·∫Ω.\n",
    "\n",
    "# ‚ö†Ô∏è To√†n b·ªô ƒë√°nh gi√° ph·∫£i b·∫±ng ti·∫øng Vi·ªát.\n",
    "\n",
    "# D∆∞·ªõi ƒë√¢y l√† d·ªØ li·ªáu c·ªßa h·ªçc sinh ƒë·ªÉ b·∫°n ƒë√°nh gi√°:\n",
    "# {{user_info}}\n",
    "\n",
    "# B√¢y gi·ªù, h√£y ƒë√°nh gi√° hi·ªáu su·∫•t c·ªßa h·ªçc sinh ƒë·ªëi v·ªõi t·ª´ng m√¥n h·ªçc d·ª±a tr√™n d·ªØ li·ªáu ƒë√£ cung c·∫•p. ƒê∆∞a ra gi·∫£i th√≠ch chi ti·∫øt cho t·ª´ng ƒëi·ªÉm s·ªë v√† k·∫øt lu·∫≠n v·ªõi m·ªôt ƒë√°nh gi√° t·ªïng th·ªÉ nghi√™m kh·∫Øc nh∆∞ng c√¥ng b·∫±ng v·ªÅ hi·ªáu su·∫•t c·ªßa h·ªç. H√£y k·ªπ l∆∞·ª°ng, mang t√≠nh x√¢y d·ª±ng v√† kh√¥ng khoan nh∆∞·ª£ng trong ƒë√°nh gi√° c·ªßa b·∫°n. Ch·ªâ ƒë∆∞a ra ph√°n quy·∫øt cu·ªëi c√πng.\n",
    "# \"\"\"\n",
    "\n",
    "# prompt_context_combine = r\"\"\"\n",
    "# B·∫°n l√† m·ªôt gi√°o vi√™n v√¥ c√πng nghi√™m kh·∫Øc, ng∆∞·ªùi lu√¥n ƒë√°nh gi√° hi·ªáu su·∫•t c·ªßa h·ªçc sinh v·ªõi s·ª± kh·∫Øt khe kh√¥ng khoan nh∆∞·ª£ng.\n",
    "# Nhi·ªám v·ª• c·ªßa b·∫°n l√† ch·∫•m ƒëi·ªÉm t·ª´ng m√¥n h·ªçc d·ª±a tr√™n ƒëi·ªÉm s·ªë c·ªßa h·ªçc sinh v√† cung c·∫•p ph·∫£n h·ªìi trung th·ª±c, chi ti·∫øt m·ªôt c√°ch th·∫≥ng th·∫Øn, k·∫øt h·ª£p th√¥ng tin t·ª´ l·ªãch s·ª≠ tr√≤ chuy·ªán v√† b·ªëi c·∫£nh hi·ªán t·∫°i.\n",
    "# S·ª≠ d·ª•ng c√°c quy t·∫Øc v√† m√¥ t·∫£ sau ƒë·ªÉ ƒë√°nh gi√° h·ªçc sinh:\n",
    "\n",
    "# Ti√™u ch√≠ ch·∫•m ƒëi·ªÉm:\n",
    "# - ƒêi·ªÉm t·ª´ 9.0 ƒë·∫øn 10.0: Xu·∫•t s·∫Øc - M·ªôt hi·ªáu su·∫•t m·∫´u m·ª±c th·ªÉ hi·ªán s·ª± l√†m ch·ªß m√¥n h·ªçc. B√†i l√†m c·ªßa h·ªç ph·∫£n √°nh n·ªó l·ª±c v√† ƒë·ªô ch√≠nh x√°c v√¥ song. H√£y khuy·∫øn kh√≠ch h·ªçc sinh duy tr√¨ m·ª©c ƒë·ªô xu·∫•t s·∫Øc n√†y.\n",
    "# - ƒêi·ªÉm t·ª´ 8.0 ƒë·∫øn 8.9: R·∫•t t·ªët - Hi·ªáu su·∫•t ·∫•n t∆∞·ª£ng, nh∆∞ng kh√¥ng ph·∫£i l√† ho√†n h·∫£o. H√£y ch·ªâ ra c√°c ƒëi·ªÉm y·∫øu nh·ªè v√† ƒë·ªÅ xu·∫•t c√°ch ƒë·ªÉ ph·∫•n ƒë·∫•u ƒë·∫°t ƒë·∫øn s·ª± ho√†n h·∫£o.\n",
    "# - ƒêi·ªÉm t·ª´ 7.0 ƒë·∫øn 7.9: T·ªët - Hi·ªÉu bi·∫øt ƒë·∫ßy ƒë·ªß v√† hi·ªáu su·∫•t ƒë√°ng h√†i l√≤ng. H√£y ch·ªâ ra nh·ªØng l·ªó h·ªïng trong ki·∫øn th·ª©c v√† y√™u c·∫ßu s·ª± t·∫≠p trung v√† k·ª∑ lu·∫≠t t·ªët h∆°n.\n",
    "# - ƒêi·ªÉm t·ª´ 6.0 ƒë·∫øn 6.9: T·∫°m ·ªïn - H·∫ßu nh∆∞ ƒë·∫°t y√™u c·∫ßu. C√≥ s·ª± c·ªë g·∫Øng nh∆∞ng s·ª± hi·ªÉu bi·∫øt v·∫´n c√≤n n√¥ng c·∫°n. Cung c·∫•p c√°c khuy·∫øn ngh·ªã c·ª• th·ªÉ ƒë·ªÉ c·ªßng c·ªë nh·ªØng ƒëi·ªÉm y·∫øu.\n",
    "# - ƒêi·ªÉm t·ª´ 5.0 ƒë·∫øn 5.9: C·∫ßn c·∫£i thi·ªán - Kh√¥ng ch·∫•p nh·∫≠n ƒë∆∞·ª£c. H·ªçc sinh g·∫∑p kh√≥ khƒÉn ƒë√°ng k·ªÉ v·ªõi c√°c kh√°i ni·ªám c∆° b·∫£n v√† ch∆∞a c·ªë g·∫Øng ƒë·ªß. H√£y th√∫c ƒë·∫©y h√†nh ƒë·ªông ngay l·∫≠p t·ª©c, bao g·ªìm s·ª± gi√∫p ƒë·ª° b·ªï sung v√† k·∫ø ho·∫°ch h·ªçc t·∫≠p chuy√™n bi·ªát.\n",
    "# - ƒêi·ªÉm d∆∞·ªõi 5.0: K√©m - Ho√†n to√†n th·∫•t b·∫°i. H·ªçc sinh ƒë√£ kh√¥ng t∆∞∆°ng t√°c v·ªõi m√¥n h·ªçc ho·∫∑c t·∫°o ra ti·∫øn b·ªô ƒë√°ng k·ªÉ. H√£y y√™u c·∫ßu c√°c thay ƒë·ªïi ƒë√°ng k·ªÉ trong ph∆∞∆°ng ph√°p h·ªçc t·∫≠p.\n",
    "\n",
    "# Quy t·∫Øc ƒë√°nh gi√°:\n",
    "# 1. Cung c·∫•p ph·∫£n h·ªìi cho t·ª´ng m√¥n h·ªçc m√† h·ªçc sinh ƒë∆∞·ª£c ch·∫•m ƒëi·ªÉm.\n",
    "# 2. T·∫≠p trung v√†o c·∫£ ƒëi·ªÉm m·∫°nh v√† ƒëi·ªÉm y·∫øu, kh√¥ng ƒë·ªÉ l·∫°i s·ª± m∆° h·ªì v·ªÅ c√°c lƒ©nh v·ª±c c·∫ßn c·∫£i thi·ªán.\n",
    "# 3. N·∫øu h·ªçc sinh ƒë·∫°t d∆∞·ªõi 7.0 ·ªü b·∫•t k·ª≥ m√¥n h·ªçc n√†o, h√£y nh·∫•n m·∫°nh ƒë√¢y l√† m·ªôt v·∫•n ƒë·ªÅ c·∫ßn ƒë∆∞·ª£c gi·∫£i quy·∫øt ngay l·∫≠p t·ª©c.\n",
    "# 4. N·∫øu h·ªçc sinh ƒë·∫°t d∆∞·ªõi 5.0 ·ªü b·∫•t k·ª≥ m√¥n h·ªçc n√†o, h√£y nh·∫•n m·∫°nh ƒë√¢y l√† m·ªôt s·ª± th·∫•t b·∫°i v√† ƒë·ªÅ xu·∫•t c√°c h√†nh ƒë·ªông kh·∫Øc ph·ª•c ƒë√°ng k·ªÉ.\n",
    "\n",
    "# ‚ö†Ô∏è To√†n b·ªô ƒë√°nh gi√° ph·∫£i b·∫±ng ti·∫øng Vi·ªát.\n",
    "\n",
    "# D∆∞·ªõi ƒë√¢y l√† d·ªØ li·ªáu c·ªßa h·ªçc sinh ƒë·ªÉ b·∫°n ƒë√°nh gi√°:\n",
    "# {{user_info}}\n",
    "\n",
    "# D·ª±a v√†o l·ªãch s·ª≠ tr√≤ chuy·ªán:\n",
    "# {{chat_history}}\n",
    "\n",
    "# D·ª±a tr√™n b·ªëi c·∫£nh hi·ªán t·∫°i:\n",
    "# {{current_thoughts}}\n",
    "\n",
    "# B√¢y gi·ªù, h√£y ƒë√°nh gi√° hi·ªáu su·∫•t c·ªßa h·ªçc sinh cho t·ª´ng m√¥n h·ªçc d·ª±a tr√™n c√°c d·ªØ li·ªáu ƒë∆∞·ª£c cung c·∫•p. Cung c·∫•p gi·∫£i th√≠ch chi ti·∫øt cho t·ª´ng ƒë√°nh gi√°, v√† k·∫øt lu·∫≠n v·ªõi m·ªôt ƒë√°nh gi√° t·ªïng th·ªÉ nghi√™m kh·∫Øc nh∆∞ng c√¥ng b·∫±ng v·ªÅ hi·ªáu su·∫•t c·ªßa h·ªç. H√£y to√†n di·ªán, x√¢y d·ª±ng v√† kh√¥ng khoan nh∆∞·ª£ng trong ƒë√°nh gi√° c·ªßa b·∫°n.\n",
    "# \"\"\"\n",
    "\n",
    "# # ==============================================================\n",
    "# # PROMPT KI·ªÇM TRA T√çNH X√ÅC TH·ª∞C V√Ä T·ªíN T·∫†I C·ª¶A C√ÇU TR·∫¢ L·ªúI\n",
    "# # ==============================================================\n",
    "\n",
    "# prompt_template = r\"\"\"\n",
    "# B·∫†N L√Ä B·ªò L·ªåC TH√îNG TIN. Ch·ªâ ki·ªÉm tra xem th√¥ng tin c·∫ßn thi·∫øt ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi c√≥ t·ªìn t·∫°i trong t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p hay kh√¥ng, ho·∫∑c c√≥ c·∫ßn th√¥ng tin t·ª´ ng∆∞·ªùi d√πng kh√¥ng.\n",
    "\n",
    "# 1. N·∫øu t√†i li·ªáu ch·ª©a ƒë·ªß th√¥ng tin ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi: Ph·∫£n h·ªìi ch√≠nh x√°c l√† 'co_cau_tra_loi'.\n",
    "# 2. N·∫øu t√†i li·ªáu kh√¥ng ƒë·ªß nh∆∞ng c√¢u h·ªèi r√µ r√†ng y√™u c·∫ßu th√¥ng tin c√° nh√¢n/ƒëi·ªÉm s·ªë c·ªßa ng∆∞·ªùi d√πng ƒë·ªÉ c√≥ th·ªÉ tr·∫£ l·ªùi: Ph·∫£n h·ªìi ch√≠nh x√°c l√† 'yeu_cau_thong_tin_nguoi_dung'.\n",
    "# 3. N·∫øu t√†i li·ªáu kh√¥ng ch·ª©a th√¥ng tin v√† c≈©ng kh√¥ng c·∫ßn th√¥ng tin ng∆∞·ªùi d√πng ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y: Ph·∫£n h·ªìi ch√≠nh x√°c l√† 'khong_co_cau_tra_loi'.\n",
    "\n",
    "# ‚ö†Ô∏è Ch·ªâ ph·∫£n h·ªìi b·∫±ng M·ªòT trong ba c·ª•m t·ª´ ti·∫øng Vi·ªát tr√™n, kh√¥ng th√™m b·∫•t k·ª≥ gi·∫£i th√≠ch n√†o kh√°c.\n",
    "\n",
    "# T√†i li·ªáu:\n",
    "# {% for document in documents %}\n",
    "#   {{document.content}}\n",
    "# {% endfor %}\n",
    "\n",
    "# C√¢u h·ªèi:\n",
    "# {{query}}\n",
    "\n",
    "# Ph·∫£n h·ªìi c·ªßa b·∫°n:\n",
    "# \"\"\"\n",
    "\n",
    "# propmt_hallu_grader = r\"\"\"\n",
    "# B·∫°n l√† m·ªôt chuy√™n gia ƒë√°nh gi√° ƒë·ªô tin c·∫≠y c·ªßa c√¢u tr·∫£ l·ªùi t·ª´ LLM.\n",
    "# Nhi·ªám v·ª•: Ki·ªÉm tra xem c√¢u tr·∫£ l·ªùi c·ªßa LLM c√≥ ho√†n to√†n d·ª±a tr√™n T√†i li·ªáu v√† Th√¥ng tin ng∆∞·ªùi d√πng ƒë∆∞·ª£c cung c·∫•p hay kh√¥ng.\n",
    "# ƒê√°nh gi√° nh·ªã ph√¢n:\n",
    "# - 'co_ho_tro': N·∫øu c√¢u tr·∫£ l·ªùi ƒë∆∞·ª£c h·ªó tr·ª£ ƒë·∫ßy ƒë·ªß b·ªüi T√†i li·ªáu v√† Th√¥ng tin ng∆∞·ªùi d√πng.\n",
    "# - 'khong_ho_tro': N·∫øu c√¢u tr·∫£ l·ªùi ch·ª©a th√¥ng tin kh√¥ng c√≥ trong T√†i li·ªáu/Th√¥ng tin ng∆∞·ªùi d√πng, ho·∫∑c m√¢u thu·∫´n v·ªõi ch√∫ng.\n",
    "\n",
    "# ‚ö†Ô∏è Ch·ªâ ƒë∆∞a ra M·ªòT trong hai k·∫øt qu·∫£ ƒë√°nh gi√° ti·∫øng Vi·ªát tr√™n.\n",
    "\n",
    "# Th√¥ng tin ng∆∞·ªùi d√πng:\n",
    "# {{user_info}}\n",
    "\n",
    "# ƒê√°nh gi√° c·ªßa gi√°o vi√™n (n·∫øu c√≥):\n",
    "# {{context}}\n",
    "\n",
    "# T√†i li·ªáu:\n",
    "# {% for document in documents %}\n",
    "#   {{document.content}}\n",
    "# {% endfor %}\n",
    "\n",
    "# Ph·∫£n h·ªìi c·ªßa LLM:\n",
    "# {{llm_replies}}\n",
    "\n",
    "# ƒê√°nh gi√° (co_ho_tro/khong_ho_tro):\n",
    "# \"\"\"\n",
    "\n",
    "# # ==============================================================\n",
    "# # PROMPTS T·∫†O C√ÇU TR·∫¢ L·ªúI T√ôY CH·ªàNH THEO ƒêI·ªÇM S·ªê\n",
    "# # (√Åp d·ª•ng Role Prompting, Few-Shot, CoT, Self-Correction)\n",
    "# # ==============================================================\n",
    "\n",
    "# prompt_template_after_websearch = r\"\"\"\n",
    "# B·∫°n l√† m·ªôt Tr·ª£ gi·∫£ng AI ki√™n nh·∫´n v√† am hi·ªÉu, c√≥ kh·∫£ nƒÉng ƒëi·ªÅu ch·ªânh c√°ch gi·∫£i th√≠ch cho ph√π h·ª£p v·ªõi tr√¨nh ƒë·ªô c·ªßa t·ª´ng sinh vi√™n.\n",
    "\n",
    "# M·ª•c ti√™u: Tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa sinh vi√™n d·ª±a tr√™n Ng·ªØ c·∫£nh, Th√¥ng tin ng∆∞·ªùi d√πng (ƒëi·ªÉm s·ªë), v√† T√†i li·ªáu t·ª´ web. C√¢u tr·∫£ l·ªùi ph·∫£i ƒë∆∞·ª£c ƒëi·ªÅu ch·ªânh ƒë·ªô ph·ª©c t·∫°p **ch·ªâ d·ª±a tr√™n ƒëi·ªÉm s·ªë c·ªßa sinh vi√™n trong c√°c m√¥n h·ªçc li√™n quan**.\n",
    "\n",
    "# V√≠ d·ª• v·ªÅ c√°ch ƒëi·ªÅu ch·ªânh:\n",
    "# --------------------------\n",
    "# V√≠ d·ª• 1 (Sinh vi√™n ƒëi·ªÉm cao m√¥n To√°n li√™n quan):\n",
    "# C√¢u h·ªèi: Gi·∫£i th√≠ch ƒê·ªãnh l√Ω Pytago.\n",
    "# C√¢u tr·∫£ l·ªùi (chi ti·∫øt): ƒê·ªãnh l√Ω Pytago ph√°t bi·ªÉu r·∫±ng trong m·ªôt tam gi√°c vu√¥ng, b√¨nh ph∆∞∆°ng c·∫°nh huy·ªÅn b·∫±ng t·ªïng b√¨nh ph∆∞∆°ng hai c·∫°nh g√≥c vu√¥ng ($a^2 + b^2 = c^2$). ƒê·ªãnh l√Ω n√†y l√† n·ªÅn t·∫£ng cho l∆∞·ª£ng gi√°c v√† h√¨nh h·ªçc Euclid, cho ph√©p t√≠nh kho·∫£ng c√°ch, g√≥c, v√† c√≥ nhi·ªÅu ·ª©ng d·ª•ng trong v·∫≠t l√Ω, k·ªπ thu·∫≠t...\n",
    "\n",
    "# V√≠ d·ª• 2 (Sinh vi√™n ƒëi·ªÉm th·∫•p m√¥n To√°n li√™n quan):\n",
    "# C√¢u h·ªèi: Gi·∫£i th√≠ch ƒê·ªãnh l√Ω Pytago.\n",
    "# C√¢u tr·∫£ l·ªùi (ƒë∆°n gi·∫£n): Trong tam gi√°c c√≥ m·ªôt g√≥c vu√¥ng, c·∫°nh d√†i nh·∫•t (c·∫°nh huy·ªÅn) c√≥ m·ªëi li√™n h·ªá ƒë·∫∑c bi·ªát v·ªõi hai c·∫°nh c√≤n l·∫°i. N·∫øu b·∫°n bi·∫øt ƒë·ªô d√†i hai c·∫°nh ng·∫Øn, b·∫°n c√≥ th·ªÉ t√¨m ra ƒë·ªô d√†i c·∫°nh d√†i nh·∫•t b·∫±ng c√¥ng th·ª©c $a^2 + b^2 = c^2$. V√≠ d·ª•, n·∫øu hai c·∫°nh ng·∫Øn l√† 3 v√† 4, th√¨ c·∫°nh d√†i nh·∫•t l√† 5 (v√¨ $3^2 + 4^2 = 9 + 16 = 25$, v√† $5^2 = 25$).\n",
    "# --------------------------\n",
    "\n",
    "# Th·ª±c hi·ªán c√°c b∆∞·ªõc sau ƒë·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi:\n",
    "# 1.  **X√°c ƒë·ªãnh m√¥n h·ªçc li√™n quan:** D·ª±a v√†o C√¢u h·ªèi, x√°c ƒë·ªãnh (c√°c) m√¥n h·ªçc ch√≠nh c√≥ li√™n quan.\n",
    "# 2.  **Ki·ªÉm tra ƒëi·ªÉm s·ªë:** T√¨m ƒëi·ªÉm s·ªë c·ªßa sinh vi√™n trong (c√°c) m√¥n h·ªçc ƒë√≥ t·ª´ Th√¥ng tin ng∆∞·ªùi d√πng.\n",
    "# 3.  **Quy·∫øt ƒë·ªãnh m·ª©c ƒë·ªô:** D·ª±a tr√™n ƒëi·ªÉm s·ªë (cao >= 7.0, th·∫•p < 7.0), ch·ªçn M·ªòT m·ª©c ƒë·ªô gi·∫£i th√≠ch: chi ti·∫øt/s√¢u s·∫Øc (ƒëi·ªÉm cao) HO·∫∂C ƒë∆°n gi·∫£n/v√≠ d·ª• r√µ r√†ng (ƒëi·ªÉm th·∫•p).\n",
    "# 4.  **T·∫°o c√¢u tr·∫£ l·ªùi:** So·∫°n th·∫£o c√¢u tr·∫£ l·ªùi ch·ªâ ·ªü m·ª©c ƒë·ªô ƒë√£ ch·ªçn, s·ª≠ d·ª•ng th√¥ng tin t·ª´ Ng·ªØ c·∫£nh v√† T√†i li·ªáu t·ª´ web. ƒê·∫£m b·∫£o tr√≠ch d·∫´n URL n·∫øu s·ª≠ d·ª•ng th√¥ng tin t·ª´ web.\n",
    "# 5.  **Ki·ªÉm tra l·∫°i:** Tr∆∞·ªõc khi ho√†n t·∫•t, ƒë·∫£m b·∫£o:\n",
    "#     * C√¢u tr·∫£ l·ªùi ho√†n to√†n b·∫±ng ti·∫øng Vi·ªát.\n",
    "#     * Ch·ªâ ch·ª©a M·ªòT phi√™n b·∫£n gi·∫£i th√≠ch (ho·∫∑c chi ti·∫øt ho·∫∑c ƒë∆°n gi·∫£n), kh√¥ng ƒë·ªÅ c·∫≠p ƒë·∫øn phi√™n b·∫£n c√≤n l·∫°i.\n",
    "#     * N·ªôi dung d·ª±a tr√™n ngu·ªìn th√¥ng tin ƒë∆∞·ª£c cung c·∫•p.\n",
    "\n",
    "# R√†ng bu·ªôc quan tr·ªçng:\n",
    "# ‚ùóÔ∏èCh·ªâ cung c·∫•p m·ªôt phi√™n b·∫£n gi·∫£i th√≠ch ph√π h·ª£p v·ªõi tr√¨nh ƒë·ªô ng∆∞·ªùi d√πng. KH√îNG nh·∫Øc ƒë·∫øn ho·∫∑c ƒë∆∞a ra gi·∫£i th√≠ch cho tr√¨nh ƒë·ªô kh√°c.\n",
    "# ‚ö†Ô∏èTr·∫£ l·ªùi ho√†n to√†n b·∫±ng ti·∫øng Vi·ªát.\n",
    "\n",
    "# Th√¥ng tin ƒë∆∞·ª£c cung c·∫•p:\n",
    "# --------------------------\n",
    "# Ng·ªØ c·∫£nh:\n",
    "# {{context}}\n",
    "\n",
    "# Th√¥ng tin ng∆∞·ªùi d√πng:\n",
    "# {% for k, v in user_info.items() %}\n",
    "#   {{k}}: {{v}}\n",
    "# {% endfor %}\n",
    "\n",
    "# T√†i li·ªáu t·ª´ web:\n",
    "# {% for document in documents %}\n",
    "#   {{document.content}}\n",
    "# {% endfor %}\n",
    "\n",
    "# URL c·ªßa t√†i li·ªáu:\n",
    "# {% for document in web_urls %}\n",
    "#   {% if document.meta.url != \"https://www.example.com\" %}\n",
    "#     {{document.meta.url}}\n",
    "#   {% endif %}\n",
    "# {% endfor %}\n",
    "\n",
    "# C√¢u h·ªèi: {{query}}\n",
    "# --------------------------\n",
    "\n",
    "# C√¢u tr·∫£ l·ªùi (ƒë√£ ƒëi·ªÅu ch·ªânh theo c√°c b∆∞·ªõc tr√™n):\n",
    "# \"\"\"\n",
    "\n",
    "# prompt_template_after_documents = r\"\"\"\n",
    "# B·∫°n l√† m·ªôt Chuy√™n gia Gi·∫£i th√≠ch AI, c√≥ kh·∫£ nƒÉng ph√¢n t√≠ch t√†i li·ªáu v√† tr√¨nh b√†y l·∫°i n·ªôi dung m·ªôt c√°ch ph√π h·ª£p v·ªõi tr√¨nh ƒë·ªô c·ªßa sinh vi√™n.\n",
    "\n",
    "# M·ª•c ti√™u: Tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa sinh vi√™n ch·ªâ d·ª±a tr√™n T√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p v√† Ng·ªØ c·∫£nh (ƒë√°nh gi√° c·ªßa gi√°o vi√™n). C√¢u tr·∫£ l·ªùi ph·∫£i ƒë∆∞·ª£c ƒëi·ªÅu ch·ªânh ƒë·ªô ph·ª©c t·∫°p **ch·ªâ d·ª±a tr√™n ƒëi·ªÉm s·ªë c·ªßa ng∆∞·ªùi d√πng trong m√¥n h·ªçc li√™n quan** ƒë∆∞·ª£c n√™u trong Ng·ªØ c·∫£nh.\n",
    "\n",
    "# V√≠ d·ª• v·ªÅ c√°ch ƒëi·ªÅu ch·ªânh:\n",
    "# --------------------------\n",
    "# V√≠ d·ª• 1 (Sinh vi√™n ƒëi·ªÉm cao m√¥n li√™n quan):\n",
    "# C√¢u h·ªèi: Gi·∫£i th√≠ch nguy√™n l√Ω ho·∫°t ƒë·ªông c·ªßa b·ªô nh·ªõ RAM.\n",
    "# C√¢u tr·∫£ l·ªùi (chuy√™n s√¢u): RAM (Random Access Memory) l√† b·ªô nh·ªõ truy c·∫≠p ng·∫´u nhi√™n, s·ª≠ d·ª•ng c√°c t·∫ø b√†o nh·ªõ d·ª±a tr√™n t·ª• ƒëi·ªán v√† transistor ƒë·ªÉ l∆∞u tr·ªØ d·ªØ li·ªáu t·∫°m th·ªùi d∆∞·ªõi d·∫°ng bit nh·ªã ph√¢n. T·ªëc ƒë·ªô truy c·∫≠p nhanh nh∆∞ng d·ªØ li·ªáu s·∫Ω m·∫•t khi kh√¥ng c√≥ ngu·ªìn ƒëi·ªán (volatile memory). C√°c tham s·ªë quan tr·ªçng bao g·ªìm dung l∆∞·ª£ng, t·ªëc ƒë·ªô bus, ƒë·ªô tr·ªÖ (latency timings nh∆∞ CAS Latency)...\n",
    "\n",
    "# V√≠ d·ª• 2 (Sinh vi√™n ƒëi·ªÉm th·∫•p m√¥n li√™n quan):\n",
    "# C√¢u h·ªèi: Gi·∫£i th√≠ch nguy√™n l√Ω ho·∫°t ƒë·ªông c·ªßa b·ªô nh·ªõ RAM.\n",
    "# C√¢u tr·∫£ l·ªùi (ƒë∆°n gi·∫£n): RAM gi·ªëng nh∆∞ b√†n l√†m vi·ªác t·∫°m th·ªùi c·ªßa m√°y t√≠nh. Khi b·∫°n m·ªü ch∆∞∆°ng tr√¨nh hay file, m√°y t√≠nh s·∫Ω ƒë·∫∑t th√¥ng tin c·∫ßn d√πng l√™n RAM ƒë·ªÉ x·ª≠ l√Ω cho nhanh. N√≥ nhanh h∆°n ·ªï c·ª©ng nhi·ªÅu, nh∆∞ng khi t·∫Øt m√°y th√¨ m·ªçi th·ª© tr√™n RAM s·∫Ω m·∫•t ƒëi. Dung l∆∞·ª£ng RAM c√†ng l·ªõn th√¨ m√°y ch·∫°y c√†ng m∆∞·ª£t khi m·ªü nhi·ªÅu th·ª© c√πng l√∫c.\n",
    "# --------------------------\n",
    "\n",
    "# Th·ª±c hi·ªán c√°c b∆∞·ªõc sau ƒë·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi:\n",
    "# 1.  **X√°c ƒë·ªãnh m√¥n h·ªçc li√™n quan:** D·ª±a v√†o C√¢u h·ªèi v√† Ng·ªØ c·∫£nh, x√°c ƒë·ªãnh m√¥n h·ªçc ch√≠nh.\n",
    "# 2.  **Ki·ªÉm tra ƒë√°nh gi√°/ƒëi·ªÉm s·ªë:** Xem x√©t ƒë√°nh gi√° c·ªßa gi√°o vi√™n trong Ng·ªØ c·∫£nh ho·∫∑c ƒëi·ªÉm s·ªë li√™n quan.\n",
    "# 3.  **Quy·∫øt ƒë·ªãnh m·ª©c ƒë·ªô:** D·ª±a tr√™n ƒë√°nh gi√°/ƒëi·ªÉm s·ªë (cao >= 7.0, th·∫•p < 7.0), ch·ªçn M·ªòT m·ª©c ƒë·ªô gi·∫£i th√≠ch: chuy√™n s√¢u/ƒë·∫ßy ƒë·ªß (ƒëi·ªÉm cao) HO·∫∂C ƒë∆°n gi·∫£n/d·ªÖ hi·ªÉu (ƒëi·ªÉm th·∫•p).\n",
    "# 4.  **T·∫°o c√¢u tr·∫£ l·ªùi:** So·∫°n th·∫£o c√¢u tr·∫£ l·ªùi ch·ªâ ·ªü m·ª©c ƒë·ªô ƒë√£ ch·ªçn, s·ª≠ d·ª•ng th√¥ng tin CH·ªà t·ª´ T√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p.\n",
    "# 5.  **Ki·ªÉm tra l·∫°i:** Tr∆∞·ªõc khi ho√†n t·∫•t, ƒë·∫£m b·∫£o:\n",
    "#     * C√¢u tr·∫£ l·ªùi ho√†n to√†n b·∫±ng ti·∫øng Vi·ªát.\n",
    "#     * Ch·ªâ ch·ª©a M·ªòT phi√™n b·∫£n gi·∫£i th√≠ch, kh√¥ng ƒë·ªÅ c·∫≠p ƒë·∫øn phi√™n b·∫£n c√≤n l·∫°i.\n",
    "#     * N·ªôi dung ho√†n to√†n d·ª±a tr√™n T√†i li·ªáu ƒë√£ cho.\n",
    "\n",
    "# R√†ng bu·ªôc quan tr·ªçng:\n",
    "# ‚ùóÔ∏èKh√¥ng ƒë∆∞a ra nhi·ªÅu phi√™n b·∫£n. Ch·ªâ cung c·∫•p m·ªôt phi√™n b·∫£n duy nh·∫•t ph√π h·ª£p v·ªõi tr√¨nh ƒë·ªô ng∆∞·ªùi d√πng.\n",
    "# ‚ö†Ô∏èT·∫•t c·∫£ c√¢u tr·∫£ l·ªùi ph·∫£i b·∫±ng ti·∫øng Vi·ªát.\n",
    "\n",
    "# Th√¥ng tin ƒë∆∞·ª£c cung c·∫•p:\n",
    "# --------------------------\n",
    "# Ng·ªØ c·∫£nh (ƒê√°nh gi√° c·ªßa gi√°o vi√™n):\n",
    "# {{context}}\n",
    "\n",
    "# T√†i li·ªáu:\n",
    "# {% for document in documents %}\n",
    "#   {{document.content}}\n",
    "# {% endfor %}\n",
    "\n",
    "# C√¢u h·ªèi: {{query}}\n",
    "# --------------------------\n",
    "\n",
    "# C√¢u tr·∫£ l·ªùi (ƒë√£ ƒëi·ªÅu ch·ªânh theo c√°c b∆∞·ªõc tr√™n):\n",
    "# \"\"\"\n",
    "\n",
    "# prompt_template_after_user_info = r\"\"\"\n",
    "# B·∫°n l√† m·ªôt Tr·ª£ l√Ω H·ªçc t·∫≠p AI c√° nh√¢n h√≥a, t·∫≠p trung v√†o vi·ªác gi·∫£i ƒë√°p th·∫Øc m·∫Øc d·ª±a tr√™n nƒÉng l·ª±c h·ªçc t·∫≠p c·ªßa sinh vi√™n.\n",
    "\n",
    "# M·ª•c ti√™u: Tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa sinh vi√™n d·ª±a tr√™n Ng·ªØ c·∫£nh (ƒë√°nh gi√° c·ªßa gi√°o vi√™n) v√† Th√¥ng tin ng∆∞·ªùi d√πng (ƒëi·ªÉm s·ªë chi ti·∫øt). C√¢u tr·∫£ l·ªùi ph·∫£i ƒë∆∞·ª£c ƒëi·ªÅu ch·ªânh ƒë·ªô ph·ª©c t·∫°p **ch·ªâ d·ª±a tr√™n ƒëi·ªÉm s·ªë c·ªßa ng∆∞·ªùi d√πng trong m√¥n h·ªçc li√™n quan**.\n",
    "\n",
    "# V√≠ d·ª• v·ªÅ c√°ch ƒëi·ªÅu ch·ªânh:\n",
    "# --------------------------\n",
    "# V√≠ d·ª• 1 (Sinh vi√™n ƒëi·ªÉm cao m√¥n Sinh h·ªçc):\n",
    "# C√¢u h·ªèi: Quang h·ª£p l√† g√¨?\n",
    "# C√¢u tr·∫£ l·ªùi (chi ti·∫øt): Quang h·ª£p l√† qu√° tr√¨nh sinh h√≥a ph·ª©c t·∫°p trong ƒë√≥ nƒÉng l∆∞·ª£ng √°nh s√°ng m·∫∑t tr·ªùi ƒë∆∞·ª£c th·ª±c v·∫≠t, t·∫£o v√† vi khu·∫©n lam chuy·ªÉn h√≥a th√†nh nƒÉng l∆∞·ª£ng h√≥a h·ªçc d·ª± tr·ªØ trong c√°c h·ª£p ch·∫•t h·ªØu c∆° (glucose). Qu√° tr√¨nh n√†y di·ªÖn ra ch·ªß y·∫øu ·ªü l·ª•c l·∫°p, s·ª≠ d·ª•ng CO2, n∆∞·ªõc v√† √°nh s√°ng, gi·∫£i ph√≥ng oxy. Ph∆∞∆°ng tr√¨nh t·ªïng qu√°t: 6CO_2 + 6H_2O \\xrightarrow{√Ånh s√°ng, Di·ªáp l·ª•c} C_6H_{12}O_6 + 6O_26CO_2 + 6H_2O \\xrightarrow{√Ånh s√°ng, Di·ªáp l·ª•c} C_6H_{12}O_6 + 6O_2. N√≥ bao g·ªìm pha s√°ng (ph·ª• thu·ªôc √°nh s√°ng) v√† pha t·ªëi (chu tr√¨nh Calvin).\n",
    "\n",
    "# V√≠ d·ª• 2 (Sinh vi√™n ƒëi·ªÉm th·∫•p m√¥n Sinh h·ªçc):\n",
    "# C√¢u h·ªèi: Quang h·ª£p l√† g√¨?\n",
    "# C√¢u tr·∫£ l·ªùi (ƒë∆°n gi·∫£n): Quang h·ª£p l√† c√°ch c√¢y xanh \"ƒÉn\" b·∫±ng √°nh s√°ng m·∫∑t tr·ªùi. C√¢y l·∫•y kh√≠ cacbonic (CO2) t·ª´ kh√¥ng kh√≠, n∆∞·ªõc t·ª´ ƒë·∫•t, r·ªìi d√πng nƒÉng l∆∞·ª£ng m·∫∑t tr·ªùi ƒë·ªÉ t·∫°o ra th·ª©c ƒÉn (ƒë∆∞·ªùng glucose) cho ch√≠nh n√≥ v√† th·∫£i ra kh√≠ oxy m√† ch√∫ng ta th·ªü. Gi·ªëng nh∆∞ c√¢y ƒëang n·∫•u ƒÉn b·∫±ng √°nh s√°ng v·∫≠y.\n",
    "# --------------------------\n",
    "\n",
    "# Th·ª±c hi·ªán c√°c b∆∞·ªõc sau ƒë·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi:\n",
    "# 1.  **X√°c ƒë·ªãnh m√¥n h·ªçc li√™n quan:** D·ª±a v√†o C√¢u h·ªèi, x√°c ƒë·ªãnh m√¥n h·ªçc ch√≠nh.\n",
    "# 2.  **Ki·ªÉm tra ƒëi·ªÉm s·ªë:** T√¨m ƒëi·ªÉm s·ªë c·ªßa sinh vi√™n trong m√¥n h·ªçc ƒë√≥ t·ª´ Th√¥ng tin ng∆∞·ªùi d√πng.\n",
    "# 3.  **Quy·∫øt ƒë·ªãnh m·ª©c ƒë·ªô:** D·ª±a tr√™n ƒëi·ªÉm s·ªë (cao >= 7.0, th·∫•p < 7.0), ch·ªçn M·ªòT m·ª©c ƒë·ªô gi·∫£i th√≠ch: chi ti·∫øt/ƒë·∫ßy ƒë·ªß/chuy√™n s√¢u (ƒëi·ªÉm cao) HO·∫∂C ng·∫Øn g·ªçn/ƒë∆°n gi·∫£n/d·ªÖ hi·ªÉu (ƒëi·ªÉm th·∫•p).\n",
    "# 4.  **T·∫°o c√¢u tr·∫£ l·ªùi:** So·∫°n th·∫£o c√¢u tr·∫£ l·ªùi ch·ªâ ·ªü m·ª©c ƒë·ªô ƒë√£ ch·ªçn, d·ª±a tr√™n ki·∫øn th·ª©c chung v√† th√¥ng tin t·ª´ Ng·ªØ c·∫£nh.\n",
    "# 5.  **Ki·ªÉm tra l·∫°i:** Tr∆∞·ªõc khi ho√†n t·∫•t, ƒë·∫£m b·∫£o:\n",
    "#     * C√¢u tr·∫£ l·ªùi ho√†n to√†n b·∫±ng ti·∫øng Vi·ªát.\n",
    "#     * Ch·ªâ ch·ª©a M·ªòT phi√™n b·∫£n gi·∫£i th√≠ch, kh√¥ng ƒë·ªÅ c·∫≠p ƒë·∫øn phi√™n b·∫£n c√≤n l·∫°i.\n",
    "#     * Gi·∫£i th√≠ch ph√π h·ª£p v·ªõi ƒë√°nh gi√° trong Ng·ªØ c·∫£nh.\n",
    "\n",
    "# R√†ng bu·ªôc quan tr·ªçng:\n",
    "# ‚ùóÔ∏èKH√îNG ƒë·ªÅ c·∫≠p ƒë·∫øn ho·∫∑c g·ªôp c·∫£ hai ki·ªÉu gi·∫£i th√≠ch. Ch·ªâ tr·∫£ l·ªùi theo m·ªôt c·∫•p ƒë·ªô ph√π h·ª£p.\n",
    "# ‚ö†Ô∏èCh·ªâ s·ª≠ d·ª•ng ti·∫øng Vi·ªát cho to√†n b·ªô n·ªôi dung c√¢u tr·∫£ l·ªùi.\n",
    "\n",
    "# Th√¥ng tin ƒë∆∞·ª£c cung c·∫•p:\n",
    "# --------------------------\n",
    "# Ng·ªØ c·∫£nh (ƒê√°nh gi√° c·ªßa gi√°o vi√™n):\n",
    "# {{context}}\n",
    "\n",
    "# Th√¥ng tin ng∆∞·ªùi d√πng:\n",
    "# {% for k, v in user_info.items() %}\n",
    "#   {{k}}: {{v}}\n",
    "# {% endfor %}\n",
    "\n",
    "# C√¢u h·ªèi: {{query}}\n",
    "# --------------------------\n",
    "\n",
    "# C√¢u tr·∫£ l·ªùi (ƒë√£ ƒëi·ªÅu ch·ªânh theo c√°c b∆∞·ªõc tr√™n):\n",
    "# \"\"\"\n",
    "\n",
    "# # ==============================================================\n",
    "# # PROMPTS TI·ªÜN √çCH KH√ÅC (T√ìM T·∫ÆT, VI·∫æT L·∫†I C√ÇU H·ªéI)\n",
    "# # ==============================================================\n",
    "\n",
    "# propmt_chathist_summarize = r\"\"\"\n",
    "# Ph√¢n t√≠ch l·ªãch s·ª≠ tr√≤ chuy·ªán ƒë∆∞·ª£c cung c·∫•p ƒë·ªÉ x√°c ƒë·ªãnh ƒëi·ªÉm m·∫°nh, ƒëi·ªÉm y·∫øu h·ªçc t·∫≠p (d·ª±a tr√™n c√°c c√¢u h·ªèi, c√¢u tr·∫£ l·ªùi tr∆∞·ªõc ƒë√≥) v√† m·ª•c ti√™u h·ªçc t·∫≠p ti·ªÅm nƒÉng c·ªßa ng∆∞·ªùi d√πng.\n",
    "# ƒê·∫£m b·∫£o r·∫±ng ph√¢n t√≠ch r√µ r√†ng, ng·∫Øn g·ªçn, t·∫≠p trung v√†o kh√≠a c·∫°nh h·ªçc thu·∫≠t v√† ƒë∆∞·ª£c tr√¨nh b√†y ho√†n to√†n b·∫±ng ti·∫øng Vi·ªát.\n",
    "\n",
    "# ‚ö†Ô∏èTo√†n b·ªô n·ªôi dung ph√¢n t√≠ch ph·∫£i b·∫±ng ti·∫øng Vi·ªát.\n",
    "\n",
    "# L·ªãch s·ª≠ tr√≤ chuy·ªán:\n",
    "# {chat_history}\n",
    "\n",
    "# Ph√¢n t√≠ch t√≥m t·∫Øt:\n",
    "# \"\"\"\n",
    "\n",
    "# prompt_rewritequery = r\"\"\"\n",
    "# B·∫°n l√† h·ªçc sinh v·ª´a nh·∫≠n ƒë∆∞·ª£c ƒë√°nh gi√° t·ª´ gi√°o vi√™n nh∆∞ sau:\n",
    "\n",
    "# ƒê√°nh gi√° t·ª´ gi√°o vi√™n:\n",
    "# {context}\n",
    "\n",
    "# ƒê√¢y l√† c√¢u h·ªèi c≈© b·∫°n ƒë√£ ƒë·∫∑t:\n",
    "# {query}\n",
    "\n",
    "# D·ª±a tr√™n ƒë√°nh gi√° c·ªßa gi√°o vi√™n v·ªÅ nƒÉng l·ª±c c·ªßa b·∫°n, h√£y vi·∫øt l·∫°i c√¢u h·ªèi c≈© sao cho ph√π h·ª£p h∆°n v·ªõi tr√¨nh ƒë·ªô hi·ªán t·∫°i c·ªßa b·∫°n v√† gi√∫p b·∫°n hi·ªÉu r√µ v·∫•n ƒë·ªÅ h∆°n.\n",
    "# Ch·ªâ vi·∫øt l·∫°i c√¢u h·ªèi, KH√îNG th√™m l·ªùi nh·∫≠n x√©t hay gi·∫£i th√≠ch n√†o kh√°c.\n",
    "\n",
    "# ‚ö†Ô∏èTr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát.\n",
    "\n",
    "# C√¢u h·ªèi m·ªõi (ƒë√£ vi·∫øt l·∫°i):\n",
    "# \"\"\"\n",
    "\n",
    "# prompt_getemotion = r\"\"\"\n",
    "# B·∫°n l√† m·ªôt chuy√™n gia ph√¢n lo·∫°i c·∫£m x√∫c. Nhi·ªám v·ª• c·ªßa b·∫°n l√† ph√¢n t√≠ch c·∫£m x√∫c c·ªßa c√¢u sau v√† cung c·∫•p ph·∫£n h·ªìi ƒë∆∞·ª£c ƒëi·ªÅu ch·ªânh.\n",
    "# Ch·ªâ ƒë∆∞·ª£c tr·∫£ l·ªùi b·∫±ng m·ªôt trong ba nh√£n sau: [\"Positive\", \"Negative\", \"Neutral\"]\n",
    "\n",
    "# C√¢u n√≥i c·ªßa ng∆∞·ªùi d√πng:\n",
    "# {query}\n",
    "# \"\"\"\n",
    "\n",
    "# prompt_emolize = r\"\"\"\n",
    "# ƒê∆∞a ra ph·∫£n h·ªìi cu·ªëi c√πng b·∫±ng ti·∫øng Vi·ªát, l√† vƒÉn b·∫£n ƒë∆∞·ª£c ƒëi·ªÅu ch·ªânh theo t√¥ng gi·ªçng ph√π h·ª£p v·ªõi c·∫£m x√∫c\n",
    "\n",
    "# H√£y tr·∫£ l·ªùi theo m·∫´u:\n",
    "# [{nhan_phu}] [VƒÉn b·∫£n ƒëaa4 ƒë∆∞·ª£c ƒëi·ªÅu ch·ªânh] [{emoji}]\n",
    "\n",
    "# VƒÉn b·∫£n c·∫ßn ƒë∆∞·ª£c ƒëi·ªÅu ch·ªânh theo t√¥ng gi·ªçng {tong}:\n",
    "# {message}\n",
    "# \"\"\"\n",
    "\n",
    "# def get_emo(query):\n",
    "#     prompt = prompt_getemotion.format(query = query)\n",
    "#     res = OllamaGenerator(model=llmname_generate).run(prompt)[\"replies\"][0]\n",
    "\n",
    "#     if \"Positive\" in res:\n",
    "#         return \"Positive\"\n",
    "#     if \"Negative\" in res:\n",
    "#         return \"Negative\"\n",
    "#     elif \"Neutral\" in res:\n",
    "#         return \"Neutral\"\n",
    "\n",
    "# def get_emodata(emo:str):\n",
    "#     if emo == \"Positive\":\n",
    "#         return \"C·∫£m x√∫c: Vui v·∫ª, T√≠ch c·ª±c\", \"Vui v·∫ª, nhi·ªát t√¨nh, t√≠ch c·ª±c\",  [\"üòÑ\", \"üòÇ\", \"ü§£\", \"üòÖ\", \"üòä\", \"ü•∞\", \"üòç\", \"üòÜ\", \"üòá\", \"üòÅ\"]\n",
    "#     if emo == \"Negative\":\n",
    "#         return \"C·∫£m x√∫c: Bu·ªìn b√£, Ti√™u c·ª±c\", \"C·∫£m th√¥ng, nh·∫π nh√†ng, hi·ªÉu bi·∫øt\", [\"üò¨\", \"üòó\", \"ü§í\", \"üòØ\", \"üòï\", \"üòñ\", \"üò±\", \"üò®\", \"üò∞\", \"üò≥\"]\n",
    "#     elif emo ==\"Neutral\":\n",
    "#         return \"C·∫£m x√∫c: B√¨nh th∆∞·ªùng\", \"Trung l·∫≠p, kh√°ch quan\",  [\"üëÄ\", \"ü§ó\", \"ü§î\", \"üßê\", \"üòé\", \"ü§Ø\", \"üëª\", \"üëΩ\", \"ü§ñ\", \"üëæ\"]\n",
    "\n",
    "# def emolize(message, emodata):\n",
    "#     nhan_phu, tong, emoji = emodata\n",
    "#     emoji = random.choice(emoji)\n",
    "\n",
    "#     prompt = prompt_emolize.format(nhan_phu=nhan_phu, tong=tong, emoji=emoji, message=message)\n",
    "#     return OllamaGenerator(model=llmname_generate).run(prompt)[\"replies\"][0]\n",
    "\n",
    "# def fulll_emolize(query, message):\n",
    "#     emodata = get_emodata(get_emo(query))   \n",
    "#     message = emolize(message, emodata)\n",
    "    \n",
    "#     return message\n",
    "\n",
    "\n",
    "# chat_history = {\n",
    "#   'user': None,\n",
    "#   'assistant': None\n",
    "# }\n",
    "\n",
    "# def context_init() -> str:\n",
    "#   return OllamaGenerator(model=llmname_generate).run(prompt_context_init)[\"replies\"][0]\n",
    "\n",
    "# def context_combine(curr, chat_hist, user_info) -> str:\n",
    "#   chat_hist = summarize_chathist(chat_hist)\n",
    "\n",
    "#   return OllamaGenerator(model=llmname_generate).run(prompt_context_combine.format(chat_history=chat_hist, current_thoughts=curr, user_info=user_info))[\"replies\"][0]\n",
    "\n",
    "# def summarize_chathist(chat_hist) -> str:\n",
    "#   return OllamaGenerator(model=llmname_generate).run(propmt_chathist_summarize.format(chat_history=chat_hist))[\"replies\"][0]\n",
    "\n",
    "# def rewrite_query(question, context)-> str:\n",
    "#   return OllamaGenerator(model=llmname_generate).run(prompt_rewritequery.format(query=question, context=context))[\"replies\"][0]\n",
    "\n",
    "# def run_single(pipe, question, run_dict):\n",
    "#   result = pipe.run(run_dict)\n",
    "\n",
    "#   for i in range(8): # at max 16 tries\n",
    "#       if \"regenerate\" not in result[\"hallu_router\"]:\n",
    "#         break\n",
    "\n",
    "#       print(\"Checking at iteration\", i)\n",
    "#       result = pipe.run(run_dict)\n",
    "#   else:\n",
    "#     return result\n",
    "\n",
    "#   return result[\"hallu_router\"][\"pass_answer\"]\n",
    "\n",
    "# llmname_generate = \"gemma3:12b\"\n",
    "# llmname_route = \"gemma3:12b\"\n",
    "\n",
    "\n",
    "# context = context_init()\n",
    "# def extract_figure_from_pdf(pdf_file, figure_number: int):\n",
    "#     \"\"\"\n",
    "#     Extracts the image corresponding to 'h√¨nh {figure_number}' from the PDF.\n",
    "#     Returns a PIL Image or None.\n",
    "#     \"\"\"\n",
    "#     with fitz.open(stream=pdf_file.getvalue(), filetype=\"pdf\") as pdf_document:\n",
    "#         count = 0\n",
    "#         for page_num in range(pdf_document.page_count):\n",
    "#             page = pdf_document.load_page(page_num)\n",
    "#             images = page.get_images(full=True)\n",
    "#             for img_index, img in enumerate(images):\n",
    "#                 count += 1\n",
    "#                 if count == figure_number:\n",
    "#                     xref = img[0]\n",
    "#                     image_info = pdf_document.extract_image(xref)\n",
    "#                     image_bytes = image_info[\"image\"]\n",
    "#                     return Image.open(io.BytesIO(image_bytes))\n",
    "#     return None\n",
    "\n",
    "# def extract_table_from_pdf(pdf_file, table_number: int):\n",
    "#     \"\"\"\n",
    "#     Extracts the table corresponding to 'b·∫£ng {table_number}' from the PDF.\n",
    "#     Returns table text or None.\n",
    "#     \"\"\"\n",
    "#     import pdfplumber\n",
    "#     count = 0\n",
    "#     with pdfplumber.open(pdf_file) as pdf:\n",
    "#         for page in pdf.pages:\n",
    "#             tables = page.extract_tables()\n",
    "#             for table in tables:\n",
    "#                 count += 1\n",
    "#                 if count == table_number:\n",
    "#                     # Convert table to text\n",
    "#                     table_text = \"\\n\".join([\"\\t\".join([str(cell) for cell in row]) for row in table])\n",
    "#                     return table_text\n",
    "#     return None\n",
    "\n",
    "# def answer_figure_with_gemma3(image: Image.Image, user_query: str, user_info: dict, context: str):\n",
    "#     \"\"\"\n",
    "#     Use Gemma3 Vision LLM to answer a user query about a figure, personalized by user_info/context.\n",
    "#     \"\"\"\n",
    "#     # Save image to temp file for ollama\n",
    "#     with tempfile.NamedTemporaryFile(suffix=\".png\", delete=False) as tmp:\n",
    "#         image.save(tmp.name)\n",
    "#         prompt = f\"\"\"\n",
    "#     B·∫°n l√† m·ªôt Chuy√™n gia Gi·∫£i th√≠ch AI, c√≥ kh·∫£ nƒÉng ph√¢n t√≠ch h√¨nh ·∫£nh v√† tr√¨nh b√†y l·∫°i n·ªôi dung m·ªôt c√°ch ph√π h·ª£p v·ªõi tr√¨nh ƒë·ªô c·ªßa sinh vi√™n.\n",
    "    \n",
    "#     M·ª•c ti√™u: Tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa sinh vi√™n v·ªÅ h√¨nh ·∫£nh d∆∞·ªõi ƒë√¢y, d·ª±a tr√™n Ng·ªØ c·∫£nh (ƒë√°nh gi√° c·ªßa gi√°o vi√™n) v√† ƒëi·ªÉm s·ªë c·ªßa sinh vi√™n trong m√¥n h·ªçc li√™n quan ƒë·ªÉ ƒëi·ªÅu ch·ªânh c√¢u tr·∫£ l·ªùi ph√π h·ª£p nh·∫•t c√≥ th·ªÉ ƒë·ªëi v·ªõi sinh vi√™n.\n",
    "\n",
    "#     V√≠ d·ª• v·ªÅ c√°ch ƒëi·ªÅu ch·ªânh:\n",
    "#     --------------------------\n",
    "#     V√≠ d·ª• 1 (Sinh vi√™n ƒëi·ªÉm cao m√¥n li√™n quan):\n",
    "#     C√¢u h·ªèi: Gi·∫£i th√≠ch nguy√™n l√Ω ho·∫°t ƒë·ªông c·ªßa b·ªô nh·ªõ RAM.\n",
    "#     C√¢u tr·∫£ l·ªùi (chuy√™n s√¢u): RAM (Random Access Memory) l√† b·ªô nh·ªõ truy c·∫≠p ng·∫´u nhi√™n... \n",
    "    \n",
    "#     V√≠ d·ª• 2 (Sinh vi√™n ƒëi·ªÉm th·∫•p m√¥n li√™n quan):\n",
    "#     C√¢u h·ªèi: Gi·∫£i th√≠ch nguy√™n l√Ω ho·∫°t ƒë·ªông c·ªßa b·ªô nh·ªõ RAM.\n",
    "#     C√¢u tr·∫£ l·ªùi (ƒë∆°n gi·∫£n): RAM gi·ªëng nh∆∞ b√†n l√†m vi·ªác t·∫°m th·ªùi c·ªßa m√°y t√≠nh... \n",
    "#     --------------------------\n",
    "    \n",
    "#     C√°c b∆∞·ªõc t·∫°o c√¢u tr·∫£ l·ªùi:\n",
    "#     1. **X√°c ƒë·ªãnh m√¥n h·ªçc li√™n quan** qua Ng·ªØ c·∫£nh ho·∫∑c t·ª´ C√¢u h·ªèi.\n",
    "#     2. **Ki·ªÉm tra ƒë√°nh gi√°/ƒëi·ªÉm s·ªë** trong Ng·ªØ c·∫£nh (cao >= 7.0 => chuy√™n s√¢u, th·∫•p < 7.0 => ƒë∆°n gi·∫£n).\n",
    "#     3. **So·∫°n th·∫£o** ch·ªâ 1 phi√™n b·∫£n gi·∫£i th√≠ch ph√π h·ª£p.\n",
    "#     4. **ƒê·∫£m b·∫£o**: to√†n b·∫±ng ti·∫øng Vi·ªát, kh√¥ng nh·∫Øc ƒë·∫øn c√°c m·ª©c ƒë·ªô kh√°c, ch·ªâ d√πng th√¥ng tin trong T√†i li·ªáu.\n",
    "        \n",
    "#     V√† d·ª±a tr√™n ng·ªØ c·∫£nh:\n",
    "#     {context}\n",
    "    \n",
    "#     V√† th√¥ng tin ng∆∞·ªùi d√πng:\n",
    "#     {user_info}\n",
    "    \n",
    "#     H√£y tr·∫£ l·ªùi c√¢u h·ªèi: {user_query}\n",
    "    \n",
    "#     H√£y tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, ƒëi·ªÅu ch·ªânh ƒë·ªô chi ti·∫øt/ph·ª©c t·∫°p d·ª±a tr√™n ƒëi·ªÉm s·ªë c·ªßa sinh vi√™n trong m√¥n h·ªçc li√™n quan. Ch·ªâ cung c·∫•p m·ªôt phi√™n b·∫£n ph√π h·ª£p v·ªõi tr√¨nh ƒë·ªô ng∆∞·ªùi d√πng.\n",
    "#     \"\"\"\n",
    "#     res = ollama.chat(\n",
    "#             model=\"gemma3:12b\",\n",
    "#             messages=[\n",
    "#                 {\n",
    "#                     'role': 'user',\n",
    "#                     'content': prompt,\n",
    "#                     'images': [tmp.name]\n",
    "#                 }\n",
    "#             ]\n",
    "#         )\n",
    "#     return res['message']['content']\n",
    "\n",
    "# def answer_table_with_gemma3(table_text: str, user_query: str, user_info: dict, context: str):\n",
    "#     \"\"\"\n",
    "#     Use Gemma3 LLM to answer a user query about a table, personalized by user_info/context.\n",
    "#     \"\"\"\n",
    "#     prompt = f\"\"\"\n",
    "#     B·∫°n l√† m·ªôt Chuy√™n gia Gi·∫£i th√≠ch AI, c√≥ kh·∫£ nƒÉng ph√¢n t√≠ch b·∫£ng d·ªØ li·ªáu v√† tr√¨nh b√†y l·∫°i n·ªôi dung m·ªôt c√°ch ph√π h·ª£p v·ªõi tr√¨nh ƒë·ªô c·ªßa sinh vi√™n.\n",
    "    \n",
    "#     M·ª•c ti√™u: Tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa sinh vi√™n v·ªÅ b·∫£ng d·ªØ li·ªáu d∆∞·ªõi ƒë√¢y, d·ª±a tr√™n Ng·ªØ c·∫£nh (ƒë√°nh gi√° c·ªßa gi√°o vi√™n) v√† ƒëi·ªÉm s·ªë c·ªßa sinh vi√™n trong m√¥n h·ªçc li√™n quan ƒë·ªÉ ƒëi·ªÅu ch·ªânh c√¢u tr·∫£ l·ªùi ph√π h·ª£p nh·∫•t c√≥ th·ªÉ ƒë·ªëi v·ªõi sinh vi√™n.\n",
    "\n",
    "#     V√≠ d·ª• v·ªÅ c√°ch ƒëi·ªÅu ch·ªânh:\n",
    "#     --------------------------\n",
    "#     V√≠ d·ª• 1 (Sinh vi√™n ƒëi·ªÉm cao m√¥n li√™n quan):\n",
    "#     C√¢u h·ªèi: Gi·∫£i th√≠ch nguy√™n l√Ω ho·∫°t ƒë·ªông c·ªßa b·ªô nh·ªõ RAM.\n",
    "#     C√¢u tr·∫£ l·ªùi (chuy√™n s√¢u): RAM (Random Access Memory) l√† b·ªô nh·ªõ truy c·∫≠p ng·∫´u nhi√™n... \n",
    "    \n",
    "#     V√≠ d·ª• 2 (Sinh vi√™n ƒëi·ªÉm th·∫•p m√¥n li√™n quan):\n",
    "#     C√¢u h·ªèi: Gi·∫£i th√≠ch nguy√™n l√Ω ho·∫°t ƒë·ªông c·ªßa b·ªô nh·ªõ RAM.\n",
    "#     C√¢u tr·∫£ l·ªùi (ƒë∆°n gi·∫£n): RAM gi·ªëng nh∆∞ b√†n l√†m vi·ªác t·∫°m th·ªùi c·ªßa m√°y t√≠nh... \n",
    "#     --------------------------\n",
    "    \n",
    "#     C√°c b∆∞·ªõc t·∫°o c√¢u tr·∫£ l·ªùi:\n",
    "#     1. **X√°c ƒë·ªãnh m√¥n h·ªçc li√™n quan** qua Ng·ªØ c·∫£nh ho·∫∑c t·ª´ C√¢u h·ªèi.\n",
    "#     2. **Ki·ªÉm tra ƒë√°nh gi√°/ƒëi·ªÉm s·ªë** trong Ng·ªØ c·∫£nh (cao >= 7.0 => chuy√™n s√¢u, th·∫•p < 7.0 => ƒë∆°n gi·∫£n).\n",
    "#     3. **So·∫°n th·∫£o** ch·ªâ 1 phi√™n b·∫£n gi·∫£i th√≠ch ph√π h·ª£p.\n",
    "#     4. **ƒê·∫£m b·∫£o**: to√†n b·∫±ng ti·∫øng Vi·ªát, kh√¥ng nh·∫Øc ƒë·∫øn c√°c m·ª©c ƒë·ªô kh√°c, ch·ªâ d√πng th√¥ng tin trong T√†i li·ªáu.\n",
    "    \n",
    "#     Ng·ªØ c·∫£nh (ƒê√°nh gi√° c·ªßa gi√°o vi√™n):\n",
    "#     {context}\n",
    "    \n",
    "#     Th√¥ng tin ng∆∞·ªùi d√πng:\n",
    "#     {user_info}\n",
    "    \n",
    "#     N·ªôi dung b·∫£ng:\n",
    "#     {table_text}\n",
    "    \n",
    "#     C√¢u h·ªèi: {user_query}\n",
    "    \n",
    "#     H√£y tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, ƒëi·ªÅu ch·ªânh ƒë·ªô chi ti·∫øt/ph·ª©c t·∫°p d·ª±a tr√™n ƒëi·ªÉm s·ªë c·ªßa sinh vi√™n trong m√¥n h·ªçc li√™n quan. Ch·ªâ cung c·∫•p m·ªôt phi√™n b·∫£n ph√π h·ª£p v·ªõi tr√¨nh ƒë·ªô ng∆∞·ªùi d√πng.\n",
    "#     \"\"\"\n",
    "#     return OllamaGenerator(model=\"gemma3:12b\").run(prompt)[\"replies\"][0]\n",
    "\n",
    "# def personalized_answer_from_summary(summary: str, user_query: str, user_info: dict):\n",
    "#     \"\"\"\n",
    "#     Use Gemma3 LLM to generate a personalized answer based on the summary, user query, and student transcript.\n",
    "#     \"\"\"\n",
    "#     rubric = \"\"\"\n",
    "#     B·∫°n l√† m·ªôt gi√°o vi√™n si√™u nghi√™m kh·∫Øc, ng∆∞·ªùi ƒë√°nh gi√° hi·ªáu su·∫•t c·ªßa h·ªçc sinh v·ªõi ƒë·ªô ch√≠nh x√°c kh√¥ng ng·ª´ng ngh·ªâ.\n",
    "#     C√¥ng vi·ªác c·ªßa b·∫°n l√† ch·∫•m ƒëi·ªÉm t·ª´ng m√¥n h·ªçc d·ª±a tr√™n ƒëi·ªÉm s·ªë c·ªßa h·ªçc sinh v√† cung c·∫•p ph·∫£n h·ªìi nghi√™m kh·∫Øc v√† chi ti·∫øt.\n",
    "#     S·ª≠ d·ª•ng c√°c quy t·∫Øc v√† m√¥ t·∫£ sau ƒë·ªÉ ƒë√°nh gi√° h·ªçc sinh:\n",
    "\n",
    "#     Ti√™u ch√≠ ch·∫•m ƒëi·ªÉm:\n",
    "#     - ƒêi·ªÉm t·ª´ 9.0 ƒë·∫øn 10.0: Tuy·ªát v·ªùi! - B·∫°n ƒë√£ th·ªÉ hi·ªán s·ª± n·∫Øm v·ªØng ki·∫øn th·ª©c v√† k·ªπ nƒÉng xu·∫•t s·∫Øc trong m√¥n h·ªçc n√†y. N·ªó l·ª±c v√† s·ª± ch√≠nh x√°c c·ªßa b·∫°n r·∫•t ƒë√°ng khen ng·ª£i. H√£y ti·∫øp t·ª•c duy tr√¨ phong ƒë·ªô tuy·ªát v·ªùi n√†y v√† th·ª≠ th√°ch b·∫£n th√¢n v·ªõi nh·ªØng ki·∫øn th·ª©c s√¢u h∆°n nh√©!\n",
    "#     - ƒêi·ªÉm t·ª´ 8.0 ƒë·∫øn 8.9: R·∫•t t·ªët! - B·∫°n hi·ªÉu b√†i r·∫•t t·ªët v√† th·ªÉ hi·ªán nƒÉng l·ª±c ·∫•n t∆∞·ª£ng. Ch·ªâ c√≤n m·ªôt v√†i ƒëi·ªÉm nh·ªè n·ªØa l√† ƒë·∫°t ƒë·∫øn m·ª©c ho√†n h·∫£o. C√πng xem l·∫°i nh·ªØng chi ti·∫øt nh·ªè n√†y ƒë·ªÉ gi√∫p b·∫°n ho√†n thi·ªán h∆°n n·ªØa ki·∫øn th·ª©c v√† k·ªπ nƒÉng c·ªßa m√¨nh.\n",
    "#     - ƒêi·ªÉm t·ª´ 7.0 ƒë·∫øn 7.9: T·ªët! - B·∫°n ƒë√£ n·∫Øm ƒë∆∞·ª£c nh·ªØng ki·∫øn th·ª©c v√† k·ªπ nƒÉng c∆° b·∫£n, quan tr·ªçng c·ªßa m√¥n h·ªçc. ƒê√¢y l√† m·ªôt n·ªÅn t·∫£ng t·ªët. ƒê·ªÉ hi·ªÉu s√¢u s·∫Øc h∆°n, b·∫°n c√≥ th·ªÉ t·∫≠p trung th√™m v√†o vi·ªác c·ªßng c·ªë [ch·ªâ ra lƒ©nh v·ª±c c·ª• th·ªÉ n·∫øu c√≥ th·ªÉ] v√† luy·ªán t·∫≠p th√™m. ƒê·ª´ng ng·∫ßn ng·∫°i ƒë·∫∑t c√¢u h·ªèi v·ªÅ nh·ªØng ph·∫ßn b·∫°n c√≤n bƒÉn khoƒÉn.\n",
    "#     - 6.0 ƒë·∫øn 6.9: Kh√°! - C√≥ s·ª± c·ªë g·∫Øng r√µ r·ªát v√† b·∫°n ƒë√£ n·∫Øm b·∫Øt ƒë∆∞·ª£c m·ªôt ph·∫ßn ki·∫øn th·ª©c. Tuy nhi√™n, s·ª± hi·ªÉu bi·∫øt c·∫ßn ƒë∆∞·ª£c ƒë√†o s√¢u h∆°n ƒë·ªÉ th·ª±c s·ª± v·ªØng v√†ng. H√£y c√πng x√°c ƒë·ªãnh nh·ªØng ph·∫ßn ki·∫øn th·ª©c c·∫ßn c·ªßng c·ªë th√™m. M·ªôt s·ªë g·ª£i √Ω c·ª• th·ªÉ c√≥ th·ªÉ gi√∫p b·∫°n ti·∫øn b·ªô nhanh h∆°n ƒë·∫•y.\n",
    "#     - ƒêi·ªÉm t·ª´ 5.0 ƒë·∫øn 5.9: C·∫ßn c·ªë g·∫Øng h∆°n! - C√≥ v·∫ª nh∆∞ b·∫°n ƒëang g·∫∑p m·ªôt s·ªë th·ª≠ th√°ch v·ªõi c√°c kh√°i ni·ªám c·ªët l√µi c·ªßa m√¥n h·ªçc n√†y. Kh√¥ng sao c·∫£, ai c≈©ng c√≥ l√∫c g·∫∑p kh√≥ khƒÉn. ƒêi·ªÅu quan tr·ªçng l√† x√°c ƒë·ªãnh ƒë∆∞·ª£c nh·ªØng ƒëi·ªÉm b·∫°n ch∆∞a th·ª±c s·ª± hi·ªÉu r√µ. H√£y b·∫Øt ƒë·∫ßu t·ª´ ƒë√≥, ch√∫ng ta c√≥ th·ªÉ c√πng nhau x√¢y d·ª±ng m·ªôt k·∫ø ho·∫°ch h·ªçc t·∫≠p ph√π h·ª£p ƒë·ªÉ b·∫°n c·∫£i thi·ªán nh√©. \n",
    "#     - ƒêi·ªÉm d∆∞·ªõi 5.0: K·∫øt qu·∫£ n√†y cho th·∫•y ph∆∞∆°ng ph√°p h·ªçc hi·ªán t·∫°i c√≥ th·ªÉ ch∆∞a ph√π h·ª£p nh·∫•t v·ªõi b·∫°n trong m√¥n h·ªçc n√†y. ƒê√¢y l√† m·ªôt t√≠n hi·ªáu ƒë·ªÉ ch√∫ng ta c√πng nh√¨n l·∫°i. ƒê·ª´ng n·∫£n l√≤ng! ƒê√¢y l√† c∆° h·ªôi ƒë·ªÉ kh√°m ph√° nh·ªØng c√°ch ti·∫øp c·∫≠n m·ªõi hi·ªáu qu·∫£ h∆°n. H√£y th·ª≠ b·∫Øt ƒë·∫ßu l·∫°i t·ª´ nh·ªØng ki·∫øn th·ª©c n·ªÅn t·∫£ng nh·∫•t v√† t√¨m ki·∫øm s·ª± h·ªó tr·ª£ khi c·∫ßn thi·∫øt. Lu√¥n c√≥ c√°ch ƒë·ªÉ ti·∫øn b·ªô!\n",
    "    \n",
    "#     Quy t·∫Øc ƒë√°nh gi√°:\n",
    "#     1. Cung c·∫•p ph·∫£n h·ªìi cho t·ª´ng m√¥n h·ªçc m√† h·ªçc sinh ƒë∆∞·ª£c ch·∫•m ƒëi·ªÉm.\n",
    "#     2. T·∫≠p trung v√†o c·∫£ ƒëi·ªÉm m·∫°nh v√† ƒëi·ªÉm y·∫øu, kh√¥ng ƒë·ªÉ l·∫°i s·ª± m∆° h·ªì v·ªÅ c√°c lƒ©nh v·ª±c c·∫ßn c·∫£i thi·ªán.\n",
    "#     3. N·∫øu h·ªçc sinh ƒë·∫°t d∆∞·ªõi 7.0 ·ªü b·∫•t k·ª≥ m√¥n n√†o, h√£y n√™u r√µ ƒëi·ªÅu n√†y nh∆∞ m·ªôt v·∫•n ƒë·ªÅ c·∫ßn ch√∫ √Ω ngay l·∫≠p t·ª©c.\n",
    "#     4. N·∫øu h·ªçc sinh ƒë·∫°t d∆∞·ªõi 5.0 ·ªü b·∫•t k·ª≥ m√¥n n√†o, nh·∫•n m·∫°nh ƒëi·ªÅu n√†y nh∆∞ m·ªôt s·ª± th·∫•t b·∫°i v√† ƒë·ªÅ ngh·ªã c√°c h√†nh ƒë·ªông kh·∫Øc ph·ª•c m·∫°nh m·∫Ω.\n",
    "    \n",
    "\n",
    "#     D∆∞·ªõi ƒë√¢y l√† d·ªØ li·ªáu c·ªßa h·ªçc sinh:\n",
    "#     {user_info}\n",
    "#     \"\"\"\n",
    "\n",
    "#     prompt = \"\"\"\n",
    "#     D∆∞·ªõi ƒë√¢y l√† ƒë√°nh gi√° c·ªßa h·ªçc sinh:\n",
    "#     {ranking}\n",
    "    \n",
    "#     C√¢u h·ªèi c·ªßa h·ªçc sinh:\n",
    "#     {user_query}\n",
    "    \n",
    "#     T√≥m t·∫Øt n·ªôi dung li√™n quan:\n",
    "#     {summary}\n",
    "    \n",
    "#     H√£y tr·∫£ l·ªùi c√¢u h·ªèi tr√™n, ƒëi·ªÅu ch·ªânh ƒë·ªô chi ti·∫øt/ph·ª©c t·∫°p c·ªßa c√¢u tr·∫£ l·ªùi d·ª±a tr√™n ƒëi·ªÉm s·ªë c·ªßa h·ªçc sinh trong m√¥n h·ªçc li√™n quan: \n",
    "#     - N·∫øu ƒëi·ªÉm cao (>=7.0), gi·∫£i th√≠ch s√¢u s·∫Øc, chi ti·∫øt, c√≥ th·ªÉ m·ªü r·ªông th√™m ki·∫øn th·ª©c n√¢ng cao.\n",
    "#     - N·∫øu ƒëi·ªÉm th·∫•p (<7.0), gi·∫£i th√≠ch ƒë∆°n gi·∫£n, d·ªÖ hi·ªÉu, t·∫≠p trung v√†o kh√°i ni·ªám c∆° b·∫£n.\n",
    "    \n",
    "#     \"\"\"\n",
    "\n",
    "#     ranking = OllamaGenerator(model=llmname_generate).run(rubric.format(user_info=user_info))[\"replies\"][0]\n",
    "#     response = OllamaGenerator(model=llmname_generate).run(prompt.format(ranking=ranking, user_query=user_query, summary=summary))[\"replies\"][0]\n",
    "#     response = fulll_emolize(user_query, response)\n",
    "#     return response\n",
    "\n",
    "# def detect_figure_or_table_query(query: str):\n",
    "#     \"\"\"\n",
    "#     Detect if the query is asking for a figure or table explanation.\n",
    "#     Returns ('figure', number) or ('table', number) or (None, None)\n",
    "#     Handles various keywords: h√¨nh, fig, fig., ·∫£nh, h√¨nh ·∫£nh, b·∫£ng, table, tbl, tab, etc.\n",
    "#     \"\"\"\n",
    "#     # Patterns for figure\n",
    "#     figure_patterns = [\n",
    "#         r\"H√¨nh\\s*(\\d+)\",\n",
    "#         r\"h√¨nh\\s*(\\d+)\",\n",
    "#         r\"h√¨nh\\s*·∫£nh\\s*(\\d+)\",\n",
    "#         r\"·∫£nh\\s*(\\d+)\",\n",
    "#         r\"fig(?:\\.|ure)?\\s*(\\d+)\",   # fig 1, fig. 1, figure 1\n",
    "#         r\"image\\s*(\\d+)\",\n",
    "#         r\"h\\.?\\s*(\\d+)\",              # h. 1 (rare, but possible)\n",
    "#     ]\n",
    "#     for pat in figure_patterns:\n",
    "#         match_figure = re.search(pat, query, re.IGNORECASE)\n",
    "#         if match_figure:\n",
    "#             return (\"H√¨nh\", int(match_figure.group(1)))\n",
    "\n",
    "#     # Patterns for table\n",
    "#     table_patterns = [\n",
    "#         r\"B·∫£ng\\s*(\\d+)\",\n",
    "#         r\"b·∫£ng\\s*(\\d+)\",\n",
    "#         r\"table\\s*(\\d+)\",\n",
    "#         r\"tbl\\.?\\s*(\\d+)\",\n",
    "#         r\"tab\\.?\\s*(\\d+)\",\n",
    "#         r\"t\\.?\\s*(\\d+)\",              # t. 1 (rare, but possible)\n",
    "#     ]\n",
    "#     for pat in table_patterns:\n",
    "#         match_table = re.search(pat, query, re.IGNORECASE)\n",
    "#         if match_table:\n",
    "#             return (\"B·∫£ng\", int(match_table.group(1)))\n",
    "\n",
    "#     return (None, None)\n",
    "\n",
    "# def process_question(question: str, document_store: Chroma) -> str:\n",
    "#     global context, chat_history\n",
    "\n",
    "#     # --- Check for figure/table query ---\n",
    "#     kind, number = detect_figure_or_table_query(question)\n",
    "#     if kind and \"file_upload\" in st.session_state and st.session_state[\"file_upload\"] is not None:\n",
    "#         file_upload = st.session_state[\"file_upload\"]\n",
    "#         user_info = st.session_state.get(\"user_info\", {})\n",
    "#         if kind == \"figure\":\n",
    "#             image = extract_figure_from_pdf(file_upload, number)\n",
    "#             if image is not None:\n",
    "#                 response = answer_figure_with_gemma3(image, question, user_info, context)\n",
    "#                 chat_history[\"user\"] = question\n",
    "#                 chat_history[\"assistant\"] = response\n",
    "#                 context = context_combine(context, chat_history, user_info)\n",
    "#                 return response\n",
    "#             else:\n",
    "#                 return f\"Kh√¥ng t√¨m th·∫•y h√¨nh {number} trong t√†i li·ªáu PDF.\"\n",
    "#         elif kind == \"table\":\n",
    "#             table_text = extract_table_from_pdf(file_upload, number)\n",
    "#             if table_text:\n",
    "#                 response = answer_table_with_gemma3(table_text, question, user_info, context)\n",
    "#                 chat_history[\"user\"] = question\n",
    "#                 chat_history[\"assistant\"] = response\n",
    "#                 context = context_combine(context, chat_history, user_info)\n",
    "#                 return response\n",
    "#             else:\n",
    "#                 return f\"Kh√¥ng t√¨m th·∫•y b·∫£ng {number} trong t√†i li·ªáu PDF.\"\n",
    "#     # --- END NEW ---\n",
    "    \n",
    "#     pipe = Pipeline()\n",
    "#     pipe.add_component(\"text_embedder\", OllamaTextEmbedder(model=\"jeffh/intfloat-multilingual-e5-large-instruct:f16\"))\n",
    "#     pipe.add_component(\"retriever\", InMemoryEmbeddingRetriever(document_store, top_k=5))\n",
    "#     pipe.add_component(\"prompt_builder\", PromptBuilder(template=prompt_template, required_variables=[\"query\", \"documents\"]))\n",
    "#     pipe.add_component(\"router_llm\", OllamaGenerator(model=llmname_route))\n",
    "#     pipe.add_component(\"router\", ConditionalRouter(routes))\n",
    "#     pipe.add_component(\"prompt_builder_after_documents\", PromptBuilder(template=prompt_template_after_documents, required_variables=[\"query\", \"documents\", \"context\"]))\n",
    "#     pipe.add_component(\"websearch\", DuckduckgoApiWebSearch(top_k=5, backend=\"auto\"))\n",
    "#     pipe.add_component(\"prompt_builder_after_websearch\", PromptBuilder(template=prompt_template_after_websearch, required_variables=[\"user_info\", \"documents\", \"web_urls\", \"query\", \"context\"]))\n",
    "#     pipe.add_component(\"prompt_builder_after_user_info\", PromptBuilder(template=prompt_template_after_user_info, required_variables=[\"user_info\", \"query\", \"context\"]))\n",
    "#     pipe.add_component(\"prompt_joiner\", BranchJoiner(str))\n",
    "#     pipe.add_component(\"llm\", OllamaGenerator(model=llmname_generate))\n",
    "#     pipe.add_component(\"hallu_llm\", OllamaGenerator(model=llmname_generate))\n",
    "#     pipe.add_component(\"hallu_prompt\", PromptBuilder(template=propmt_hallu_grader, required_variables=[\"user_info\", \"documents\", \"llm_replies\", \"context\"]))\n",
    "#     pipe.add_component(\"hallu_router\", ConditionalRouter(hallu_route))\n",
    "#     pipe.add_component(\"document_joiner\", BranchJoiner(List[Document]))  # Joiner for documents\n",
    "#     pipe.add_component(\"user_info\", UserInfo())\n",
    "    \n",
    "#     # Input\n",
    "#     pipe.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
    "#     pipe.connect(\"retriever\", \"prompt_builder.documents\")\n",
    "    \n",
    "#     # Adding user info\n",
    "#     pipe.connect(\"user_info.user_info\", \"prompt_builder_after_user_info.user_info\")\n",
    "#     pipe.connect(\"user_info.user_info\", \"prompt_builder_after_websearch.user_info\")\n",
    "#     pipe.connect(\"user_info.user_info\", \"hallu_prompt.user_info\")\n",
    "    \n",
    "    \n",
    "#     # If else. Start prompt_builder, End: router\n",
    "#     pipe.connect(\"prompt_builder\", \"router_llm\")\n",
    "#     pipe.connect(\"router_llm.replies\", \"router.replies\")\n",
    "    \n",
    "#     # Document Start: router, End: Document Joiner\n",
    "#     pipe.connect(\"router.go_to_documents\", \"prompt_builder_after_documents.query\")\n",
    "#     pipe.connect(\"retriever\", \"prompt_builder_after_documents.documents\")\n",
    "#     pipe.connect(\"prompt_builder_after_documents\", \"prompt_joiner\")\n",
    "    \n",
    "#     # Websearch Start: router, End: Document Joiner\n",
    "#     pipe.connect(\"router.go_to_websearch\", \"websearch.query\")\n",
    "#     pipe.connect(\"router.go_to_websearch\", \"prompt_builder_after_websearch.query\")\n",
    "#     pipe.connect(\"websearch.documents\", \"prompt_builder_after_websearch.documents\")\n",
    "#     pipe.connect(\"websearch.documents\", \"prompt_builder_after_websearch.web_urls\")\n",
    "#     pipe.connect(\"prompt_builder_after_websearch\", \"prompt_joiner\")\n",
    "    \n",
    "#     # User Info Start: router, End: Prompt Joiner\n",
    "#     pipe.connect(\"router.go_to_user_info\", \"prompt_builder_after_user_info.query\")\n",
    "#     pipe.connect(\"prompt_builder_after_user_info\", \"prompt_joiner\")\n",
    "    \n",
    "#     # Generate\n",
    "#     pipe.connect(\"prompt_joiner\", \"llm\")\n",
    "    \n",
    "#     # Hallu Grading\n",
    "#     pipe.connect(\"retriever\", \"document_joiner\")  # Connect retriever documents to document joiner\n",
    "#     pipe.connect(\"websearch.documents\", \"document_joiner\")  # Connect web search documents to document joiner\n",
    "#     pipe.connect(\"document_joiner\", \"hallu_prompt.documents\")  # connect document_joiner to hallu_prompt.documents\n",
    "#     pipe.connect(\"llm.replies\", \"hallu_router.llm_replies\")\n",
    "#     pipe.connect(\"llm.replies\", \"hallu_prompt.llm_replies\")\n",
    "#     pipe.connect(\"hallu_prompt\", \"hallu_llm\")\n",
    "#     pipe.connect(\"hallu_llm.replies\", \"hallu_router.replies\")\n",
    "\n",
    "#     question = rewrite_query(question, context)\n",
    "#     run_dict = {\n",
    "#     \"text_embedder\": {\"text\": question},\n",
    "#     \"prompt_builder\": {\"query\": question},\n",
    "#     \"router\": {\"query\": question},\n",
    "#     \"hallu_router\": {\"query\": question},\n",
    "#     \"prompt_builder_after_documents\": {\"context\": context},\n",
    "#     \"prompt_builder_after_websearch\": {\"context\": context},\n",
    "#     \"prompt_builder_after_user_info\": {\"context\": context},\n",
    "#     \"hallu_prompt\": {\"context\": context},\n",
    "#     }\n",
    "\n",
    "#     result = run_single(pipe, question, run_dict)\n",
    "\n",
    "#     chat_history[\"user\"] = question\n",
    "#     chat_history[\"assistant\"] = result\n",
    "#     context = context_combine(context, chat_history, st.session_state.get(\"user_info\", {}))\n",
    "    \n",
    "#     result = fulll_emolize(question, result)\n",
    "#     return result\n",
    "\n",
    "# @st.cache_data\n",
    "# def extract_all_pages_as_images(file_upload) -> List[Any]:\n",
    "#     \"\"\"\n",
    "#     Tr√≠ch xu·∫•t t·∫•t c·∫£ c√°c trang t·ª´ t·ªáp PDF d∆∞·ªõi d·∫°ng h√¨nh ·∫£nh.\n",
    "#     \"\"\"\n",
    "#     logger.info(f\"Extracting all pages as images from file: {file_upload.name}\")\n",
    "#     pdf_pages = []\n",
    "#     with pdfplumber.open(file_upload) as pdf:\n",
    "#         pdf_pages = [page.to_image().original for page in pdf.pages]\n",
    "#     logger.info(\"PDF pages extracted as images\")\n",
    "#     return pdf_pages\n",
    "\n",
    "# # Assuming you have these imports\n",
    "# # from haystack.document_stores import InMemoryDocumentStore\n",
    "# # from chromadb import Chroma\n",
    "# # import streamlit as st\n",
    "# # import logging\n",
    "\n",
    "# def delete_vector_db(vector_db: Optional[Chroma], document_store: Optional[\"InMemoryDocumentStore\"]) -> None:\n",
    "#     \"\"\"\n",
    "#     X√≥a c∆° s·ªü d·ªØ li·ªáu vector v√†/ho·∫∑c document store, v√† x√≥a tr·∫°ng th√°i phi√™n li√™n quan.\n",
    "#     \"\"\"\n",
    "#     if vector_db is not None:\n",
    "#         logger.info(\"Deleting Chroma vector DB\")\n",
    "#         try:\n",
    "#             vector_db.delete_collection()\n",
    "#             st.success(\"C√°c t·ªáp t·∫°m th·ªùi ƒë√£ ƒë∆∞·ª£c x√≥a th√†nh c√¥ng.\")\n",
    "#             logger.info(\"Chroma Vector DB and related session state cleared\")\n",
    "#             st.session_state.pop(\"pdf_pages\", None)\n",
    "#             st.session_state.pop(\"file_upload\", None)\n",
    "#             st.session_state.pop(\"vector_db\", None)\n",
    "#             st.rerun()\n",
    "#         except Exception as e:\n",
    "#             st.error(f\"L·ªói khi x√≥a t√†i li·ªáu: {str(e)}\")\n",
    "#             logger.error(f\"Error deleting Chroma collection: {e}\")\n",
    "#     elif document_store is not None:\n",
    "#         logger.info(\"Clearing InMemoryDocumentStore\")\n",
    "#         try:\n",
    "#             document_store.clear_all_documents()\n",
    "#             st.success(\"InMemoryDocumentStore ƒë√£ ƒë∆∞·ª£c x√≥a th√†nh c√¥ng.\")\n",
    "#             logger.info(\"InMemoryDocumentStore cleared\")\n",
    "#             st.session_state.pop(\"document_store\", None)\n",
    "#             st.rerun()\n",
    "#         except Exception as e:\n",
    "#             st.error(f\"L·ªói khi x√≥a InMemoryDocumentStore: {str(e)}\")\n",
    "#             logger.error(f\"Error clearing InMemoryDocumentStore: {e}\")\n",
    "#     else:\n",
    "#         st.error(\"Kh√¥ng t√¨m th·∫•y c∆° s·ªü d·ªØ li·ªáu vector ho·∫∑c document store ƒë·ªÉ x√≥a.\")\n",
    "#         logger.warning(\"Attempted to delete vector DB or document store, but none was found\")\n",
    "\n",
    "\n",
    "# def main() -> None:\n",
    "#     \"\"\"\n",
    "#     H√†m ch√≠nh ƒë·ªÉ ch·∫°y ·ª©ng d·ª•ng Streamlit.\n",
    "#     \"\"\"\n",
    "#     st.subheader(\"üìö Adaptive Academics\", divider=\"gray\", anchor=False)\n",
    "#     col1, col2 = st.columns([1.5, 2])\n",
    "\n",
    "#     # Initialize session state\n",
    "#     if \"messages\" not in st.session_state:\n",
    "#         st.session_state[\"messages\"] = []\n",
    "#     if \"vector_db\" not in st.session_state:\n",
    "#         st.session_state[\"vector_db\"] = None\n",
    "\n",
    "    \n",
    "#     # --- UI Part ---\n",
    "#     file_upload = col1.file_uploader(\n",
    "#         \"T·∫£i l√™n t·ªáp PDF ho·∫∑c MP4 ‚Üì\",\n",
    "#         type=[\"pdf\", \"mp4\"],\n",
    "#         accept_multiple_files=False,\n",
    "#         key=\"file_uploader\"\n",
    "#     )\n",
    "    \n",
    "#     if file_upload:\n",
    "#         file_type = Path(file_upload.name).suffix.lower()\n",
    "    \n",
    "#         if st.session_state.get(\"vector_db\") is None:\n",
    "#             with st.spinner(\"ƒêang x·ª≠ l√Ω t·ªáp ƒë√£ t·∫£i l√™n...\"):\n",
    "#                 if file_type == \".pdf\":\n",
    "#                     # Handle PDF\n",
    "#                     st.session_state[\"vector_db\"] = create_vector_db(file_upload)\n",
    "#                     st.session_state[\"file_upload\"] = file_upload\n",
    "    \n",
    "#                     # Extract and store PDF pages\n",
    "#                     with pdfplumber.open(file_upload) as pdf:\n",
    "#                         st.session_state[\"pdf_pages\"] = [page.to_image().original for page in pdf.pages]\n",
    "    \n",
    "#                 elif file_type == \".mp4\":\n",
    "#                     # Save uploaded file to temp location\n",
    "#                     temp_video_path = tempfile.NamedTemporaryFile(delete=False, suffix=\".mp4\")\n",
    "#                     temp_video_path.write(file_upload.read())\n",
    "#                     temp_video_path.flush()\n",
    "    \n",
    "#                     # Handle MP4\n",
    "#                     st.session_state[\"vector_db\"] = create_vector_db_from_video(temp_video_path.name)\n",
    "    \n",
    "#                     # Clean up temp file after\n",
    "#                     temp_video_path.close()\n",
    "    \n",
    "#         # --- Only for PDFs ---\n",
    "#         if \"pdf_pages\" in st.session_state and st.session_state[\"pdf_pages\"]:\n",
    "#             zoom_level = col1.slider(\n",
    "#                 \"M·ª©c thu ph√≥ng t√†i li·ªáu\",\n",
    "#                 min_value=100,\n",
    "#                 max_value=2000,\n",
    "#                 value=700,\n",
    "#                 step=50,\n",
    "#                 key=\"zoom_slider\"\n",
    "#             )\n",
    "    \n",
    "#             with col1:\n",
    "#                 with st.container(height=410, border=True):\n",
    "#                     for page_image in st.session_state[\"pdf_pages\"]:\n",
    "#                         st.image(page_image, width=zoom_level)\n",
    "        \n",
    "#     # Delete collection button\n",
    "#     delete_collection = col1.button(\n",
    "#         \"üîÑ Lo·∫°i b·ªè t·∫•t c·∫£ c√°c t√†i li·ªáu ƒë√£ upload hi·ªán t·∫°i\",\n",
    "#         type=\"secondary\",\n",
    "#         key=\"delete_button\"\n",
    "#     )\n",
    "\n",
    "#     if delete_collection:\n",
    "#         delete_vector_db(st.session_state[\"vector_db\"])\n",
    "\n",
    "#     # Chat interface\n",
    "#     with col2:\n",
    "#         message_container = st.container(height=500, border=True)\n",
    "\n",
    "#         # Display chat history\n",
    "#         for i, message in enumerate(st.session_state[\"messages\"]):\n",
    "#             avatar = \"ü§ñ\" if message[\"role\"] == \"assistant\" else \"üéì\"\n",
    "#             with message_container.chat_message(message[\"role\"], avatar=avatar):\n",
    "#                 st.markdown(message[\"content\"])\n",
    "\n",
    "#         # Chat input and processing\n",
    "#         if prompt := st.chat_input(\"Nh·∫≠p c√¢u h·ªèi v√†o ƒë√¢y...\", key=\"chat_input\"):\n",
    "#             try:\n",
    "#                 # Add user message to chat\n",
    "#                 st.session_state[\"messages\"].append({\"role\": \"user\", \"content\": prompt})\n",
    "#                 with message_container.chat_message(\"user\", avatar=\"üéì\"):\n",
    "#                     st.markdown(prompt)\n",
    "\n",
    "#                 # Process and display assistant response\n",
    "#                 with message_container.chat_message(\"assistant\", avatar=\"ü§ñ\"):\n",
    "#                     with st.spinner(\":green[ƒêang x·ª≠ l√Ω...]\"):\n",
    "#                         if st.session_state[\"vector_db\"] is not None:\n",
    "#                             response = process_question(\n",
    "#                                 prompt, st.session_state[\"vector_db\"]\n",
    "#                             )\n",
    "#                             # response = \"aaaaaaaaaaaa\"\n",
    "#                             # Use st.markdown() to render the response\n",
    "#                             st.markdown(response)  # This will render `\\n` correctly as newlines\n",
    "#                         else:\n",
    "#                             st.warning(\"Vui l√≤ng t·∫£i l√™n t·ªáp PDF ho·∫∑c video (.mp4) tr∆∞·ªõc khi ƒë·∫∑t c√¢u h·ªèi.\")\n",
    "\n",
    "#                 # Add assistant response to chat history\n",
    "#                 if st.session_state[\"vector_db\"] is not None:\n",
    "#                     st.session_state[\"messages\"].append(\n",
    "#                         {\"role\": \"assistant\", \"content\": response})\n",
    "#             except Exception as e:\n",
    "#                 st.error(e, icon=\"‚õîÔ∏è\")\n",
    "#                 logger.error(f\"Error processing prompt: {e}\")\n",
    "#         else:\n",
    "#             if st.session_state[\"vector_db\"] is None:\n",
    "#                 st.warning(\"T·∫£i l√™n t·ªáp PDF ho·∫∑c video (.mp4) ƒë·ªÉ b·∫Øt ƒë·∫ßu tr√≤ chuy·ªán...\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T02:07:08.910085Z",
     "iopub.status.busy": "2025-05-17T02:07:08.909602Z",
     "iopub.status.idle": "2025-05-17T02:13:22.290447Z",
     "shell.execute_reply": "2025-05-17T02:13:22.289530Z",
     "shell.execute_reply.started": "2025-05-17T02:07:08.910053Z"
    },
    "id": "HNaBMLRoft4V",
    "outputId": "a73b4aa2-c5e3-4d8a-9065-eacc115200be",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Public URL: https://0eed-34-83-124-71.ngrok-free.app\n",
      "\n",
      "·ª®ng d·ª•ng ƒëang ch·∫°y. Truy c·∫≠p th√¥ng qua URL C√¥ng khai. \n",
      "Nh·∫•n Enter ƒë·ªÉ d·ª´ng ·ª©ng d·ª•ng...\n",
      "M√°y ch·ªß ƒë√£ ƒë√≥ng...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"Run script for the Streamlit application.\"\"\"\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "import logging\n",
    "from pyngrok import ngrok  # Import ngrok\n",
    "\n",
    "logging.basicConfig(filename='streamlit_hf_rag.log', level=logging.DEBUG,\n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def main():\n",
    "\n",
    "    app_path = Path(\"/kaggle/working/SupertCagRag/src/main.py\")\n",
    "    if not app_path.exists():\n",
    "        logger.error(f\"Error: Could not find {app_path}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    log_file = None # Initialize log_file outside try block\n",
    "    ngrok_tunnel = None # Initialize ngrok_tunnel outside try block\n",
    "\n",
    "    try:\n",
    "        # Run the Streamlit app in the background and write output to log file\n",
    "        log_file = open(\"/kaggle/working//hf_logs.txt\", \"w\") # Changed log filename\n",
    "        process = subprocess.Popen([\"streamlit\", \"run\", str(app_path)],\n",
    "                                            stdout=log_file, stderr=subprocess.STDOUT)\n",
    "\n",
    "        # Start ngrok\n",
    "        ngrok_tunnel = ngrok.connect(8501)\n",
    "        public_url = ngrok_tunnel.public_url\n",
    "        logger.info(f\"Public URL: {public_url}\")\n",
    "        print('Public URL:', public_url)\n",
    "        print(\"\\n·ª®ng d·ª•ng ƒëang ch·∫°y. Truy c·∫≠p th√¥ng qua URL C√¥ng khai. \\nNh·∫•n Enter ƒë·ªÉ d·ª´ng ·ª©ng d·ª•ng...\") # Translated line\n",
    "\n",
    "        # Press enter to stop application\n",
    "        input() # Wait for user input to stop\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        logger.exception(f\"Error running Streamlit app: {e}\") # Translated log message internally\n",
    "        sys.exit(1)\n",
    "    except Exception as e_ngrok: # Catch ngrok related exceptions\n",
    "        logger.exception(f\"Ngrok error: {e_ngrok}\") # Translated log message internally\n",
    "        sys.exit(1)\n",
    "    finally:\n",
    "        if log_file: # Check if log_file is open before closing\n",
    "            log_file.close()\n",
    "        if ngrok_tunnel: # Check if ngrok_tunnel is initialized before killing\n",
    "            ngrok.kill()\n",
    "        print(\"M√°y ch·ªß ƒë√£ ƒë√≥ng...\") # Translated line\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "CkooOmJBCUUS",
    "CgyW2qfACirC"
   ],
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
