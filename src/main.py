
from aicore_database import vectordb
from aicore.airesource import ultils as aiultils
from aicore import (
    emotionilzer,
    init_pipe,
    run_pipe,
    prompt_caller,
    tablenvideo,
)
import aicore.airesource.config as config
import os
import sys
import logging
import tempfile
from pathlib import Path
from typing import List, Any


BASE_DIR = Path(__file__).resolve().parent
SRC_PARENT = BASE_DIR.parent  # This is the directory containing 'src'
if str(SRC_PARENT) not in sys.path:
    sys.path.insert(0, str(SRC_PARENT))

from haystack_integrations.components.embedders.ollama import OllamaDocumentEmbedder
from haystack_integrations.components.generators.ollama import OllamaGenerator
import pdfplumber
import streamlit as st


# Set protobuf environment variable to avoid error messages
os.environ["PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION"] = "python"
PERSIST_DIRECTORY = os.path.join("data", "vectors")

# Streamlit page configuration
st.set_page_config(
    page_title="Adaptive Academics",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="collapsed",
)

# Initialize user info in session state if not already present
if "user_info" not in st.session_state:
    st.session_state["user_info"] = {
        "TÃªn": "B",
        "ChuyÃªn ngÃ nh": "Kinh táº¿",
        "Kinh táº¿ vi mÃ´": 8.7,
        "Kinh táº¿ vÄ© mÃ´": 7.3,
        "NguyÃªn lÃ½ káº¿ toÃ¡n": 8.6,
        "Marketing cÆ¡ báº£n": 8.4,
        "Quáº£n trá»‹": 8.1,
    }

chat_history = {
    "user": None,
    "assistant": None,
}

# Logging configuration
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
logger = logging.getLogger(__name__)

LLM_generate = OllamaGenerator(model=config.LLMNAME_GENERATE)
LLM_router = OllamaGenerator(model=config.LLMNAME_ROUTE)
LLM_embedd = OllamaDocumentEmbedder(model=config.LLMNAME_EMBEDDER)

Pipe = init_pipe.init_pipeline(st.session_state["user_info"])
Context = prompt_caller.context_init(LLM_generate)


def process_question(question: str, document_store) -> str:
    global Context, chat_history

    kind, number = aiultils.detect_figure_or_table_query(question)
    file_upload = st.session_state.get("file_upload")
    user_info = st.session_state.get("user_info", {})

    if kind and file_upload is not None:
        if kind == "figure":
            image = aiultils.extract_figure_from_pdf(file_upload, number)
            if image is not None:
                response = tablenvideo.answer_figure_with_gemma3(
                    image, question, user_info, Context
                )
            else:
                return f"KhÃ´ng tÃ¬m tháº¥y hÃ¬nh {number} trong tÃ i liá»‡u PDF."
        elif kind == "table":
            table_text = aiultils.extract_table_from_pdf(file_upload, number)
            if table_text:
                response = tablenvideo.answer_table_with_gemma3(
                    LLM_generate, table_text, question, user_info, Context
                )
            else:
                return f"KhÃ´ng tÃ¬m tháº¥y báº£ng {number} trong tÃ i liá»‡u PDF."
        else:
            response = None

        if response:
            chat_history["user"] = question
            chat_history["assistant"] = response
            Context = prompt_caller.context_combine(
                LLM_generate, Context, chat_history, user_info
            )
            return response

    # Standard question processing
    pipe2 = init_pipe.setup_pipeline_with_document_store(Pipe, document_store)
    rewritten_question = prompt_caller.rewrite_query(
        LLM_generate, question, Context)
    run_dict = {
        "text_embedder": {"text": rewritten_question},
        "prompt_builder": {"query": rewritten_question},
        "router": {"query": rewritten_question},
        "hallu_router": {"query": rewritten_question},
        "prompt_builder_after_documents": {"context": Context},
        "prompt_builder_after_websearch": {"context": Context},
        "prompt_builder_after_user_info": {"context": Context},
        "hallu_prompt": {"context": Context},
    }

    result = run_pipe.run_single(pipe2, rewritten_question, run_dict)
    chat_history["user"] = rewritten_question
    chat_history["assistant"] = result
    Context = prompt_caller.context_combine(
        LLM_generate, Context, chat_history, user_info
    )

    result = emotionilzer.fulll_emolize(LLM_generate, question, result)
    return result


@st.cache_data
def extract_all_pages_as_images(file_upload) -> List[Any]:
    logger.info(
        f"Extracting all pages as images from file: {file_upload.name}")
    with pdfplumber.open(file_upload) as pdf:
        pdf_pages = [page.to_image().original for page in pdf.pages]
    logger.info("PDF pages extracted as images")
    return pdf_pages


def delete_vector_db(vector_db, document_store=None) -> None:
    if vector_db is not None:
        logger.info("Deleting Chroma vector DB")
        try:
            vector_db.delete_collection()
            st.success("CÃ¡c tá»‡p táº¡m thá»i Ä‘Ã£ Ä‘Æ°á»£c xÃ³a thÃ nh cÃ´ng.")
            logger.info("Chroma Vector DB and related session state cleared")
            st.session_state.pop("pdf_pages", None)
            st.session_state.pop("file_upload", None)
            st.session_state.pop("vector_db", None)
            st.rerun()
        except Exception as e:
            st.error(f"Lá»—i khi xÃ³a tÃ i liá»‡u: {str(e)}")
            logger.error(f"Error deleting Chroma collection: {e}")
    elif document_store is not None:
        logger.info("Clearing InMemoryDocumentStore")
        try:
            document_store.clear_all_documents()
            st.success("InMemoryDocumentStore Ä‘Ã£ Ä‘Æ°á»£c xÃ³a thÃ nh cÃ´ng.")
            logger.info("InMemoryDocumentStore cleared")
            st.session_state.pop("document_store", None)
            st.rerun()
        except Exception as e:
            st.error(f"Lá»—i khi xÃ³a InMemoryDocumentStore: {str(e)}")
            logger.error(f"Error clearing InMemoryDocumentStore: {e}")
    else:
        st.error("KhÃ´ng tÃ¬m tháº¥y cÆ¡ sá»Ÿ dá»¯ liá»‡u vector hoáº·c document store Ä‘á»ƒ xÃ³a.")
        logger.warning(
            "Attempted to delete vector DB or document store, but none was found")


def main() -> None:
    st.subheader("ğŸ“š Adaptive Academics", divider="gray", anchor=False)
    col1, col2 = st.columns([1.5, 2])

    # Initialize session state
    if "messages" not in st.session_state:
        st.session_state["messages"] = []
    if "vector_db" not in st.session_state:
        st.session_state["vector_db"] = None

    # --- UI Part ---
    file_upload = col1.file_uploader(
        "Táº£i lÃªn tá»‡p PDF hoáº·c MP4 â†“",
        type=["pdf", "mp4"],
        accept_multiple_files=False,
        key="file_uploader",
    )

    if file_upload:
        file_type = Path(file_upload.name).suffix.lower()

        if st.session_state.get("vector_db") is None:
            with st.spinner("Äang xá»­ lÃ½ tá»‡p Ä‘Ã£ táº£i lÃªn..."):
                if file_type == ".pdf":
                    st.session_state["vector_db"] = vectordb.create_vector_db(
                        file_upload, logger, LLM_embedd
                    )
                    st.session_state["file_upload"] = file_upload
                    with pdfplumber.open(file_upload) as pdf:
                        st.session_state["pdf_pages"] = [
                            page.to_image().original for page in pdf.pages
                        ]
                elif file_type == ".mp4":
                    temp_video_path = tempfile.NamedTemporaryFile(
                        delete=False, suffix=".mp4"
                    )
                    temp_video_path.write(file_upload.read())
                    temp_video_path.flush()
                    st.session_state["vector_db"] = vectordb.create_vector_db_from_video(
                        temp_video_path.name
                    )
                    temp_video_path.close()

        # --- Only for PDFs ---
        if "pdf_pages" in st.session_state and st.session_state["pdf_pages"]:
            zoom_level = col1.slider(
                "Má»©c thu phÃ³ng tÃ i liá»‡u",
                min_value=100,
                max_value=2000,
                value=700,
                step=50,
                key="zoom_slider",
            )
            with col1:
                with st.container(height=410, border=True):
                    for page_image in st.session_state["pdf_pages"]:
                        st.image(page_image, width=zoom_level)

    # Delete collection button
    delete_collection = col1.button(
        "ğŸ”„ Loáº¡i bá» táº¥t cáº£ cÃ¡c tÃ i liá»‡u Ä‘Ã£ upload hiá»‡n táº¡i",
        type="secondary",
        key="delete_button",
    )

    if delete_collection:
        delete_vector_db(st.session_state["vector_db"])

    # Chat interface
    with col2:
        message_container = st.container(height=500, border=True)

        # Display chat history
        for message in st.session_state["messages"]:
            avatar = "ğŸ¤–" if message["role"] == "assistant" else "ğŸ“"
            with message_container.chat_message(message["role"], avatar=avatar):
                st.markdown(message["content"])

        # Chat input and processing
        if prompt := st.chat_input("Nháº­p cÃ¢u há»i vÃ o Ä‘Ã¢y...", key="chat_input"):
            try:
                st.session_state["messages"].append(
                    {"role": "user", "content": prompt}
                )
                with message_container.chat_message("user", avatar="ğŸ“"):
                    st.markdown(prompt)

                with message_container.chat_message("assistant", avatar="ğŸ¤–"):
                    with st.spinner(":green[Äang xá»­ lÃ½...]"):
                        if st.session_state["vector_db"] is not None:
                            response = process_question(
                                prompt, st.session_state["vector_db"]
                            )
                            st.markdown(response)
                        else:
                            st.warning(
                                "Vui lÃ²ng táº£i lÃªn tá»‡p PDF hoáº·c video (.mp4) trÆ°á»›c khi Ä‘áº·t cÃ¢u há»i."
                            )

                if st.session_state["vector_db"] is not None:
                    st.session_state["messages"].append(
                        {"role": "assistant", "content": response}
                    )
            except Exception as e:
                st.error(e, icon="â›”ï¸")
                logger.error(f"Error processing prompt: {e}")
        else:
            if st.session_state["vector_db"] is None:
                st.warning(
                    "Táº£i lÃªn tá»‡p PDF hoáº·c video (.mp4) Ä‘á»ƒ báº¯t Ä‘áº§u trÃ² chuyá»‡n..."
                )


if __name__ == "__main__":
    main()
